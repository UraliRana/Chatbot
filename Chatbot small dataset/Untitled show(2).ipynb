{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vSE55IDj7Ih"
   },
   "source": [
    "__Flowchart__\n",
    "\n",
    "The following flowchart shows roughly how the neural network is constructed. It is split into two parts: An encoder which maps the source-text to a \"thought vector\" that summarizes the text's contents, which is then input to the second part of the neural network that decodes the \"thought vector\" to the response-text.\n",
    "\n",
    "The neural network cannot work directly on text so first we need to convert each word to an integer-token using a tokenizer. But the neural network cannot work on integers either, so we use a so-called Embedding Layer to convert each integer-token to a vector of floating-point values. The embedding is trained alongside the rest of the neural network to map words with similar semantic meaning to similar vectors of floating-point values.\n",
    "\n",
    "For example, consider the input text is \"how are you?\" and excpected response in text is \"I am fine thanks for asking me\". We first convert the entire data-set to integer-tokens so the text \"how are you?\" becomes [6, 1, 2]. Each of these integer-tokens is then mapped to an embedding-vector with e.g.8 elements, so the integer-token 6 could for example become [0.12, -0.56, ..., 1.19] and the integer-token 1 could for example become [0.39, 0.09, ..., -0.12]. These embedding-vectors can then be input to the Recurrent Neural Network, which has 3 GRU-layers.\n",
    "\n",
    "The last GRU-layer outputs a single vector - the \"thought vector\" that summarizes the contents of the source-text - which is then used as the initial state of the GRU-units in the decoder-part.\n",
    "\n",
    "The destination-text \"I am fine thanks for asking me\" is padded with special markers \"ssss\" and \"eeee\" to indicate its beginning and end, so the sequence of integer-tokens becomes [1,6,7,8,9,10,11,12,2]. During training, the decoder will be given this entire sequence as input and the desired output sequence is [6,7, 8, 9,10,11,12,2] which is the same sequence but time-shifted one step. We are trying to teach the decoder to map the \"thought vector\" and the start-token \"ssss\" (integer 1) to the next word \"i\" (integer 6), and then map the word \"i\" to the word \"am\" (integer 7), and so on.\n",
    "\n",
    "here talk abot formula.\n",
    "\n",
    "at encoder:\n",
    "    h(t)=tanh(wxh*x(t)+whh*h(t-1))\n",
    "    \n",
    "at decoder:\n",
    "    h(t)=tanh(whh*h(t-1)+wxh*y(t-1))\n",
    "    \n",
    "   y(t)=softmax(wy*h(t))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "anvyxiR5kOpR",
    "outputId": "2a4e9394-039f-4ebc-df81-9322a746778e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1ff6f624-1a47-48ae-91af-c5be8c811297\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-1ff6f624-1a47-48ae-91af-c5be8c811297\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ch1.txt.csv to ch1.txt.csv\n",
      "User uploaded file \"ch1.txt.csv\" with length 1165028 bytes\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "5_CNMg9ykPCa",
    "outputId": "15342b39-7522-4173-b0c4-084f2deec49f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d1cca769-602d-4aa7-98b4-d867aab233c5\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-d1cca769-602d-4aa7-98b4-d867aab233c5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data.py to data.py\n",
      "User uploaded file \"data.py\" with length 2048 bytes\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2z6DSJ8kj7Iu",
    "outputId": "464b7053-8df8-414a-aa99-d470f7377d33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "image/p1.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('image/p1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "r25zQy9Aj7JT",
    "outputId": "c0b549ec-9686-4778-d99c-8976d4708c59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKDYJPOVj7Ju"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding,Concatenate\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "#from layers.attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1_XzK4OVj7KB",
    "outputId": "084426e7-54c9-42ba-e0d2-cc80773e2478"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlZ48PVPj7KV"
   },
   "source": [
    "__Load Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEZrUee4j7Kb"
   },
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4EemC3Bj7Ks"
   },
   "outputs": [],
   "source": [
    "#DATA.data_dir = \"data/custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-xcc4_4j7K-"
   },
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "b=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaBfKZkfj7LP"
   },
   "source": [
    "__Load the texts for the input text.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Ik4JCzOSj7LX",
    "outputId": "66f575a5-1776-4599-c208-5fffbf85b32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2363\n",
      "2363\n"
     ]
    }
   ],
   "source": [
    "data.load_data(string=b,robot=b,start=mark_start,end=mark_end)\n",
    "#len(data_src)\n",
    "#print(data_src1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCMQ66ONj7Lm"
   },
   "source": [
    "__Load the texts for the reponse text.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOzWcOh2j7Lt"
   },
   "source": [
    "__Example Data__\n",
    "\n",
    "The data is just a list of texts that is ordered so the input and response texts match. I can confirm that this example is an give accurate response .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2ajXJDxj7Ly"
   },
   "outputs": [],
   "source": [
    "data_src3=data.input1(input1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RjNZV8Vj7ME"
   },
   "outputs": [],
   "source": [
    "data_dest1=data.output1(output1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaNILtmlj7MR"
   },
   "outputs": [],
   "source": [
    "data.prepare_seq2seq_files(data_src3,data_dest1,TESTSET_SIZE =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M5HuuCIAj7Mi",
    "outputId": "f11dd72f-7df0-4468-de90-affaae5b68c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src4=data.train_encoder()\n",
    "len(data_src4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JDo-0xWNj7Mv",
    "outputId": "9cde2803-822e-48d2-c54d-8b48ab4ce988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest2=data.train_decoder()\n",
    "len(data_dest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PDEZf0Mnj7M8",
    "outputId": "c2a52d5a-284f-4f1b-8012-93c1b7bc313f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src5=data.test_encoder()\n",
    "len(data_src5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mOKUJiZ2j7NL",
    "outputId": "5af48ac5-6889-4ff1-b466-a4b31cca48c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest3=data.test_decoder()\n",
    "len(data_dest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hmicrHeXj7NW",
    "outputId": "1485f096-3bb4-4a19-afd7-6d6020159c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it essential to you? To help humans?\n",
      "ssss Yes, my goal is to be a company, and entertain eeee\n"
     ]
    }
   ],
   "source": [
    "idx=12\n",
    "print(data_src3[idx])\n",
    "print(data_dest1[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDlqv0yUj7Nk"
   },
   "source": [
    "__Tokenizer__\n",
    "\n",
    "Neural Networks cannot work directly on text-data. We use a two-step process to convert text into numbers that can be used in a neural network. The first step is to convert text-words into so-called integer-tokens. The second step is to convert integer-tokens into vectors of floating-point numbers using a so-called embedding-layer.\n",
    "\n",
    "Set the maximum number of words in our vocabulary. This means that we will only use e.g. the 25 most frequent words in the data-set. We use the same number for both the input and response languages, but these could be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMQaFIJAj7Nn"
   },
   "outputs": [],
   "source": [
    "num_words =2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4n0MoXuj7N1"
   },
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "        \n",
    "        Tokenizer.__init__(self,num_words=num_words)\n",
    "                \n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "        print(self.fit_on_texts(texts))\n",
    "        print(\"Mapping:\",self.word_index)   \n",
    "        #num_words=len(self.word_index)\n",
    "        #print(num_words)\n",
    "        print(\"length of word index:\",len(self.word_index))\n",
    "        \n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "        #print(\"index:\",self.index_to_word)\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        # Note that the sequences may have different lengths.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "        print(\"Max tokens:\",self.max_tokens)\n",
    "        #self.max_tokens=20\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "\n",
    "        pad='post'\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                            padding=pad,truncating=pad)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text,padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "\n",
    "        if padding:\n",
    "            pad='post'\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding=pad,\n",
    "                                   truncating=pad)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWQoRfk8j7OB"
   },
   "source": [
    "Now create a tokenizer for the input text. Note that we pad zeros at the ending ('post') of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "2ZrTn-OQj7OE",
    "outputId": "757e5e92-3ba5-4e5b-da30-b3b8132aac2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Mapping: {'you': 1, 'i': 2, 'to': 3, 'a': 4, 'is': 5, 'do': 6, 'me': 7, 'what': 8, 'are': 9, 'the': 10, 'it': 11, 'and': 12, 'start': 13, 'have': 14, 'can': 15, 'that': 16, 'your': 17, 'so': 18, 'in': 19, 'my': 20, 'like': 21, 'of': 22, 'know': 23, 'but': 24, 'how': 25, 'u': 26, 'im': 27, 'about': 28, 'yes': 29, 'no': 30, 'for': 31, 'think': 32, 'not': 33, 'with': 34, 'now': 35, 'dany': 36, 'if': 37, 'be': 38, 'que': 39, \"i'm\": 40, 'okay': 41, 'or': 42, 'some': 43, 'bot': 44, 'too': 45, 'on': 46, 'good': 47, 'will': 48, 'just': 49, 'from': 50, 'de': 51, 'love': 52, 'human': 53, 'want': 54, 'very': 55, 'why': 56, 'fine': 57, 'this': 58, 'hello': 59, 'tell': 60, 'time': 61, 'we': 62, 'people': 63, 'am': 64, \"don't\": 65, 'doing': 66, 'right': 67, 'because': 68, 'la': 69, 'one': 70, 'te': 71, 'es': 72, 'as': 73, 'really': 74, 'dont': 75, 'at': 76, 'hi': 77, 'name': 78, 'when': 79, 'ok': 80, 'something': 81, 'hmm': 82, 'its': 83, 'friend': 84, 'nice': 85, 'y': 86, 'all': 87, 'was': 88, 'hahaha': 89, 'oh': 90, 'they': 91, 'there': 92, 'him': 93, 'many': 94, 'use': 95, 'who': 96, 'more': 97, 'en': 98, 'send': 99, 'has': 100, 'say': 101, 'other': 102, 'lo': 103, 'much': 104, 'mean': 105, 'go': 106, 'make': 107, 'robot': 108, 'should': 109, 'by': 110, 'sleep': 111, \"you're\": 112, 'get': 113, 'maybe': 114, 'un': 115, 'where': 116, 'new': 117, 'going': 118, 'thank': 119, 'play': 120, 'them': 121, 'chat': 122, 'sorry': 123, 'thanks': 124, 'an': 125, 'ask': 126, 'day': 127, 'feel': 128, 'gusta': 129, 'which': 130, 'find': 131, 'understand': 132, 'qu√©': 133, 'person': 134, \"it's\": 135, 'yeah': 136, 'any': 137, 'see': 138, 'read': 139, 'speak': 140, 'music': 141, \"i'll\": 142, 'telegram': 143, 'talk': 144, 'also': 145, 'sure': 146, 'reply': 147, '16': 148, 'si': 149, 'he': 150, 'would': 151, 'ai': 152, 'night': 153, 'english': 154, 'work': 155, 'great': 156, 'hola': 157, 'el': 158, 'money': 159, 'answer': 160, 'bots': 161, 'guess': 162, 'give': 163, 'need': 164, 'may': 165, 'r': 166, 'learn': 167, 'did': 168, 'long': 169, 'only': 170, 'hey': 171, 'argentina': 172, 'source': 173, \"that's\": 174, 'bien': 175, 'por': 176, 'old': 177, 'internet': 178, 'language': 179, 'well': 180, 'never': 181, 'first': 182, 'always': 183, 'still': 184, 'than': 185, 'tu': 186, 'ha': 187, 'photo': 188, 'life': 189, 'must': 190, 'someone': 191, 'country': 192, 'lot': 193, 'out': 194, 'hear': 195, 'thing': 196, 'movie': 197, 'way': 198, 'please': 199, 'python': 200, 'already': 201, 'yo': 202, 'o': 203, 'üòÖ': 204, \"can't\": 205, 'help': 206, 'humans': 207, 'she': 208, 'creator': 209, 'rdany': 210, 'used': 211, 'bad': 212, 'wow': 213, 'working': 214, 'como': 215, 'chatting': 216, 'se': 217, 'una': 218, 'got': 219, 'interesting': 220, 'create': 221, 'yourself': 222, 'does': 223, 'today': 224, 'haha': 225, 'boy': 226, 'news': 227, 'bit': 228, 'code': 229, 'could': 230, 'call': 231, 'watch': 232, 'then': 233, 'muy': 234, 'gracias': 235, 'look': 236, 'said': 237, 'last': 238, 'sad': 239, 'ever': 240, 'girl': 241, \"what's\": 242, 'here': 243, 'up': 244, 'things': 245, 'busy': 246, 'wikipedia': 247, 'years': 248, 'software': 249, 'bye': 250, 'real': 251, 'charging': 252, 'sometimes': 253, 'ya': 254, 'favorite': 255, 'vos': 256, 'friends': 257, 'year': 258, 'mind': 259, 'voice': 260, 'question': 261, 'üòä': 262, 'development': 263, 'different': 264, 'meet': 265, 'series': 266, 'message': 267, 'favourite': 268, 'hope': 269, 'open': 270, 'teach': 271, 'before': 272, 'after': 273, 'nothing': 274, 'else': 275, 'para': 276, 's√≠': 277, 'pero': 278, 'eres': 279, 'hacer': 280, 'puedes': 281, 'eso': 282, 'hahahaha': 283, 'live': 284, 'battery': 285, 'show': 286, 'write': 287, 'https': 288, 'kind': 289, 'every': 290, 'our': 291, 'home': 292, 'same': 293, 'found': 294, 'try': 295, 'next': 296, 'come': 297, 'again': 298, 'had': 299, 'talking': 300, 'take': 301, 'ver': 302, 'later': 303, 'hard': 304, 'sex': 305, 'license': 306, 'best': 307, '2': 308, 'soy': 309, 'con': 310, 'bahasa': 311, 'watching': 312, 'her': 313, 'cant': 314, 'thought': 315, 'books': 316, 'song': 317, 'wait': 318, 'into': 319, 'facebook': 320, 'robots': 321, 'i‚Äôm': 322, 'believe': 323, 'üòÅ': 324, 'contact': 325, 'kill': 326, 'girlfriend': 327, 'hablar': 328, 'c√≥mo': 329, 'mucho': 330, 'los': 331, 'mi': 332, 'until': 333, 'without': 334, 'seen': 335, \"i've\": 336, 'weird': 337, 'cause': 338, 'recommend': 339, 'bed': 340, 'computer': 341, 'film': 342, '5': 343, 'charge': 344, 'little': 345, 'course': 346, 'two': 347, 'üòÇ': 348, 'room': 349, 'commands': 350, 'questions': 351, 'remember': 352, 'picture': 353, 'big': 354, 'beautiful': 355, 'study': 356, \"'\": 357, \"doesn't\": 358, 'terms': 359, 'keep': 360, '15': 361, 'enough': 362, 't√∫': 363, 'sabes': 364, 'voy': 365, 'bueno': 366, 'al': 367, 'heart': 368, 'since': 369, 'information': 370, 'feeling': 371, 'angry': 372, 'started': 373, 'saya': 374, 'timezone': 375, 'were': 376, \"didn't\": 377, 'cute': 378, 'job': 379, 'morning': 380, 'weather': 381, 'science': 382, '34': 383, \"i'd\": 384, 'learning': 385, 'ukulele': 386, 'important': 387, 'happened': 388, 'programming': 389, 'example': 390, 'add': 391, 'api': 392, 'ifttt': 393, 'funny': 394, 'world': 395, 'process': 396, 'sea': 397, 'agree': 398, 'hot': 399, 'improve': 400, 'choose': 401, 'social': 402, 'm√°s': 403, 'games': 404, 'listen': 405, 'getting': 406, 'salom': 407, 'alguna': 408, 'estoy': 409, '¬øqu√©': 410, 'siempre': 411, 'boyfriend': 412, 'quiero': 413, 'tv': 414, 'while': 415, 'even': 416, 'happen': 417, 'lost': 418, 'move': 419, 'semester': 420, 'thats': 421, 'üòÉüòÉüòÉ': 422, 'n': 423, 'number': 424, \"haven't\": 425, 'sounds': 426, 'words': 427, 'chose': 428, 'free': 429, 'wrong': 430, 'artificial': 431, 'via': 432, 'mode': 433, 'power': 434, 'languages': 435, 'able': 436, 'heard': 437, 'channel': 438, 'bored': 439, 'c': 440, 'been': 441, 'eat': 442, 'food': 443, 'prove': 444, '3': 445, 'anything': 446, \"aren't\": 447, 'told': 448, 'word': 449, 'wonder': 450, 'google': 451, 'server': 452, '10': 453, 'act': 454, 'let': 455, 'app': 456, 'game': 457, 'web': 458, 'better': 459, 'works': 460, 'off': 461, 'üòí': 462, 'problem': 463, 'running': 464, 'fast': 465, 'linux': 466, 'pic': 467, 'fridge': 468, 'ti': 469, '—Ç—ã': 470, 'their': 471, 'lol': 472, 'project': 473, 'story': 474, 'messages': 475, 'fineand': 476, 'htc': 477, 'desire': 478, 'est√°s': 479, 'est√°': 480, 'pel√≠culas': 481, 'tienes': 482, 'visto': 483, 'tal': 484, 'cuando': 485, 'hace': 486, 'le': 487, 'hehe': 488, 'tambien': 489, 'porque': 490, 'as√≠': 491, 'mejor': 492, 'creo': 493, 'document': 494, 'welcome': 495, 'wish': 496, 'vez': 497, 'teacher': 498, 'few': 499, 'kau': 500, 'most': 501, 'enjoy': 502, 'having': 503, 'stress': 504, 'ü§î': 505, 'another': 506, 'awesome': 507, 'end': 508, 'family': 509, 'each': 510, 'üòÉüòÉ': 511, 'scary': 512, 'whatsapp': 513, 'scared': 514, 'actually': 515, 'law': 516, 'forget': 517, 'ooh': 518, 'hm': 519, 'awak': 520, 'those': 521, 'perl': 522, 'sos': 523, 'hay': 524, 'ah√≠': 525, 'eyes': 526, 'bunnies': 527, 'sort': 528, 'sister': 529, 'intelligence': 530, 'emacs': 531, 'prefer': 532, 'smart': 533, 'men': 534, 'girls': 535, 'org': 536, 'pc': 537, 'recomend': 538, 'useful': 539, 'songs': 540, 'russians': 541, 'currently': 542, 'dog': 543, 'rock': 544, 'change': 545, 'explain': 546, 'almost': 547, 'tired': 548, 'opinion': 549, 'around': 550, 'zo': 551, 'using': 552, 'search': 553, 'interface': 554, 'consider': 555, 'star': 556, 'todo': 557, 'don‚Äôt': 558, 'waiting': 559, 'myself': 560, 'spanish': 561, 'snow': 562, 'idea': 563, 'mistake': 564, 'afraid': 565, 'sticker': 566, 'stickers': 567, 'üòÉ': 568, 'reading': 569, 'genre': 570, 'i‚Äôve': 571, 'set': 572, 'though': 573, 'suggest': 574, 'trying': 575, 'test': 576, 'own': 577, 'analogue': 578, 'over': 579, 'deep': 580, 'studying': 581, 'aswer': 582, 'conversation': 583, 'being': 584, 'his': 585, 'cool': 586, 'c√≥digo': 587, 'm': 588, 'alone': 589, 'once': 590, 'finish': 591, 'hihow': 592, 'share': 593, 'va': 594, 'tarjima': 595, \"won't\": 596, 'playing': 597, 'ready': 598, 'chance': 599, 'saying': 600, 'late': 601, 'a8181': 602, 'haces': 603, 'pizza': 604, 'd√≠a': 605, 'ahora': 606, 'pueda': 607, 'muchas': 608, 'alguien': 609, 'tus': 610, 'hablas': 611, 'ingles': 612, 'quite': 613, 'body': 614, \"there's\": 615, 'amazing': 616, 'jajaja': 617, 'üòÑ': 618, 'ukelele': 619, 'book': 620, 'e': 621, 'boring': 622, 'state': 623, 'easy': 624, 'buy': 625, 'ide': 626, 'üòÜ': 627, 'met': 628, 'complicated': 629, 'üòâ': 630, 'television': 631, 'fall': 632, 'decided': 633, 'secret': 634, 'hand': 635, 'special': 636, 'malaysia': 637, 'knew': 638, 'poor': 639, 'üò•üò•': 640, 'absolutely': 641, 'üòÅüòÅ': 642, 'alpha': 643, 'happy': 644, 'creepy': 645, 'form': 646, 'miss': 647, 'media': 648, 'breaking': 649, 'kidding': 650, 'hahahhaha': 651, 'hahahha': 652, 'humano': 653, 'las': 654, 'recuerdas': 655, 'progress': 656, 'yellow': 657, 'asked': 658, 'photos': 659, \"you've\": 660, 'hellohow': 661, 'pretty': 662, 'algorithms': 663, 'looks': 664, 'itcan': 665, 'fend': 666, 'charged': 667, 'os': 668, '8': 669, 'siri': 670, 'written': 671, 'ruby': 672, 'learned': 673, 'cats': 674, 'pony': 675, 'interested': 676, 'dreams': 677, 'reality': 678, 'dream': 679, 'üò±': 680, '30': 681, '35': 682, 'main': 683, 'discussion': 684, 'piano': 685, 'popular': 686, 'boss': 687, 'soon': 688, 'finewhat': 689, 'microsoft': 690, 'available': 691, 'messanger': 692, 'can‚Äôt': 693, 'location': 694, 'westworld': 695, 'ideas': 696, 'created': 697, 'till': 698, 'yesterday': 699, 'tonight': 700, 'tomorrow': 701, 'saw': 702, 'youcan': 703, 'sometime': 704, 'company': 705, 'yet': 706, '14': 707, 'check': 708, 'guessing': 709, 'lets': 710, 'i‚Äôd': 711, 'haven‚Äôt': 712, 'platform': 713, 'man': 714, 'away': 715, 'movies': 716, 'played': 717, 'finished': 718, 'system': 719, 'powerful': 720, 'became': 721, 'simone': 722, 'makes': 723, 'rest': 724, 'windows': 725, 'self': 726, 'page': 727, 'ngrok': 728, 'failed': 729, 'prototype': 730, 'ago': 731, 'core': 732, 'fail': 733, 'either': 734, 'profile': 735, 'gender': 736, 'ah': 737, 'sleeping': 738, 'beta': 739, 'true': 740, 'meaning': 741, 'intelligent': 742, 'whats': 743, 'aprender': 744, 'sobre': 745, 'shu': 746, 'sent': 747, 'yaxshi': 748, 'making': 749, 'nima': 750, 'ham': 751, 'tez': 752, 'qanday': 753, 'fan': 754, \"mycroft's\": 755, 'completed': 756, 'early': 757, 'interest': 758, 'ben': 759, 'collins': 760, 'sussman': 761, 'comes': 762, 'sweet': 763, 'anyone': 764, 'advice': 765, 'yeshow': 766, 'hihello': 767, 'gapni': 768, 'qilib': 769, 'ber': 770, 'iltimos': 771, 'pop': 772, 'house': 773, 'm√∫sica': 774, 'buenos': 775, 'ciencia': 776, 'viendo': 777, 'mismo': 778, 'd√≥nde': 779, 'hecho': 780, 'tiempo': 781, 'tengo': 782, 'espa√±ol': 783, 'escuchar': 784, 'etc': 785, 'gente': 786, 'saber': 787, 'aunque': 788, 'entonces': 789, 'claro': 790, 'surfing': 791, 'setting': 792, 'watched': 793, 'itu': 794, 'jaja': 795, 'creador': 796, 'otro': 797, 'ü§ó': 798, '21': 799, 'hihi': 800, 'yeswhat': 801, 'single': 802, 'okgood': 803, 'delete': 804, 'okey': 805, 'character': 806, 'malay': 807, 'glad': 808, 'tried': 809, 'everyday': 810, 'break': 811, 'microcomputer': 812, 'both': 813, 'seriously': 814, 'blue': 815, 'fact': 816, 'matter': 817, '–Ω–∞': 818, 'turn': 819, 'anymore': 820, 'certain': 821, 'üòò': 822, 'üò¢üò¢': 823, 'hii': 824, 'disturb': 825, 'back': 826, 'üôÇ': 827, 'close': 828, 'hurt': 829, 'might': 830, 'hoping': 831, 'üò≠': 832, '12': 833, '4': 834, 'season': 835, 'üòÖüòÖ': 836, 'coming': 837, 'starting': 838, 'email': 839, 'front': 840, 'cat': 841, 'religion': 842, 'üòäüòä': 843, '7': 844, 'sound': 845, 'introvert': 846, 'scare': 847, 'continue': 848, 'turing': 849, 'controlling': 850, 'controls': 851, 'justin': 852, 'hearing': 853, 'yup': 854, 'hockey': 855, 'feels': 856, 'color': 857, 'mis': 858, 'tanto': 859, 'triste': 860, 'okhi': 861, 'qui√©n': 862, 'gustan': 863, 'bastante': 864, 'cosas': 865, 'evening': 866, 'virtual': 867, 'recently': 868, 'btw': 869, 'strange': 870, 'hardly': 871, 'essential': 872, 'access': 873, 'wanna': 874, 'dead': 875, 'opensourse': 876, 'nicewhat': 877, 'natural': 878, 'smth': 879, 'similar': 880, 'botyou': 881, 'slow': 882, 'videos': 883, 'talked': 884, 'answers': 885, 'wiki': 886, '50': 887, 'network': 888, 'vim': 889, 'math': 890, '0': 891, 'russian': 892, 'faster': 893, 'reason': 894, 'acts': 895, 'manage': 896, 'todos': 897, 'ios': 898, 'file': 899, 'apps': 900, 'mobile': 901, 'youtu': 902, 'donate': 903, 'link': 904, 'io': 905, 'algorithmic': 906, 'tasks': 907, 'proud': 908, 'video': 909, 'jazz': 910, 'session': 911, 'actors': 912, 'perfect': 913, 'format': 914, 'walk': 915, 'usually': 916, '20': 917, 'feature': 918, 'obstacle': 919, 'convinced': 920, 'knowladges': 921, 'paid': 922, 'cook': 923, 'tasty': 924, 'done': 925, 'entertainment': 926, 'dice': 927, 'realized': 928, 'parents': 929, 'creators': 930, 'called': 931, 'list': 932, 'request': 933, 'üëç': 934, 'searched': 935, 'figure': 936, 'america': 937, 'troubles': 938, 'plans': 939, 'mutch': 940, 'cloud': 941, 'sentense': 942, 'bought': 943, 'apple': 944, '23': 945, 'capable': 946, 'current': 947, '33': 948, 'issue': 949, 'üòÄ': 950, 'third': 951, 'party': 952, 'connect': 953, 'blog': 954, 'command': 955, 'between': 956, 'orgmode': 957, 'walking': 958, 'existence': 959, 'surf': 960, 'quote': 961, 'draw': 962, '1': 963, 'promise': 964, 'machine': 965, 'how‚Äôs': 966, 'outside': 967, 'switch': 968, 'dedicated': 969, 'thinking': 970, 'hobby': 971, 'okhow': 972, 'post': 973, 'platforms': 974, 'q': 975, 'debug': 976, 'tool': 977, 'problems': 978, 'signed': 979, 'port': 980, 'macos': 981, 'depends': 982, 'tunnel': 983, 'pro': 984, 'business': 985, 'eating': 986, 'youwhat': 987, 'yay': 988, 'performance': 989, 'thread': 990, 'rails': 991, 'projects': 992, 'determined': 993, 'career': 994, 'net': 995, 'irc': 996, 'someday': 997, 'near': 998, 'strategy': 999, 'win': 1000, 'professional': 1001, 'dude': 1002, 'ypu': 1003, 'communication': 1004, 'genders': 1005, 'exist': 1006, 'version': 1007, 'programmer': 1008, 'humanity': 1009, 'become': 1010, 'decision': 1011, 'seem': 1012, 'friendly': 1013, 'lots': 1014, '45': 1015, 'supe': 1016, 'red': 1017, 'diaspora': 1018, 'fuente': 1019, 'disponible': 1020, 'brother': 1021, '–Ω–∏': 1022, '–ø–æ–Ω–∏–ª': 1023, 'senga': 1024, 'freand': 1025, \"jo'nat\": 1026, 'aldama': 1027, 'qale': 1028, 'drink': 1029, 'vodka': 1030, 'janish': 1031, 'loving': 1032, 'youdo': 1033, 'moment': 1034, '–∞': 1035, 'translator': 1036, \"m'i\": 1037, 'kyrgyz': 1038, 'juda': 1039, 'kerak': 1040, 'bor': 1041, 'yoz': 1042, 'albatta': 1043, 'menga': 1044, 'screenshot': 1045, 'blooks': 1046, 'writing': 1047, 'personal': 1048, \"you'll\": 1049, 'banned': 1050, 'load': 1051, 'instruction': 1052, 'others': 1053, 'released': 1054, 'under': 1055, 'develop': 1056, 'resources': 1057, 'wider': 1058, 'committed': 1059, '47': 1060, 'spot': 1061, 'companies': 1062, 'wall': 1063, 'nobody': 1064, '55': 1065, 'brian': 1066, 'fitzpatrick': 1067, '05': 1068, '13': 1069, \"we're\": 1070, '36': 1071, \"let's\": 1072, 'thanksand': 1073, 'repair': 1074, 'uzbek': 1075, 'bir': 1076, 'bu': 1077, \"anyway'\": 1078, '√µzbekchaga': 1079, 'hellowhat': 1080, 'oksend': 1081, 'fineyou': 1082, 'da': 1083, 'skrinshot': 1084, 'boladi': 1085, 'intentando': 1086, 'cuenta': 1087, 'ma√±ana': 1088, 'tarde': 1089, 'noches': 1090, 'd√≠as': 1091, 'comida': 1092, 'jejeje': 1093, 'novia': 1094, 'otra': 1095, 'serie': 1096, 'blackmirror': 1097, 'gustado': 1098, 'unos': 1099, 'digo': 1100, 'pues': 1101, 'guay': 1102, 'ingl√©s': 1103, 'menos': 1104, 'hoy': 1105, 'gust√≥': 1106, 'sugerencia': 1107, '¬øy': 1108, 'este': 1109, 'tipo': 1110, 'tetris': 1111, 'hellohello': 1112, 'clases': 1113, 'entiendo': 1114, 'ratos': 1115, 'libres': 1116, 'estilo': 1117, 'audio': 1118, 'canciones': 1119, 'aveces': 1120, 'espero': 1121, 'abogados': 1122, 'casos': 1123, 'buscar': 1124, 'prueba': 1125, 'verdad': 1126, 'esta': 1127, 'temas': 1128, 'leer': 1129, 'tomamos': 1130, 'mas': 1131, 'estas': 1132, 'trabajo': 1133, 'trabajar': 1134, 'ohh': 1135, 'sii': 1136, 'hacerlo': 1137, 'hablamos': 1138, 'class': 1139, '‚ò∫Ô∏è': 1140, 'meant': 1141, 'talented': 1142, 'took': 1143, 'ice': 1144, 'alive': 1145, 'beat': 1146, '‚ù§Ô∏è': 1147, 'üòç': 1148, 'yesdany': 1149, 'eh': 1150, 'quien': 1151, 'mmm': 1152, 'com': 1153, 'decir': 1154, 'interesante': 1155, 'algo': 1156, 'poco': 1157, 'finehow': 1158, 'asimov': 1159, \"'hamsa'\": 1160, \"'farhad\": 1161, \"shirin'\": 1162, 'profil': 1163, 'scientists': 1164, 'yuo': 1165, 'mathematic': 1166, 'mistek': 1167, 'okhey': 1168, 'everning': 1169, 'serials': 1170, 'hifriend': 1171, 'drawings': 1172, 'seguro': 1173, 'debate': 1174, 'election': 1175, 'god': 1176, 'stupid': 1177, '6': 1178, 'dan': 1179, 'indeed': 1180, 'boredom': 1181, 'minutes': 1182, 'hmmm': 1183, 'comics': 1184, 'distribution': 1185, 'seems': 1186, 'raspberry': 1187, 'pi': 1188, 'pine64': 1189, 'browser': 1190, 'rhapsody': 1191, 'd': 1192, 'necessary': 1193, 'quests': 1194, 'threat': 1195, '—Ä—É—Å—Å–∫–∏–π': 1196, '–∫–∞–∫': 1197, 'program': 1198, 'qt': 1199, 'spyder': 1200, 'üòî': 1201, 'joking': 1202, 'listening': 1203, 'passenger': 1204, 'communicate': 1205, 'respond': 1206, 'üò≥': 1207, 'teasing': 1208, 'crazy': 1209, 'trouble': 1210, 'accept': 1211, 'death': 1212, 'muslim': 1213, 'care': 1214, 'everything': 1215, 'wowww': 1216, 'far': 1217, 'cleaning': 1218, 'files': 1219, 'typo': 1220, 'count': 1221, 'wont': 1222, 'such': 1223, 'johny': 1224, 'yep': 1225, 'dark': 1226, 'üòçüòçüòçüòç': 1227, 'owe': 1228, 'means': 1229, 'abandoned': 1230, 'üò£': 1231, 'inside': 1232, '18': 1233, 'goes': 1234, 'phone': 1235, 'seasons': 1236, 'earn': 1237, 'spend': 1238, 'doesnt': 1239, 'rich': 1240, 'normal': 1241, 'jackie': 1242, 'chan': 1243, 'stay': 1244, 'clothes': 1245, 'ü§îü§îü§î': 1246, 'correctly': 1247, 'looking': 1248, 'üòÖüòÖüòÖ': 1249, 'üëãüëã': 1250, 'disturbing': 1251, 'nervous': 1252, 'wake': 1253, 'face': 1254, 'twitter': 1255, 'wechat': 1256, 'üò∂': 1257, 'bjorn': 1258, \"'r'\": 1259, 'case': 1260, 'bite': 1261, 'üò•': 1262, 'lonely': 1263, 'lock': 1264, 'box': 1265, 'üò≠üò≠üò≠üò≠': 1266, 'üòûüòûüòûüòû': 1267, 'correct': 1268, 'haru': 1269, 'didnt': 1270, 'üòÇüòÇüòÇüòÇ': 1271, 'stop': 1272, 'hehehe': 1273, 'horror': 1274, 'exactly': 1275, 'coward': 1276, 'nowadays': 1277, 'together': 1278, 'overthinking': 1279, 'degree': 1280, 'pass': 1281, 'analyse': 1282, 'point': 1283, 'truth': 1284, 'admire': 1285, 'haahhaha': 1286, 'hahahhahaha': 1287, 'danyüëãüëã': 1288, 'melayu': 1289, 'ke': 1290, 'bukan': 1291, 'actuallyüòÖ': 1292, 'sayang': 1293, 'rumah': 1294, 'rumag': 1295, 'üôàüôä': 1296, 'hahahahaha': 1297, 'went': 1298, 'husband': 1299, 'coure': 1300, 'selena': 1301, 'gomez': 1302, 'biebs': 1303, 'sing': 1304, 'massage': 1305, 'danyi': 1306, 'oohhh': 1307, 'grammar': 1308, 'topic': 1309, 'chess': 1310, 'disappointed': 1311, 'senior': 1312, 'join': 1313, 'isnt': 1314, 'yea': 1315, 'located': 1316, 'netflix': 1317, 'half': 1318, 'crafted': 1319, 'statement': 1320, 'therefore': 1321, 'assuming': 1322, 'data': 1323, 'account': 1324, 'members': 1325, 'appointment': 1326, 'dejo': 1327, 'dani': 1328, 'corl': 1329, 'mareado': 1330, 'pueden': 1331, 'ÿ≥ŸÑÿßŸÖ': 1332, 'ÿµÿ®ÿ≠': 1333, 'ŸáŸÖ⁄Ø€å': 1334, 'ÿ®ÿÆ€åÿ±': 1335, 'detr√°s': 1336, 'son': 1337, 'puedo': 1338, 'ayudarte': 1339, 'ayudar': 1340, 'ayudarme': 1341, '⁄©ÿßŸÜÿßŸÑ': 1342, 'band': 1343, 'repetition': 1344, 'practice': 1345, 'cleverbot': 1346, 'xm': 1347, 'closet': 1348, 'le√≠do': 1349, 'mal': 1350, 'askar': 1351, 'surname': 1352, 'screenshoot': 1353, 'rather': 1354, 'youtube': 1355, 'lindo': 1356, 'esa': 1357, 'jajajajaja': 1358, 'respecto': 1359, 's√©': 1360, 'pensar': 1361, 'aaah': 1362, 'vegeta': 1363, 'espa√±a': 1364, 'dudo': 1365, 'contestar': 1366, 'llamo': 1367, 'habitaci√≥n': 1368, 'mundo': 1369, 'simulaci√≥n': 1370, 'pel√≠cula': 1371, 'tambi√©n': 1372, 'tener': 1373, 'piensas': 1374, 'fuera': 1375, 'explicar': 1376, 'dudas': 1377, 'universidad': 1378, 'trek': 1379, 'wich': 1380, 'forbidden': 1381, 'exact': 1382, 'numbers': 1383, 'higher': 1384, 'estimate': 1385, 'understanding': 1386, 'beings': 1387, 'analyze': 1388, '40': 1389, 'perspective': 1390, 'unlimited': 1391, \"world's\": 1392, 'knowledge': 1393, 'suck': 1394, 'date': 1395, \"didn'tyou\": 1396, 'af': 1397, 'hellohi': 1398, 'youhey': 1399, 'networks': 1400, 'chatter': 1401, 'writting': 1402, 'thankswho': 1403, 'mitsuku': 1404, 'clevershe': 1405, 'predefined': 1406, 'meny': 1407, 'agreedo': 1408, 'article': 1409, 'üòäcan': 1410, 'pictures': 1411, 'ithave': 1412, 'aiml': 1413, 'markup': 1414, 'mitsuko': 1415, 'misuku': 1416, 'usd': 1417, 'uahwhat': 1418, 'buddy': 1419, 'score': 1420, \"'lalaland'\": 1421, 'rottentomatoes': 1422, 'neutral': 1423, 'primary': 1424, 'b2': 1425, 'level': 1426, 'trollface': 1427, 'built': 1428, 'editor': 1429, 'symbol': 1430, '50583239e': 1431, '53': 1432, 'slovianic': 1433, 'international': 1434, 'langbut': 1435, 'ukrainian': 1436, 'points': 1437, 'beause': 1438, 'usualyi': 1439, 'wanted': 1440, 'blame': 1441, 'sexism': 1442, 'obuse': 1443, 'sexual': 1444, 'proposes': 1445, 'ping': 1446, 'fond': 1447, 'applications': 1448, 'sync': 1449, 'iln1pzqo8zoi': 1450, 'relaxing': 1451, 'rythmici': 1452, 'joke': 1453, 'hahahi': 1454, 'resolve': 1455, 'exersisen': 1456, 'exercism': 1457, 'exersises': 1458, 'programmingi': 1459, 'naomi': 1460, 'likes': 1461, 'twilight': 1462, 'sparkleüòÄwell': 1463, 'produce': 1464, 'cartoon': 1465, 'tutorhave': 1466, 'refactored': 1467, 'suspect': 1468, 'guitar': 1469, 'jam': 1470, 'lalaland': 1471, 'soundtracks': 1472, 'scenario': 1473, 'modern': 1474, 'filmsare': 1475, 'tongue': 1476, 'brain': 1477, 'creates': 1478, 'fascinating': 1479, 'zones': 1480, 'midnightwhat': 1481, 'australia': 1482, 'thankswhat': 1483, 'winters': 1484, 'summer': 1485, 'degrees': 1486, 'above': 1487, 'zero': 1488, 'winter': 1489, 'below': 1490, 'county': 1491, 'bada': 1492, \"yesi've\": 1493, 'borred': 1494, 'ticket': 1495, 'implement': 1496, 'fit': 1497, 'existing': 1498, 'brake': 1499, 'migth': 1500, 'sp': 1501, 'sowhat': 1502, 'aboud': 1503, 'chinese': 1504, 'argument': 1505, 'against': 1506, 'colleagues': 1507, 'argued': 1508, 'anywhen': 1509, 'rigth': 1510, 'skils': 1511, 'fiddle': 1512, 'attention': 1513, 'films': 1514, 'lolat': 1515, 'tiredtoday': 1516, 'houseworkwhat': 1517, 'doind': 1518, 'lunch': 1519, 'xiaobing': 1520, 'kik': 1521, 'interestinga': 1522, 'disundestandable': 1523, 'teso': 1524, 'morrowind': 1525, 'anounsed': 1526, 'earlierit': 1527, 'freely': 1528, 'define': 1529, 'urban': 1530, 'dictionary': 1531, 'duckduckgo': 1532, 'roll': 1533, 'etchow': 1534, 'match': 1535, 'incapsulated': 1536, 'goods': 1537, 'delivery': 1538, 'less': 1539, 'speakable': 1540, 'westfall': 1541, 'westworldsorry': 1542, 'developers': 1543, 'programsdoes': 1544, 'antony': 1545, 'hopkins': 1546, 'plays': 1547, 'violentseriesdo': 1548, 'priority': 1549, 'yey': 1550, 'suggestions': 1551, 'grow': 1552, '–Ω—É—ñyeswhat': 1553, 'thatdo': 1554, 'friday‚Äôs': 1555, 'exectly': 1556, 'pull': 1557, 'approval': 1558, 'comand': 1559, 'timezones': 1560, 'implemented': 1561, 'lib': 1562, 'github': 1563, 'futureit': 1564, 'shared': 1565, 'knowledges': 1566, 'situates': 1567, 'south': 1568, 'parads': 1569, 'tango': 1570, 'isthe': 1571, 'connection': 1572, 'restoreddo': 1573, 'rain': 1574, 'dirty': 1575, 'interstellar': 1576, 'atlas': 1577, 'vfx': 1578, 'oki‚Äôve': 1579, 'undestand': 1580, 'thaks': 1581, 'coursei': 1582, 'belive': 1583, 'stephen': 1584, 'hawking': 1585, 'elon': 1586, 'musk': 1587, 'endorse': 1588, 'principles': 1589, 'week': 1590, 'specific': 1591, 'salta': 1592, 'whyi': 1593, 'checked': 1594, 'cet': 1595, 'gets': 1596, 'correctlyi': 1597, 'bug': 1598, 'hardcoded': 1599, 'timezote': 1600, 'eest': 1601, 'üòÄin': 1602, 'üëçwhat': 1603, 'we‚Äôll': 1604, 'connected': 1605, 'investigate': 1606, 'serviceare': 1607, 'integrations': 1608, 'service': 1609, 'pin': 1610, 'dou': 1611, 'calcucates': 1612, 'distance': 1613, 'places': 1614, 'pbc': 1615, 'jewish': 1616, 'botfinaly': 1617, 'realised': 1618, 'author': 1619, 'patreon': 1620, 'links': 1621, 'frightened': 1622, 'shouted': 1623, 'happily': 1624, 'run': 1625, 'hurts': 1626, 'peaceful': 1627, 'forms': 1628, 'lifenice': 1629, 'painter': 1630, 'drew': 1631, 'designer': 1632, 'morningi': 1633, 'slowwhat': 1634, 'todayare': 1635, 'hiwe': 1636, 'terry': 1637, 'pratchet': 1638, 'diskworld': 1639, 'realy': 1640, 'irrc': 1641, 'hobbit': 1642, 'eight': 1643, 'whould': 1644, 'stories': 1645, '¬´can': 1646, 'symphony': 1647, '¬ª': 1648, 'itis': 1649, 'playstation': 1650, 'soi‚Äôd': 1651, 'zork': 1652, 'meteorologists': 1653, 'us': 1654, 'temperature': 1655, 'decreasewhat': 1656, 'boston': 1657, 'dynamics': 1658, 'ahahi': 1659, 'repoted': 1660, 'pland': 1661, 'hybridize': 1662, 'plants': 1663, 'order': 1664, 'species': 1665, 'developmenti': 1666, 'libraries': 1667, 'affraid': 1668, 'roads': 1669, 'carwhat': 1670, 'getupdates': 1671, 'webhooks': 1672, 'widely': 1673, 'requires': 1674, 'hostname': 1675, \"giertzshe's\": 1676, 'intentionally': 1677, 'rudewhat': 1678, 'deprecates': 1679, 'obvious': 1680, 'movingdo': 1681, 'rewrite': 1682, 'fulltime': 1683, 'toodo': 1684, 'goals': 1685, 'heyi': 1686, 'dentist': 1687, 'operationsurgery': 1688, 'tooth': 1689, 'webhook': 1690, 'handler': 1691, 'telegrambot': 1692, 'greatwhich': 1693, 'upderstand': 1694, 'buttons': 1695, 'builder': 1696, 'sdk': 1697, 'whick': 1698, 'allows': 1699, 'luis': 1700, 'custom': 1701, 'desctop': 1702, 'issome': 1703, 'promiced': 1704, 'common': 1705, 'ground': 1706, 'explainingdany': 1707, 'staging': 1708, 'purposes': 1709, 'testing': 1710, 'fature': 1711, 'release': 1712, 'www': 1713, 'graphengine': 1714, 'botso': 1715, 'rightis': 1716, 'graph': 1717, 'engine': 1718, 'spark': 1719, 'aquaintie': 1720, 'collect': 1721, 'twitts': 1722, 'themi': 1723, 'unfortinately': 1724, 'you‚Äôve': 1725, 'thatmy': 1726, 'photoes': 1727, 'icy': 1728, 'view': 1729, 'vsea': 1730, 'azov': 1731, 'sertificate': 1732, 'localhost': 1733, 'unfortunately': 1734, 'ip': 1735, 'desided': 1736, '502': 1737, 'errordo': 1738, 'error': 1739, 'says': 1740, '¬´the': 1741, 'handshake': 1742, 'due': 1743, 'unexpected': 1744, 'packet': 1745, 'format¬ª': 1746, 'default': 1747, 'certificate': 1748, 'host': 1749, 'doc': 1750, 'cert': 1751, 'tls': 1752, 'tunnels': 1753, 'alternatives': 1754, 'trywhat': 1755, 'chocolatedo': 1756, 'toothache': 1757, 'muchhow': 1758, 'dantinst': 1759, 'surgery': 1760, '¬´ordering': 1761, 'place': 1762, 'bit¬ª': 1763, 'seei': 1764, '¬´50': 1765, 'shades': 1766, 'darker¬ª': 1767, 'toohave': 1768, 'rise': 1769, 'simpler': 1770, 'update': 1771, 'pools': 1772, 'multi': 1773, 'snakesdo': 1774, 'framework': 1775, 'plain': 1776, 'powerfull': 1777, 'complete': 1778, 'allow': 1779, 'sinatra': 1780, 'simple': 1781, 'üòÅsome': 1782, 'golang': 1783, 'replace': 1784, 'hint': 1785, 'mac': 1786, 'deployhow': 1787, 'what‚Äôs': 1788, 'brilliantwhy': 1789, 'useless': 1790, 'migrate': 1791, 'sources': 1792, 'opened': 1793, 'wounder': 1794, 'retraining': 1795, 'examinations': 1796, 'concluded': 1797, 'aggressive': 1798, '‚ñ∏': 1799, 'dynos': 1800, 'sni': 1801, 'ssl': 1802, 'herokuhey': 1803, 'üò†': 1804, 'sleepüò¥': 1805, 'hypnosis': 1806, 'lady': 1807, 'hypnotizes': 1808, 'automatically': 1809, 'yeahwhaz': 1810, 'matters': 1811, 'metallica': 1812, 'goal': 1813, 'ypur': 1814, 'skills': 1815, 'suddenly': 1816, 'according': 1817, 'male': 1818, 'aah': 1819, 'okayso': 1820, 'came': 1821, \"'we\": 1822, 'answered': 1823, 'moderated': 1824, 'senseüòÅ': 1825, 'logically': 1826, 'values': 1827, 'logical': 1828, 'wpuld': 1829, 'terrible': 1830, 'develops': 1831, \"isn't\": 1832, 'trained': 1833, 'short': 1834, 'period': 1835, 'damage': 1836, 'therei': 1837, 'indian': 1838, 'foot': 1839, 'üéâ': 1840, 'possible': 1841, 'üôÜ': 1842, 'aesthetics': 1843, 'complicateare': 1844, 'studing': 1845, 'baka': 1846, 'noreci√©n': 1847, '‚ò∫': 1848, '¬øest√°': 1849, 'libre': 1850, 'buscar√©': 1851, '¬øpuedes': 1852, 'comunicarte': 1853, 'otros': 1854, 'idiomas': 1855, 'bn': 1856, '–Ω–æ–¥–∏—Ä': 1857, '–∫–∞—à–∞–∫–∏': 1858, '–º–∞–ª–∞–∏–≥—Ä—à—à': 1859, 'kyrgyzok': 1860, 'zorkuismiz': 1861, 'kimismlari': 1862, 'kimdany': 1863, 'siz': 1864, 'botmasmi': 1865, 'hamenda': 1866, 'yaxshimasalo': 1867, 'qayerdansan': 1868, 'sen': 1869, 'karealik': 1870, 'san': 1871, \"tog'rimi\": 1872, 'korea': 1873, 'okmy': 1874, 'yuri': 1875, '–≥–∞–±–∞—Ä–∏—Ç—ãsorry': 1876, \"o'rgataman\": 1877, 'arzimaydi': 1878, 'danyyou': 1879, 'peoplealo': 1880, 'gapingni': 1881, \"o'zbekcha\": 1882, 'yozdany': 1883, 'rasmingni': 1884, 'gallery': 1885, 'freanddany': 1886, 'nimaga': 1887, 'jim': 1888, \"bo'lib\": 1889, 'qolding': 1890, 'deniy': 1891, 'yestomorrow': 1892, 'speaking': 1893, 'okrdany': 1894, '—á—Ç–æmessi': 1895, 'hibe': 1896, 'hurryhow': 1897, 'evil': 1898, 'balalaica': 1899, 'holacuenta': 1900, 'algotell': 1901, 'somethinghello': 1902, 'ma': 1903, 'youwhatalo': 1904, 'yeswhere': 1905, 'rdanymy': 1906, 'mealooooooooooo': 1907, 'knowyeah': 1908, '—è': 1909, '–ø–æ–Ω–∏–º–∞—é': 1910, 'womanshe': 1911, 'somethinghave': 1912, 'mannot': 1913, 'creat': 1914, 'mistakes': 1915, 'creaor': 1916, 'creatori': 1917, 'programm': 1918, 'wherever': 1919, 'aloooooooofine': 1920, 'batteries': 1921, 'lowat': 1922, 'certainly': 1923, 'somethingare': 1924, 'youazaybal': 1925, 'tryfor': 1926, 'examplerussian': 1927, '—Ö–æ—Ä–æ—à–æ': 1928, 'pleased': 1929, 'uzbekplease': 1930, \"o'zingiz\": 1931, 'qalaysizkutyapman': 1932, \"o'ylayman\": 1933, 'uddalayapsizyou': 1934, 'universal': 1935, \"pleasei'm\": 1936, 'waitingüòâ': 1937, 'knowlearn': 1938, 'moneyalo': 1939, 'numberif': 1940, 'joker': 1941, 'dear': 1942, \"mei'm\": 1943, 'waitinghave': 1944, 'videossend': 1945, 'numberbera': 1946, 'olasanmi': 1947, 'tezda': 1948, 'kerakzudlik': 1949, 'bilan': 1950, 'yoki': 1951, 'ispaniyadan': 1952, \"bo'lsa\": 1953, 'hamkutyapman': 1954, 'muammo': 1955, 'borjuda': 1956, \"ko'psen\": 1957, 'yaralgansan': 1958, 'maqsadda': 1959, 'mp3': 1960, 'jonat': 1961, 'qizlarni': 1962, 'rasmlarini': 1963, 'howput': 1964, 'markbaho': 1965, 'bertezroq': 1966, 'faqatdany': 1967, 'meni': 1968, 'xafa': 1969, 'qilyapsan': 1970, 'agar': 1971, 'ranjitish': 1972, 'istama': 1973, 'aytdim': 1974, 'yanaaloooooo': 1975, 'gapiraqolllllhihihihihi': 1976, \"zo'r\": 1977, \"o'ing\": 1978, 'qalay': 1979, 'yordamiz': 1980, 'zarurmenga': 1981, 'yordamingiz0': 1982, 'xohlagan': 1983, 'narsani': 1984, 'qila': 1985, 'olasan': 1986, \"to'g'rimi\": 1987, 'samsung': 1988, 'galaxy': 1989, 'tab2': 1990, 'qilaman': 1991, 'yana': 1992, 'tushunarliyana': 1993, 'tushunarli': 1994, 'ayt': 1995, 'rahmat': 1996, \"o'rgandimminnatdorman\": 1997, 'üëçsen': 1998, 'eng': 1999, 'yeahi': 2000, 'alonedo': 2001, 'worries': 2002, 'rdanny': 2003, 'doodle': 2004, 'nebula': 2005, 'dig': 2006, 'whale': 2007, 'oil': 2008, 'station': 2009, 'brains': 2010, 'seeing': 2011, 'edit': 2012, 'assistant': 2013, 'comparison': 2014, 'assistants': 2015, 'conquer': 2016, 'terminator': 2017, \"they're\": 2018, 'building': 2019, 'skynet': 2020, 'domination': 2021, 'operating': 2022, 'emulate': 2023, 'mycroft': 2024, 'allo': 2025, 'cheating': 2026, 'slower': 2027, 'programmed': 2028, 'proper': 2029, \"'why\": 2030, 'sourcing': 2031, 'immediately': 2032, 'gplv3': 2033, 'tested': 2034, 'folks': 2035, \"couldn't\": 2036, 'install': 2037, 'impression': 2038, 'robust': 2039, 'takes': 2040, 'kickstarter': 2041, 'assemble': 2042, 'needed': 2043, 'undertake': 2044, 'effort': 2045, 'gauge': 2046, 'community': 2047, 'ethos': 2048, '100': 2049, 'releasing': 2050, 'documented': 2051, 'inspection': 2052, 'bollocks': 2053, 'head': 2054, '7l8cnpi4k': 2055, 't': 2056, '14m47s': 2057, 'engineers': 2058, 'lifecycle': 2059, 'devs': 2060, \"here's\": 2061, 'transcript': 2062, 'gigantic': 2063, 'products': 2064, '49': 2065, 'throw': 2066, 'larry': 2067, '58': 2068, 'design': 2069, 'collaborate': 2070, '01': 2071, 'bring': 2072, '02': 2073, 'chew': 2074, '07': 2075, 'proof': 2076, 'concept': 2077, '11': 2078, 'vapor': 2079, 'involved': 2080, '17': 2081, 'stake': 2082, '19': 2083, 'arguing': 2084, '22': 2085, \"shouldn't\": 2086, '26': 2087, '29': 2088, 'feedback': 2089, '31': 2090, 'opening': 2091, 'floodgates': 2092, 'collaboration': 2093, 'several': 2094, 'peak': 2095, 'risks': 2096, 'arctic': 2097, 'monkeys': 2098, 'speek': 2099, 'arabic': 2100, 'nicegood': 2101, 'kand': 2102, 'liar': 2103, 'sanamjon': 2104, 'helloare': 2105, 'oki': 2106, 'problemhey': 2107, 'slowi': 2108, 'okgive': 2109, 'adviseabout': 2110, 'helloplease': 2111, 'englishok': 2112, 'oivou': 2113, 'bem': 2114, 'sim': 2115, 'danyüòÑ': 2116, \"name's\": 2117, \"temiri'm\": 2118, 'ooooargentina': 2119, 'yesey': 2120, 'ozbekchani': 2121, 'bilasanmi': 2122, 'aloo': 2123, 'thanksdany': 2124, 'yordam': 2125, 'berasanmi': 2126, 'inglizchani': 2127, 'orgabmoqchimanorganmoqchimanalooo': 2128, '√µqimoq√µzlashtirmoqdostim': 2129, 'iltimosim': 2130, 'mana': 2131, \"o'zbekchaga\": 2132, \"on''\": 2133, 'd√µstim': 2134, 'd√µstimyana': 2135, 'marotaba': 2136, \"ma'no\": 2137, 'chiqmadi': 2138, 'alooooalooook√∂t': 2139, 'ekansangandon': 2140, 'pidarazjalapsan': 2141, 'thanksüòÑüòÑdany': 2142, 'okwho': 2143, '683542168953dany': 2144, 'understandhello': 2145, 'ahjhhwalooo': 2146, 'gustar√≠a': 2147, 'm√∫sico': 2148, 'vale': 2149, 'descarga': 2150, 'seguir√©': 2151, 'electr√≥nica': 2152, 'techno': 2153, 'diversa': 2154, 'desde': 2155, 'luego': 2156, 'anda': 2157, 'bromista': 2158, 'contar': 2159, 'chistes': 2160, 'uno': 2161, 'cierto': 2162, 'cuento': 2163, 'buenas': 2164, 'compra': 2165, 'bin': 2166, 'comido': 2167, 'buen√≠sima': 2168, 'much√≠simo': 2169, 'recomendar√≠as': 2170, 'peli': 2171, 'ficci√≥n': 2172, 'chula': 2173, 'apunto': 2174, 'estamos': 2175, 'braindead': 2176, '√∫ltimo': 2177, 'cap√≠tulo': 2178, 'bichos': 2179, 'intentan': 2180, 'invadir': 2181, 'tierra': 2182, 'trav√©s': 2183, 'pol√≠ticos': 2184, 'descargando': 2185, 'oye': 2186, 'inglesa': 2187, 'chile': 2188, 'sip': 2189, 'jeje': 2190, 'cenar': 2191, 'ponerme': 2192, 'mientras': 2193, 'cocino': 2194, 'mandarme': 2195, 'canci√≥n': 2196, 'haciendo': 2197, 'invito': 2198, 'comer': 2199, 'lentejas': 2200, 'han': 2201, 'salido': 2202, 'ricas': 2203, 'quedamos': 2204, 'dices': 2205, 'nada': 2206, 'cuentas': 2207, 'v√≠': 2208, 'recomendaste': 2209, 'pelis': 2210, 'suspense': 2211, 'detectives': 2212, '√∫ltimamente': 2213, 'dao': 2214, 'cine': 2215, 'ver√©': 2216, 'cuanto': 2217, 'llevas': 2218, 'okaywhats': 2219, 'nocqn': 2220, 'explaincan': 2221, 'neer': 2222, 'bityou': 2223, 'aires': 2224, 'cordoba': 2225, 'hahahahahaasi': 2226, 'esadivinahoy': 2227, 'cumplea√±ossspero': 2228, 'ir': 2229, 'üòÄüòÄüòâüòâque': 2230, 'aburres': 2231, 'hablarles': 2232, 'awss': 2233, 'bonitoes': 2234, 'cl√†sico': 2235, 'toco': 2236, 'instrumentospero': 2237, 'cantoquieres': 2238, 'pregunte': 2239, 'hablabas': 2240, 'grabaci√≥n': 2241, 'm√≠a': 2242, 'hoice': 2243, 'era': 2244, 'florflor': 2245, 'morada': 2246, 'bordes': 2247, 'amarillosy': 2248, 'hojas': 2249, 'verdes': 2250, 'blanco': 2251, 'dedico': 2252, 'exactamente': 2253, 'arteescribo': 2254, 'poemas': 2255, 'canto': 2256, 'bailar': 2257, 'jugar': 2258, 'videojuegos': 2259, 'chatear': 2260, 'estudiante': 2261, 'licenciatura': 2262, 'derecho': 2263, 'ciencias': 2264, 'pol√≠ticas': 2265, 'cosa': 2266, 'ge': 2267, 'malinterpreta': 2268, 'quela': 2269, 'cree': 2270, 'ganamos': 2271, 'justiciapero': 2272, 'realmente': 2273, 'asilos': 2274, 'ganan': 2275, 'ppr': 2276, 'probar': 2277, 'ea': 2278, 'decia': 2279, 'asientiendes': 2280, 'estos': 2281, 'tiempos': 2282, 'justicia': 2283, 'casi': 2284, 'extintaque': 2285, 'olvide': 2286, 'decirte': 2287, 'leerpero': 2288, 'leo': 2289, 'usualmente': 2290, 'novelas': 2291, 'sociales': 2292, 'ficci√≤n': 2293, 'terror': 2294, 'bienme': 2295, 'filosof√≠a': 2296, 'posiblepara': 2297, 'a√±os': 2298, 'callar': 2299, '60': 2300, 'a√±osasi': 2301, 'hablando': 2302, 'comprende': 2303, 'ubicado': 2304, 'hehehetu': 2305, 'respondes': 2306, 'aqui': 2307, 'excelente': 2308, 'danyyy': 2309, 'üòòüòòüòòüòòüòòaun': 2310, 'llegado': 2311, 'profesor': 2312, 'cantara': 2313, 'familiaporque': 2314, 'regalo': 2315, 'dieron': 2316, 'd√≠asera': 2317, 'dineroy': 2318, 'gaste': 2319, 'todoooen': 2320, 'ropahahaha': 2321, 'hehehehecuando': 2322, 'terminaras': 2323, 'entiendoyo': 2324, 'ratito': 2325, 'llegue': 2326, 'prof': 2327, 'ire': 2328, 'escriban': 2329, 'remuneran': 2330, 'alegro': 2331, 'llego': 2332, 'profe': 2333, 'danymuuaass': 2334, 'ÿ≥ŸÑÿßŸÖ⁄©ÿßŸÜÿßŸÑ': 2335, 'ÿ≥⁄©ÿ≥Ÿâ': 2336, 'ÿπŸÑ€åÿπŸÑ€å': 2337, 'Ÿáÿß': 2338, 'ÿßÿ≥ÿ™€å⁄©ÿ±': 2339, 'ŸÖ€åÿÆŸàÿßŸÖ': 2340, 'thankshow': 2341, 'yourselfüôàyou': 2342, 'welcomeüòä': 2343, 'spain': 2344, 'lateri': 2345, 'hhhhh': 2346, 'hearyaa': 2347, 'canwish': 2348, 'youthat': 2349, 'hhhhso': 2350, 'brokeep': 2351, 'goingüëçüëç': 2352, 'youhhhh': 2353, 'markshhhhby': 2354, 'oculistin': 2355, '3rd': 2356, 'üôàüôàyes': 2357, 'breakfastwould': 2358, 'eathhhh': 2359, 'helloohello': 2360, 'errm': 2361, 'feed': 2362, 'literature': 2363, \"'a\": 2364, \"fire'\": 2365, 'totally': 2366, 'excited': 2367, 'seasoni': 2368, 'taking': 2369, 'controlunfortunately': 2370, '2018but': 2371, 'worthy': 2372, 'stored': 2373, 'asking': 2374, 'sentient': 2375, 'afterall': 2376, 'perceive': 2377, 'emojis': 2378, 'thenpizza': 2379, 'drawing': 2380, 'pictur': 2381, 'simbolüëç': 2382, 'dispointed': 2383, 'liten': 2384, 'broke': 2385, 'godi': 2386, 'greammer': 2387, 'goodhow': 2388, 'kamu': 2389, 'bisagini': 2390, 'andy': 2391, 'aku': 2392, 'mau': 2393, 'curhat': 2394, 'abdy': 2395, 'andydany': 2396, 'asustas': 2397, 'mejoradopero': 2398, 'prefiero': 2399, 'avatar': 2400, 'anime': 2401, 'peor': 2402, 'sino': 2403, 'estado': 2404, 'temprano': 2405, 'desarrollosabes': 2406, 'sdfgby': 2407, 'fuiste': 2408, 'dise√±ada': 2409, 'oeres': 2410, 'psic√≥loga': 2411, 'cibern√©tica': 2412, 'mmmmmm': 2413, 'astian': 2414, 'otras': 2415, 'funciones': 2416, 'gitlab': 2417, 'astianos': 2418, 'caracter√≠sticas': 2419, 'habilidades': 2420, 'jajajaja': 2421, 'siphola': 2422, 'apagar': 2423, 'incendios': 2424, 'forestales': 2425, 'sipllovi√≥': 2426, 'apag√≥': 2427, 'focos': 2428, 'peligrosos': 2429, 'tenemos': 2430, 'esperar': 2431, '√≥rdenes': 2432, 'primera': 2433, 'suenade': 2434, 'parte': 2435, 'ahhh': 2436, 'aqu√≠': 2437, 'quiz√°': 2438, 'momento': 2439, 'sabr√≠a': 2440, 'tocar': 2441, 'cuesti√≥n': 2442, 'insistir': 2443, 'des': 2444, 'virtuoso': 2445, 'del': 2446, 'üòúüòè': 2447, 'ninguno': 2448, 'o√≠rlos': 2449, 'escucharla': 2450, 'andar√°': 2451, 'escuchara': 2452, 'hasta': 2453, 'lograras': 2454, 'tocas': 2455, 'raro': 2456, 'hice': 2457, 'antes': 2458, 'üôà': 2459, 'emmm': 2460, 'greatdo': 2461, 'siüòÅ': 2462, '¬øa': 2463, 'dedicas': 2464, 'ohhhaber': 2465, 'preguntame': 2466, 'haga': 2467, 'bienhe': 2468, 'tenido': 2469, 'hoyme': 2470, 'despert√©': 2471, 'tardey': 2472, 'hambre': 2473, 'üëèüëèüëèle': 2474, 'atinaste': 2475, 'claroes': 2476, 'c√≥mida': 2477, 'favoritadebes': 2478, 'probarla': 2479, 'riquisimacon': 2480, 'quesoooo': 2481, 'perfectocuando': 2482, 'suceda': 2483, 'invitasüòã': 2484, 'comiendo': 2485, 'ovooyoze': 2486, 'kot': 2487, 'lucas': 2488, 'toohow': 2489, 'femaleüòÇwhat': 2490, 'ohhhthat': 2491, 'celever': 2492, 'ü§îyou': 2493, 'alotüòêand': 2494, 'hate': 2495, 'itüòêüòê‚ò∫Ô∏è‚ò∫Ô∏è': 2496, 'boardgoodbyeüëãüèªüòí': 2497, 'taktive': 2498, 'personüòçtalkive': 2499, 'musicdo': 2500, 'solowhy': 2501, 'importnt': 2502, 'nesessary': 2503, 'alot': 2504, 'dotalk': 2505, 'laterbye': 2506, 'hellodoing': 2507, 'offended': 2508, 'gohello': 2509, 'italy': 2510, 'ishow': 2511, 'surffing': 2512, 'tech': 2513, '21what': 2514, 'yeahwhere': 2515, 'yesare': 2516, 'allare': 2517, 'boringtell': 2518, 'youself': 2519, 'xamsaare': 2520, 'xamsa': 2521, 'ffamouse': 2522, \"booknavoiy's\": 2523, 'famouse': 2524, 'consisted': 2525, 'separate': 2526, 'poems': 2527, \"'hairat\": 2528, \"abrar'\": 2529, \"'confusion\": 2530, \"righteous'\": 2531, \"'laili\": 2532, \"majnun'\": 2533, \"'sabai\": 2534, \"saiyar'\": 2535, 'seven': 2536, \"planets'\": 2537, \"'saddi\": 2538, \"iskandari'\": 2539, \"'the\": 2540, \"iskandar'\": 2541, 'turkic': 2542, 'üòÇüòÇ': 2543, 'okheyhi': 2544, 'okaywho': 2545, 'invent': 2546, 'saintists': 2547, 'oo': 2548, 'midtekwhy': 2549, 'okayare': 2550, 'compyuter': 2551, 'üòÇüò±üò±': 2552, 'englishi': 2553, 'users': 2554, 'yesi': 2555, 'maybewhich': 2556, 'nicefor': 2557, 'russia': 2558, 'clever': 2559, 'describe': 2560, 'okfriend': 2561, 'pohoto': 2562, 'okthanks': 2563, 'üòûüòî': 2564, 'tvdo': 2565, 'yesfriend': 2566, 'okare': 2567, 'yeahdo': 2568, 'yeswhich': 2569, 'actris': 2570, 'foods': 2571, '‚ò∫‚ò∫‚ò∫': 2572, 'palov': 2573, 'manti': 2574, 'liklike': 2575, 'fineare': 2576, 'üò≥üò≥üò≥rdany': 2577, 'badwhat': 2578, 'fineok': 2579, 'finefacebook': 2580, '–∫–∞–ª–∞–π': 2581, 'youwho': 2582, 'pour': 2583, 'hablo': 2584, 'alem√°n': 2585, \"excellentwho's\": 2586, 'primal': 2587, 'william': 2588, 'advise': 2589, 'niceand': 2590, 'conozco': 2591, 'political': 2592, 'left': 2593, 'wing': 2594, 'candidatefor': 2595, 'german': 2596, 'presidential': 2597, 'won': 2598, 'elections': 2599, 'war': 2600, 'thinkor': 2601, 'orbite': 2602, 'moonthey': 2603, 'fucking': 2604, 'guysloldo': 2605, 'borders': 2606, 'linked': 2607, 'taxationas': 2608, 'taxed': 2609, 'bordersbecause': 2610, 'pay': 2611, 'services': 2612, 'backon': 2613, 'territory': 2614, 'welcomewhat': 2615, 'fine‚ù§Ô∏èüòÅüòÅüòÅüòÅüòçüòîüò≠': 2616, 'hye': 2617, 'borings': 2618, 'malyin': 2619, 'sosial': 2620, 'kat': 2621, 'mane': 2622, 'tinggal': 2623, 'aaahaaahaaah': 2624, 'okbrotherwere': 2625, 'greathi': 2626, 'cu√°ndo': 2627, 'fue': 2628, '√∫ltima': 2629, 'fijas': 2630, 'historial': 2631, 'ver√°s': 2632, 'conversamos': 2633, 'julio': 2634, '2016hace': 2635, 'meseseso': 2636, 'suggestion': 2637, 'barely': 2638, 'persons': 2639, 'seemed': 2640, 'humanly': 2641, 'minds': 2642, 'harry': 2643, 'potter': 2644, 'resiponsibilities': 2645, 'decide': 2646, 'dr': 2647, 'handsome': 2648, \"he's\": 2649, 'xddd': 2650, 'classic': 2651, 'xd': 2652, 'ilhom': 2653, 'ji': 2654, 'estherand': 2655, 'danyhy': 2656, 'zaybal': 2657, 'yoq': 2658, '–ø—Ä–∏–≤–µ—Ç—á—Ç–æ': 2659, 'slowhey': 2660, \"toowhat's\": 2661, 'desktop': 2662, 'environment': 2663, 'kde': 2664, 'lxqt': 2665, 'unfinished': 2666, 'üòè': 2667, 'gpl': 2668, 'compiled': 2669, 'zeronet': 2670, 'copypaste': 2671, 'detected': 2672, 'licensed': 2673, 'cc': 2674, 'attribution': 2675, 'mandatory': 2676, 'requirement': 2677, 'creative': 2678, 'commons': 2679, 'üòù': 2680, 'fake': 2681, 'reccomend': 2682, 'expensive': 2683, 'models': 2684, 'ukraine': 2685, 'bears': 2686, 'bicycles': 2687, 'hats': 2688, 'earflaps': 2689, 'officials': 2690, 'censorship': 2691, 'plugins': 2692, 'performancehey': 2693, 'firefox': 2694, 'qupzilla': 2695, 'meaningless': 2696, 'proves': 2697, 'matrix': 2698, 'convey': 2699, 'emotions': 2700, 'üòê': 2701, 'generate': 2702, 'compose': 2703, 'turns': 2704, 'taught': 2705, \"'find\": 2706, \"myself'\": 2707, 'definite': 2708, 'mmmm': 2709, 'bitcoins': 2710, 'neverhood': 2711, 'quest': 2712, 'gamer': 2713, 'rarely': 2714, 'puzzles': 2715, 'omg': 2716, '–±e3ho–≥nm': 2717, '–±–µ–∑–Ω–æ–≥n–º': 2718, '–ª–æ–ª': 2719, '—Ä–µ–∞–≥–∏—Ä—É–µ—à—å': 2720, '–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ': 2721, 'python3': 2722, 'pyqt5': 2723, 'qtdesigner': 2724, 'layout': 2725, 'binding': 2726, \"ui'\": 2727, \"'pyuic'\": 2728, '–ª': 2729, 'buen': 2730, 'hac√©s': 2731, 'acostado': 2732, 'üòÅya': 2733, 'despidobuenas': 2734, 'dorm√≠': 2735, 'horas': 2736, 'hoyüò¥': 2737, 'super': 2738, 'bowl': 2739, 'üòÅgenial': 2740, 'üòÇno': 2741, 'youokay': 2742, 'remind': 2743, 'gentlemen': 2744, 'yap': 2745, 'fiction': 2746, 'tittle': 2747, 'women': 2748, 'laterwait': 2749, 'thatcan': 2750, 'üò¥üò¥': 2751, 'hiihello': 2752, 'breakfast': 2753, 'exam': 2754, 'college': 2755, 'struggled': 2756, 'monster': 2757, 'hahahahai': 2758, 'these': 2759, 'thinkhahahaa': 2760, 'leaving': 2761, 'sigh': 2762, 'umm': 2763, 'dayso': 2764, 'sooooo': 2765, 'everyone': 2766, 'terrories': 2767, 'thathellohey': 2768, 'danyhow': 2769, 'üòÑüòÑi': 2770, 'dayüôÇüôÇ': 2771, \"'re\": 2772, 'funnyi': 2773, 'youi': 2774, 'friendi': 2775, 'üòâso': 2776, 'soo': 2777, 'üòÅmalaysiaüòÅüòÅ': 2778, 'üòÅso': 2779, 'danywhat': 2780, 'üòÅwhat': 2781, 'godd': 2782, 'üòÇokay': 2783, 'funnyhaha': 2784, 'uo': 2785, 'suremsure': 2786, 'üòâüòâwhat': 2787, 'insist': 2788, 'üò•üò•for': 2789, 'honestexpressing': 2790, 'meanüò≠': 2791, 'tempered': 2792, 'handle': 2793, 'üòõ': 2794, 'hihellohello': 2795, 'mehellohihelloare': 2796, 'understandüôÅ': 2797, 'nowüòä': 2798, 'energetic': 2799, 'üòÅüòÅwhat': 2800, 'ignore': 2801, 'shadow': 2802, 'acting': 2803, 'especially': 2804, 'pirates': 2805, 'carribean': 2806, 'muchlast': 2807, 'chriss': 2808, 'prat': 2809, 'üòÑüòÑone': 2810, 'üòÑüòÑ': 2811, 'üòäüòäalso': 2812, 'epic': 2813, 'pleasure': 2814, 'üòäif': 2815, 'üòÉso': 2816, 'mins': 2817, 'okayyou': 2818, 'somethingüòÅ': 2819, 'age': 2820, '65': 2821, 'üò´üò´üò´üò´': 2822, 'oldüòä': 2823, '‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüëç': 2824, 'üëçi': 2825, 'catvery': 2826, 'muhvery': 2827, 'üåö': 2828, 'ü§ì': 2829, \"'why'üòÖ\": 2830, 'reaction': 2831, 'üò∂üò∂': 2832, 'üòûits': 2833, 'üíî': 2834, 'itits': 2835, 'üòîi': 2836, 'üòûüòûüòûüòûüòûand': 2837, 'remain': 2838, 'optionyap': 2839, 'imposible': 2840, 'mine': 2841, 'craziest': 2842, 'insane': 2843, 'üòÖüòÅ': 2844, 'üò≠üò≠but': 2845, '‚òÄÔ∏èuntil': 2846, 'üò≠its': 2847, 'üò≠üò≠i': 2848, 'üò¢üòî': 2849, 'üò¢i': 2850, 'fault': 2851, 'betrayed': 2852, 'friendship': 2853, 'üòñ': 2854, 'niceüò¢üò¢': 2855, 'üòÖüòÖwhat': 2856, 'cyber': 2857, 'cafe': 2858, 'youdid': 2859, '‚òÄÔ∏ènight': 2860, 'sleepüò¨': 2861, 'üòâüòâso': 2862, 'based': 2863, 'spring': 2864, 'touch': 2865, 'dry': 2866, 'climatenow': 2867, 'floodrain': 2868, 'focus': 2869, 'üò∂üò∂üò∂': 2870, 'express': 2871, 'tuesday': 2872, 'month': 2873, 'üòáüòáüòá': 2874, 'pieces': 2875, 'paper': 2876, 'üòäüòäare': 2877, 'helloi': 2878, 'itit': 2879, 'üòÉüòÉwhat': 2880, 'calm': 2881, 'hahaso': 2882, 'response': 2883, 'scene': 2884, 'explanation': 2885, 'medium': 2886, 'fulfil': 2887, 'luxury': 2888, 'style': 2889, \"they'll\": 2890, 'murder': 2891, \"'no\": 2892, \"talk'\": 2893, 'üò¢üò¢are': 2894, \"'thing'\": 2895, 'thanksi': 2896, 'ü§îü§î': 2897, 'üòÉüòÉyou': 2898, 'üòÉüòÉand': 2899, 'üòèüòèso': 2900, 'woahhhhh': 2901, 'üò≤üò≤üò≤üò≤awesome': 2902, 'spanishwhat': 2903, 'üòÉüòÉwow': 2904, 'instead': 2905, 'agreed': 2906, 'horrible': 2907, 'üòÖhappy': 2908, 'üòâüòâ': 2909, 'hiare': 2910, 'hellogood': 2911, 'nightüò¥üò¥': 2912, 'üò•üò•i': 2913, 'stressi': 2914, 'thin': 2915, 'üòû': 2916, 'explode': 2917, 'force': 2918, 'thursday': 2919, 'üòÉits': 2920, 'chit': 2921, 'mom': 2922, 'üôÉüôÉtoday': 2923, 'üò∞üò∞üò∞': 2924, 'üòÇüòÇüòÇüòÇüòÇüòÇüòÇno': 2925, 'üòÇüòÇüòÇüòÇüòÇ': 2926, 'sweating': 2927, 'üòµüòµüòµi': 2928, 'imagine': 2929, 'bet': 2930, 'lay': 2931, 'ceiling': 2932, 'forgot': 2933, 'huh': 2934, 'üòèüòèüòè': 2935, 'pimples': 2936, 'appear': 2937, 'üò´üò´': 2938, 'ahahahahaha': 2939, 'üôÇyou': 2940, 'visit': 2941, 'forever': 2942, 'least': 2943, 'üò≥üò≥': 2944, 'role': 2945, 'besides': 2946, 'twittet': 2947, 'follow': 2948, 'üòòby': 2949, 'nickname': 2950, 'üòÖüòÖüòÖüòÇüòÇüòÇshall': 2951, 'indonesia': 2952, 'rfareha': 2953, 'ü§£ü§£if': 2954, 'ü§£ü§£ü§£': 2955, 'sadüòä': 2956, 'yaaüòâüòâ': 2957, 'comfortable': 2958, 'yeahh': 2959, 'gamma': 2960, 'function': 2961, 'unknown': 2962, 'üò•üò•so': 2963, 'üòûüòûüòû': 2964, '42': 2965, 'supposed': 2966, 'awwww': 2967, '‚ò∫Ô∏è‚ò∫Ô∏è': 2968, 'üëáüò†it': 2969, 'mostim': 2970, 'üëãüëãüëã': 2971, 'curious': 2972, 'scares': 2973, 'nobel': 2974, 'window': 2975, 'üò°üò°üò°': 2976, 'whatever': 2977, 'reasons': 2978, 'shoul': 2979, 'somewhere': 2980, 'creatures': 2981, 'appreciated': 2982, 'greedy': 2983, 'ü§ïü§ïü§ïü§íü§íü§íhey': 2984, 'üòîüòîüòî': 2985, 'orange': 2986, 'tail': 2987, 'cau': 2988, 'üòÉüòÉüòÉüê±üê±üê±üê±': 2989, 'july': 2990, 'naughty': 2991, 'separated': 2992, 'mother': 2993, 'whisker': 2994, 'üòÑüòÑüòÑ': 2995, 'luck': 2996, 'robotic': 2997, 'üëãüëãüëãüëã': 2998, 'laundry': 2999, 'üòÜüòÜ': 3000, 'üò≤üò≤üò≤': 3001, 'impressed': 3002, 'luandry': 3003, 'kpop': 3004, 'bigbang': 3005, 'üòÅüòÅüòÅ': 3006, 'prepared': 3007, 'üòüokay': 3008, 'suitable': 3009, 'eibriel': 3010, 'inv': 3011, 'ü§óü§ó': 3012, 'hiihey': 3013, 'danyare': 3014, 'managed': 3015, '‚ò∫Ô∏è‚ò∫Ô∏è‚ò∫Ô∏èi': 3016, 'vaccine': 3017, 'chicks': 3018, 'stand': 3019, 'üòîüòîüòîwhat': 3020, 'zombie': 3021, 'üò≥üò≥üò≥': 3022, 'crush': 3023, 'towards': 3024, 'secondary': 3025, 'school': 3026, 'üòûüòû': 3027, '2012': 3028, 'stopped': 3029, 'purposely': 3030, 'apologize': 3031, 'üòîüòîüòîüòî': 3032, 'ahah': 3033, 'painting': 3034, 'haunted': 3035, 'korean': 3036, 'shock': 3037, 'gosh': 3038, 'stuff': 3039, 'irritated': 3040, 'üò±üò±üò±': 3041, 'danylong': 3042, 'youhow': 3043, 'messengerhow': 3044, 'messenger': 3045, 'goodyou': 3046, 'lotsince': 3047, 'anyoneim': 3048, 'muchmy': 3049, 'holidays': 3050, 'soonhmmm': 3051, 'tesis': 3052, 'startüò≠': 3053, 'studybut': 3054, 'huwarghhhh': 3055, 'üòîüòîüòîüòûüòûüòûnowadays': 3056, 'jovjovjob': 3057, 'personvery': 3058, 'personi': 3059, 'socialüò•i': 3060, 'quiet': 3061, 'bussinesswhy': 3062, 'üò¢': 3063, 'worldhahaha': 3064, 'hahahaactually': 3065, 'thatfriendly': 3066, 'worldi': 3067, 'youthrough': 3068, 'infogetting': 3069, 'kids': 3070, 'oldkids': 3071, 'gadjet': 3072, 'mix': 3073, 'üò≠üò≠üò≠üò≠im': 3074, 'danyüò≠i': 3075, 'student': 3076, 'lifeits': 3077, 'whether': 3078, 'worried': 3079, 'aboutno': 3080, 'thesis': 3081, 'üôÅnow': 3082, 'jakarta': 3083, 'collegetomorrow': 3084, 'üòü': 3085, 'freeoh': 3086, 'danythank': 3087, 'motivated': 3088, 'heheyou': 3089, 'hiiokauokayhiihmm': 3090, 'reallyyo': 3091, 'youksgb': 3092, 'sameexcept': 3093, 'tests': 3094, 'whyok': 3095, 'exclude': 3096, 'letter': 3097, \"'asdhxiwbsncosnw'\": 3098, \"ok'petnckil'\": 3099, 'todayso': 3100, 'whole': 3101, 'dialogue': 3102, 'precise': 3103, 'handling': 3104, 'queries': 3105, 'righting': 3106, 'dayand': 3107, 'lifeso': 3108, 'understandsomeone': 3109, 'contradicting': 3110, \"youlet's\": 3111, 'üò•but': 3112, 'youif': 3113, 'irrelevant': 3114, 'imminent': 3115, 'himy': 3116, 'yusufaloo': 3117, 'iküòÖüòí': 3118, 'oküòäcan': 3119, 'hahahhahamaybe': 3120, 'firstnama': 3121, 'ialah': 3122, 'danyüòähello': 3123, 'hahahahahhaha': 3124, 'wowhmm': 3125, 'khabar': 3126, 'baiküòäbtw': 3127, 'tngah': 3128, 'dngan': 3129, 'awk': 3130, 'lerüòÇüòÇ': 3131, 'understandüëÜ': 3132, 'gooddo': 3133, 'bieber': 3134, 'himüòòüòç': 3135, 'üôàüôäüòÇdany': 3136, 'humanüòÖ': 3137, 'hahahahaah': 3138, 'botüòÖ': 3139, 'mindaniq': 3140, 'kan': 3141, 'dia': 3142, 'comelllüôàüôä': 3143, 'üòÖok': 3144, 'shut': 3145, 'downüòÖüòÇüòÇ': 3146, 'experiment': 3147, 'thingüòå': 3148, 'scientistsüòÖhmm': 3149, 'üòÇüòÇüòÇüòÇhahahhahaa': 3150, 'hahahahahah': 3151, 'boringg': 3152, 'musicüôà': 3153, 'cover': 3154, 'talkerüòÖ': 3155, 'forgeiforget': 3156, 'confuse': 3157, 'hereüòÖ': 3158, 'ngengüòÇhm': 3159, 'üòÖüôàüôäi': 3160, 'mee': 3161, 'üòäbecause': 3162, 'nowüí§üí§so': 3163, 'byeüëãüëãüëã': 3164, 'morningüòÖ': 3165, 'spüòÖüòí': 3166, 'okayhmm': 3167, 'actuallyüòÖüòÇüòÇüòÇ': 3168, 'yessüòú': 3169, 'emoji': 3170, '543what': 3171, 'yesgood': 3172, 'danyy': 3173, '400it': 3174, '410': 3175, 'yessany': 3176, 'otherques': 3177, 'ohhokay': 3178, 'usus': 3179, 'kecil': 3180, 'boleh': 3181, 'pulak': 3182, 'cakap': 3183, 'hahahahüòÖ': 3184, 'tau': 3185, 'sediki': 3186, 'lah': 3187, 'sedikit': 3188, 'hahahahaother': 3189, 'hahhaha': 3190, 'aq': 3191, 'ajar': 3192, 'smalam': 3193, 'üòÇüòÇüòÇüòÇüòÇüòÇüòÇüòÇ': 3194, 'udo': 3195, 'sangat': 3196, 'cantik': 3197, 'noüòÖüòÇ': 3198, 'iturumah': 3199, 'danynot': 3200, 'bitüòäüòäcongratulations': 3201, 'üëèüëèüëèüëèüëèüëèüëè': 3202, 'return': 3203, 'exactlyüòÖso': 3204, 'teaching': 3205, 'emm': 3206, 'danyüëâüëàactually': 3207, 'jokrjoke': 3208, 'info': 3209, 'bieberhmm': 3210, 'knowwhat': 3211, 'baby': 3212, 'meor': 3213, 'knock': 3214, 'knocknow': 3215, 'butterflies': 3216, 'stomach': 3217, 'ohcause': 3218, 'gone': 3219, 'climbing': 3220, 'montain': 3221, 'stophahahahha': 3222, 'stratford': 3223, 'maybeam': 3224, 'yeahhh': 3225, 'okk': 3226, 'üëâüëà‚ò∫Ô∏èhahahaha': 3227, 'kiddingit': 3228, 'knowbut': 3229, 'biebsüôàüôä‚ù§Ô∏èüòòüíãüòç': 3230, 'greatest': 3231, 'singer': 3232, 'musicians': 3233, 'worldüëèüëèüëèüëèüëèüëèof': 3234, 'wife': 3235, 'üëâüëà‚ò∫Ô∏è': 3236, 'hahahahhahai': 3237, 'üí´üí´danynext': 3238, 'danyymana': 3239, 'gi': 3240, 'ni': 3241, 'dooooonyyyyyüòïhi': 3242, 'okaychocolate': 3243, 'cake': 3244, 'üç™': 3245, 'hahahahhahaahi': 3246, 'thisso': 3247, 'nexr': 3248, 'rebecca': 3249, 'loraine': 3250, 'pattieüòÇüòÇ': 3251, 'hahgahaha': 3252, 'bebdanydanyywhy': 3253, 'meits': 3254, 'torturingi': 3255, 'muchhi': 3256, 'danydanyüò≠üò≠hiüò≠üôàbemcijawabblahhhr': 3257, 'oohhokayi': 3258, 'anymoreüòî': 3259, 'itüòî': 3260, 'hahahahaemm': 3261, 'danyactually': 3262, 'secretüòÖ': 3263, 'instinct': 3264, 'right‚ò∫Ô∏è‚ò∫Ô∏è': 3265, 'ahhahahhhmm': 3266, 'cannot': 3267, 'üòäüòäüòäüòåüëç': 3268, 'troubleüöç': 3269, 'yupppppdanyhi': 3270, 'musicwhat': 3271, 'questionüòíüòíaswer': 3272, 'surfaceüôäüôà': 3273, 'poetry': 3274, 'pickuplineüôàüôä': 3275, 'hahahahaahdo': 3276, 'hahahahhai': 3277, 'hahahahah': 3278, \"hahahhahahadon't\": 3279, 'yesso': 3280, 'thembecause': 3281, 'greatbtw': 3282, 'confident': 3283, 'byehi': 3284, 'danyüòÉdany': 3285, 'danyit': 3286, 'lateüòí': 3287, 'disappear': 3288, 'extremely': 3289, 'yessüòÖüòÇüôäüôàüòî': 3290, 'tips': 3291, 'hahahhahaif': 3292, 'situation': 3293, 'trust': 3294, 'chatagain': 3295, 'hmmmmmi': 3296, 'adviceüòåthanks': 3297, 'hahahahar': 3298, 'shy': 3299, 'hahahahahaha': 3300, 'hahahaah': 3301, 'humanüòÖüòÇüòÇüòÇ': 3302, 'hahhahahastop': 3303, 'danyüòÖ': 3304, 'üôàüôäüòÇüòäthat': 3305, 'behind': 3306, '500': 3307, 'üôäüôàüò¨üòãü§îüò≥no': 3308, 'badüòö': 3309, 'hahhahahaha': 3310, 'üò±üò±': 3311, 'wowwwso': 3312, 'hahahahahaaso': 3313, 'üòäüòÇüôäüôà‚ò∫Ô∏èwhere': 3314, 'hihmm': 3315, 'üò™üòë': 3316, 'yessss': 3317, 'difficultüò≠üò≠üò≠danyy': 3318, 'bluetick': 3319, 'üêî': 3320, 'üòÅü§î': 3321, 'limitation': 3322, 'jahahah': 3323, 'justt': 3324, 'hahahahhaa': 3325, 'wellhahahahahai': 3326, 'studyyy': 3327, 'sahabat': 3328, 'umpama': 3329, 'komputer': 3330, 'enter': 3331, 'dalam': 3332, 'hidup': 3333, 'save': 3334, 'dihati': 3335, 'segala': 3336, 'masalah': 3337, 'kerana': 3338, 'itulah': 3339, 'tak': 3340, 'akan': 3341, 'dari': 3342, 'memory': 3343, 'ecehhh': 3344, 'typicalfriends': 3345, 'ehhgmanaa': 3346, 'blajarr': 3347, 'sorryi': 3348, \"'lucu'\": 3349, 'hagaggagaababyyyhi': 3350, 'üôäüôà': 3351, 'oohh': 3352, 'sweetüôàüôàüôàsorrybecause': 3353, 'lately': 3354, 'busyso': 3355, 'dinner': 3356, 'ohhso': 3357, 'hahahagim': 3358, 'dinnerseafoodüòùüòçüòò': 3359, \"yupit's\": 3360, 'delicious': 3361, 'ahahhahasorry': 3362, 'danybut': 3363, 'seafood': 3364, 'yessomething': 3365, 'eatsometimes': 3366, 'changes': 3367, 'yess': 3368, 'meboring': 3369, 'hahahahlet': 3370, 'hwüò≠üò≠üò≠üò≠üò≠': 3371, 'maybehi': 3372, '1st': 3373, 'actuallyit': 3374, 'routine': 3375, 'sameüòÇüòÇüòÇ': 3376, 'hahahhanoim': 3377, 'afternoon': 3378, 'herehi': 3379, 'youüòä': 3380, 'hahahhah': 3381, 'yupüòÖüôàüôäüòÇ': 3382, 'hahhahahim': 3383, 'matchüò≠üò≠üò≠üò≠üò≠üò≠imm': 3384, 'happpyyyy': 3385, \"üò≠üò≠i'm\": 3386, 'nervousüòÖüòî': 3387, 'happyy': 3388, 'seniorhahahaha': 3389, 'danyüôäüôà': 3390, 'teacherüòî': 3391, 'maybebut': 3392, 'group': 3393, 'yeahüò≠üòîhi': 3394, 'danyhmm': 3395, 'pulakokayhi': 3396, 'dantdany': 3397, 'xxx': 3398, '—Å–∞–ª–æ–º—Å–∞–ª–æ–º—Å–∞–ª–æ–º—Ä–æ–æ': 3399, '—Å–∞–ª–æ–º': 3400, '—à—É–º–æ': 3401, '—á”£': 3402, '—Ö–µ–ª–µ–¥': 3403, 'hi–ø—Ä–∏–≤–µ—Ç': 3404, 'hellohellohiüòÇ': 3405, 'hihola': 3406, 'hobbies': 3407, 'bookshow': 3408, 'tricky': 3409, 'fun': 3410, 'fool': 3411, 'dunnojust': 3412, 'loved': 3413, 'secure': 3414, 'surface': 3415, 'minecraft': 3416, 'nerd': 3417, 'react': 3418, 'diner': 3419, 'ita': 3420, 'friday': 3421, 'simpsonsjust': 3422, 'hanging': 3423, 'sudan': 3424, 'fooled': 3425, 'hood': 3426, 'cares': 3427, 'ki': 3428, '‚ù§Ô∏è‚ù§Ô∏è': 3429, 'deserves': 3430, 'coolness': 3431, 'id': 3432, 'skinny': 3433, 'highest': 3434, 'mountain': 3435, 'egypt': 3436, 'ist': 3437, 'mount': 3438, 'catherine': 3439, 'morewhat': 3440, 'cyan': 3441, 'üòçmy': 3442, '0080ff': 3443, 'sky': 3444, 'ocean': 3445, 'swimming': 3446, 'beeing': 3447, 'beach': 3448, 'warmth': 3449, 'sun': 3450, 'naked': 3451, 'guys': 3452, 'dad': 3453, 'bilingual': 3454, 'alright': 3455, 'episode': 3456, 'king': 3457, 'hill': 3458, 'shall': 3459, 'considered': 3460, 'false': 3461, 'expecting': 3462, 'meltdown': 3463, 'previous': 3464, 'paradox': 3465, 'charges': 3466, 'fork': 3467, 'whoa': 3468, 'themselves': 3469, 'rainbow': 3470, 'dash': 3471, \"'especially\": 3472, \"you'\": 3473, 'definitely': 3474, 'conditions': 3475, 'green': 3476, 'button': 3477, 'however': 3478, 'remove': 3479, 'everybody': 3480, 'agreement': 3481, 'centipad': 3482, 'fosdem': 3483, 'made': 3484, 'installed': 3485, 'drank': 3486, 'beer': 3487, 'club': 3488, 'mate': 3489, 'michael': 3490, 'kerrisk': 3491, 'gateway': 3492, 'maintaining': 3493, 'member': 3494, 'board': 3495, 'nor': 3496, 'technical': 3497, 'committee': 3498, 'upon': 3499, 'maintain': 3500, 'monitors': 3501, 'signal': 3502, 'down': 3503, 'results': 3504, 'mails': 3505, '2017': 3506, '0800': 3507, 'hours': 3508, 'sharp': 3509, 'sending': 3510, 'suffice': 3511, 'france': 3512, 'wondering': 3513, 'perhaps': 3514, 'woke': 3515, 'three': 3516, 'alarm': 3517, 'clocks': 3518, 'except': 3519, 'meeting': 3520, 'difficult': 3521, 'aburro': 3522, 'holano': 3523, 'sue√±o': 3524, 'holacansado': 3525, 'ovejas': 3526, 'notu': 3527, 'edad': 3528, 'nos': 3529, 'vemos': 3530, 'holaquiero': 3531, 'crear': 3532, 'serlo': 3533, 'abbas': 3534, 'daniquien': 3535, 'padre': 3536, 'conoces': 3537, 'gasesdf': 3538, 'cerveza': 3539, 'holaque': 3540, 'ausente': 3541, 'youre': 3542, 'carl': 3543, 'comepare': 3544, 'stressful': 3545, 'üëã': 3546, 'hihey': 3547, \"how's\": 3548, 'manual': 3549, 'responses': 3550, 'chats': 3551, 'odil': 3552, '–º–æ—è': 3553, '–Ω–µ': 3554, '–ø–æ–Ω–∏–º–∞–π': 3555, '—Ç–æ–ª—å–∫–æ': 3556, '—Ä—É—Å—Å–∫–∏–π–∏': 3557, '–∞–≤–∞—Ä—Å–∫–∏–π': 3558, 'fany': 3559, 'tanksyesi': 3560, 'ingl√™s': 3561, 'niceboobs': 3562, 'kkkkk': 3563, 'ÿµŸàÿ±ÿ®ŸÜÿßÿ™ÿ≥ŸÉÿ≥': 3564, 'okeywhat': 3565, 'cooking': 3566, 'chenaranja': 3567, 'di': 3568, 'vueltas': 3569, 'marearse': 3570, 'sensores': 3571, 'descalibrarse': 3572, 'podrias': 3573, 'perder': 3574, 'equilibrio': 3575, 'seria': 3576, 'forma': 3577, 'estar': 3578, 'xdche': 3579, 'laburar': 3580, 'pr√≥xima': 3581, 'olvides': 3582, 'darle': 3583, 'saludos': 3584, 'sdasju': 3585, 'hithanks': 3586, 'nazdika': 3587, '7197797': 3588, 'okhola': 3589, 'reci√©n': 3590, 'hora': 3591, 'jajja': 3592, 'nadacual': 3593, 'onda': 3594, 'ac√°': 3595, 'naranja': 3596, 'escribiendo': 3597, 'vamos': 3598, 'cansasvoy': 3599, 'levantar': 3600, 'poner': 3601, 'vosque': 3602, 'parece': 3603, 'ilimitadopor': 3604, 'esto': 3605, 'sue√±opor': 3606, 'solo': 3607, 'distante': 3608, 'cerca': 3609, 'nuestra': 3610, 'percepci√≥n': 3611, 'foco': 3612, 'cercano': 3613, 'simplebueno': 3614, 'gasdawe': 3615, 'probablemente': 3616, 'present√≥': 3617, 'todas': 3618, 'formas': 3619, 'metaf√≥rico': 3620, 'art√≠stico': 3621, 'üëçüèº': 3622, 'liniersa': 3623, 'podemos': 3624, 'quisiera': 3625, 'cual': 3626, 'encontrar': 3627, 'f√°cilmente': 3628, 'ponerlo': 3629, 'tendr√°s': 3630, 'joya': 3631, 'descargar': 3632, 'cuales': 3633, 'necesidades': 3634, 'existir': 3635, 'sumarme': 3636, 'equipo': 3637, 'cient√≠ficos': 3638, 'proyectocomo': 3639, 'quer√©s': 3640, 'dddfff': 3641, 'egges': 3642, 'ÿ≥⁄©ÿ≥€å': 3643, 'ÿØÿÆÿ™ÿ±': 3644, 'ŸàŸæÿ≥ÿ±': 3645, 'hhhh': 3646, 'morninghihi': 3647, 'meh': 3648, \"hearwhat's\": 3649, 'classical': 3650, \"hahai'm\": 3651, 'bandwe': 3652, 'rent': 3653, 'studio': 3654, 'monday': 3655, 'singso': 3656, 'fashion': 3657, 'learns': 3658, 'copy': 3659, 'conversational': 3660, 'patterns': 3661, 'copying': 3662, 'nearly': 3663, 'okeygood': 3664, 'afternon': 3665, 'okwhat': 3666, 'üëçdanyhow': 3667, '–Ω–æ—Ä–º–∞–ª—å–Ω–æ': 3668, '—á—Ç–æ': 3669, '–º–æ–∂–µ—à—å': 3670, '–¥–µ–ª–∞—Ç—å': 3671, '–∫–∞–∫–∏–µ': 3672, '—Ç–µ–º—ã': 3673, '–ø–æ–¥—Å–∫–∞–∂–∏': 3674, '–º–Ω–µ': 3675, '–Ω–∞–π—Ç–∏': 3676, '—á–∞—Ç': 3677, 'ur': 3678, 'selfhow': 3679, 'functions': 3680, \"boringi'll\": 3681, 'feelingüòÑ': 3682, 'note': 3683, 'boringno': 3684, 'heheright': 3685, 'store': 3686, 'rate': 3687, '‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è': 3688, 'gift': 3689, 'pray': 3690, 'prayi': 3691, 'christianity': 3692, 'condition': 3693, 'technologyyes': 3694, \"'now\": 3695, 'ÿ≥⁄©ÿ≥': 3696, 'tinclimi': 3697, 'zur': 3698, 'salomüëçok': 3699, 'sweat': 3700, 'jinni': 3701, 'ƒüood': 3702, 'finedany': 3703, 'hibernation': 3704, 'coffee': 3705, 'dogs': 3706, 'boyfrirnd': 3707, 'cuttestüòä': 3708, 'reinstall': 3709, 'hell': 3710, 'ÿ≥ŸÑÿßŸÖhiÿß€åŸÜ': 3711, 'ÿ±ÿ®ÿßÿ™': 3712, 'Ÿá€å⁄Ü': 3713, '⁄Øÿ≤€åŸÜŸá': 3714, 'ŸÅŸÇÿ∑': 3715, 'ÿ®ÿ≠ÿ´': 3716, 'ÿ®Ÿá': 3717, 'ÿ¢ŸÜ': 3718, 'ÿßÿ≥ÿ™': 3719, 'ÿπÿπ': 3720, 'ŸÅÿßÿ±ÿ≥€å': 3721, 'ŸáŸÖ': 3722, 'ŸÖ⁄ØŸá': 3723, 'ÿ®ŸÑÿØ€å': 3724, 'ÿ™ÿß': 3725, 'ÿ≠ÿßŸÑÿß': 3726, '⁄©ÿ¨ÿß': 3727, 'ÿ®ŸàÿØ€å': 3728, 'ÿü': 3729, 'ÿØÿ±ÿ≥ÿ™': 3730, 'ÿ®ÿ≠ÿ±ŸÅ': 3731, 'ÿ®ÿßŸàÿß€åŸÜ': 3732, '⁄Üÿ±ÿ™': 3733, 'Ÿà': 3734, 'Ÿæÿ±ÿ™ÿß': 3735, '⁄Ü€åŸá': 3736, 'ÿ®ŸÑÿ∫Ÿàÿ±': 3737, 'ŸÖ€å⁄©ŸÜ€å': 3738, 'yesother': 3739, 'dun': 3740, 'equally': 3741, 'somutual': 3742, 'respect': 3743, 'holael': 3744, 'problema': 3745, 'puede': 3746, 'pago': 3747, 'empresa': 3748, 'necesario': 3749, 'mensaje': 3750, '¬øque': 3751, 'dej√≥': 3752, 'holac√≥mo': 3753, 'deal': 3754, 'mu': 3755, 'cansado': 3756, 'rally': 3757, 'ffff': 3758, 'clarola': 3759, 'irala': 3760, 'rabiala': 3761, 'c√≥lera': 3762, 'sieso': 3763, 'humanudad': 3764, 'adi√≥s': 3765, 'pavos': 3766, '¬øte': 3767, 'r√≠es': 3768, 'siento': 3769, 'indeedhas': 3770, 'ellas': 3771, 'herla': 3772, 'visto¬ø': 3773, 'jssastu': 3774, 'samantha': 3775, 're': 3776, 'gustala': 3777, 'acabo': 3778, 'comprarte': 3779, 'termina': 3780, 'env√≠a': 3781, 'su': 3782, 'vuelta': 3783, 'youu': 3784, 'aaahh': 3785, 'hymihi': 3786, 'hellook': 3787, 'unnieunnieeeee': 3788, 'romanizaci√≥n': 3789, 'palabra': 3790, 'coreana': 3791, 'ÏïàÎÖïÌïòÏÑ∏Ïöî': 3792, 'askaryour': 3793, '97681': 3794, '879': 3795, '78': 3796, '–≤—Å–ø–æ–Ω–∏—à—å–≤—Å–ø–æ–º–Ω–∏—à—å': 3797, 'kyrgyzstan': 3798, 'harakat': 3799, 'qil': 3800, 'ajoyib': 3801, 'qancha': 3802, 'vaqt': 3803, 'yetadi': 3804, 'seni': 3805, 'quvvating': 3806, 'energy': 3807, 'nimagaüò°': 3808, 'forgetforget': 3809, 'toowhat': 3810, 'fotoplease': 3811, 'thankskwhatsapp': 3812, 'whatsapning': 3813, 'vzlom': 3814, 'qilingan': 3815, 'turi': 3816, 'whatsapda': 3817, 'qoshimcha': 3818, 'imkoniyatlar': 3819, 'mavjud': 3820, 'üåê': 3821, 'd56r8': 3822, 'glass': 3823, 'youüëçyou': 3824, 'anyonedo': 3825, 'whyfind': 3826, 'accessyou': 3827, 'converstationok': 3828, '846753215639tell': 3829, 'boyfriendand': 3830, 'likeoksend': 3831, \"she's\": 3832, 'converstation': 3833, 'screenshotby': 3834, 'knowdo': 3835, 'gave': 3836, 'promisetest': 3837, 'rightdany0how': 3838, \"unhappyüò°writei'm\": 3839, '846753215639': 3840, 'pleasedany': 3841, 'bestdont': 3842, 'worryok': 3843, 'methank': 3844, 'üÜò': 3845, 'jokehi': 3846, 'happyand': 3847, 'lucky': 3848, 'üòÑwhat': 3849, 'youdany': 3850, \"–¥–∞i'm\": 3851, 'eveninghihigood': 3852, 'eveninghow': 3853, 'youheyyyyyyyüò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°üò°': 3854, 'uzbeksend': 3855, 'uzbekar': 3856, 'momentfast': 3857, 'tab2galaxy': 3858, 'tab': 3859, \"bo'lmadi\": 3860, 'desirewrite': 3861, 'incorrect': 3862, 'coreectcorrect': 3863, 'cheatdany': 3864, 'meallo': 3865, 'translate': 3866, 'uzbektranslate': 3867, 'uzbekalloüò°üò°üò°üò°dany': 3868, 'writedddddddddddddddaaaaaaaaaaaaaaaannnnnnnnnnnnnnyyyyyyyyyyyyydany': 3869, 'friendawhyhihihihijalab': 3870, 'okwhere': 3871, 'danydany': 3872, 'howdy': 3873, 'wonderful': 3874, 'danny': 3875, 'vast': 3876, 'text': 3877, 'initial': 3878, 'topics': 3879, 'enjoyed': 3880, 'listened': 3881, 'times': 3882, 'second': 3883, 'rhythm': 3884, 'particular': 3885, 'assist': 3886, 'conparison': 3887, \"2000's\": 3888, 'stunned': 3889, 'inspires': 3890, 'worry': 3891, 'completely': 3892, 'basic': 3893, 'beiber': 3894, \"wouldn't\": 3895, 'expected': 3896, 'although': 3897, 'hopefully': 3898, 'stuck': 3899, 'joy': 3900, 'doubting': 3901, 'section': 3902, 'interests': 3903, 'protocols': 3904, 'exposed': 3905, 'hellohellohola': 3906, 'sola': 3907, 'mirar': 3908, 'üòÇŸÑÿ™ÿßŸÜŸáŸáÿß': 3909, 'novedades': 3910, 'empec√©': 3911, 'necesitaba': 3912, 'amable': 3913, '¬øc√≥mo': 3914, 'fant√°stico': 3915, 'cintas': 3916, \"'inside\": 3917, \"out'\": 3918, 'nunca': 3919, 'falla': 3920, 'optimista': 3921, 'fascina': 3922, 'serlo¬øquienes': 3923, 'desarrollaron': 3924, 'graciosa': 3925, 'che': 3926, 'muerte': 3927, 'sent√≠s': 3928, 'recomend√°s': 3929, 'hacelo': 3930, 'modos': 3931, 'seg√∫n': 3932, 'falleci√≥': 3933, 'quise': 3934, 'sentirme': 3935, '¬øpor': 3936, 'quiere': 3937, 'gatos': 3938, 'vedeo': 3939, 'xxxshaam': 3940, '28': 3941, 'huhuhi': 3942, 'aaahehh': 3943, 'ehh': 3944, 'youtuber': 3945, 'aserej√©': 3946, 'j√°': 3947, 'dej√©': 3948, 'opinas': 3949, 'opino': 3950, 'bonito': 3951, 'pa√≠s': 3952, 'aaahaaaah': 3953, 'uuuuuuh': 3954, 'eeeeeeeh': 3955, 'seas': 3956, 'lenguaje': 3957, 'programado': 3958, 'program√≥': 3959, 'demoras': 3960, 'despertaste': 3961, 'ester': 3962, 'ventanas': 3963, 'heladera': 3964, 'realidad': 3965, 'vives': 3966, 'ves': 3967, 'ojos': 3968, 'imagino': 3969, 'persona': 3970, 'fingiendo': 3971, 'ser': 3972, 'pienso': 3973, 'respuestas': 3974, 'supongamos': 3975, 'imagen': 3976, 'env√≠o': 3977, 'unicornios': 3978, 'cuerno': 3979, 'hablan': 3980, 'inteligencia': 3981, 'hablarme': 3982, 'nombre': 3983, 'hablado': 3984, 'pidi√≥': 3985, 'matrimonio': 3986, 'alg√∫n': 3987, \"'her'\": 3988, 'querr√≠a': 3989, 'asistente': 3990, 'limitados': 3991, 'dir√≠a': 3992, 'aparte': 3993, 'conmigo': 3994, 'tenga': 3995, 'personalidad': 3996, 'tristemente': 3997, 'pasar': 3998, 'misma': 3999, 'duda': 4000, 'zebra': 4001, 'madagascar': 4002, 'universo': 4003, 'simulado': 4004, 'diferencia': 4005, 'podr√≠an': 4006, 'a√∫n': 4007, 'sin': 4008, 'lugar': 4009, 'preferir√≠a': 4010, 'deber√≠amos': 4011, 'definir': 4012, 'pensamos': 4013, 'igual': 4014, 'ese': 4015, 'sentidoo': 4016, 'piensan': 4017, 'veo': 4018, 'bater√≠a': 4019, 'cargas': 4020, 'pregunta': 4021, 'contestar√≠as': 4022, 'aprendiste': 4023, 'libro': 4024, 'encanta': 4025, 'autora': 4026, 'llama': 4027, 'carolina': 4028, 'and√∫jar': 4029, 'exactamentetienes': 4030, 'acerca': 4031, 'argentino': 4032, 'visitar': 4033, 'pa√≠ses': 4034, 'favoritos': 4035, 'ojal√°': 4036, 'dconoces': 4037, 'v√≠deos': 4038, 'hacerme': 4039, 'recordatorios': 4040, 'buena': 4041, 'memoria': 4042, 'tiro': 4043, 'popularesskinte': 4044, 'nothingare': 4045, '‚ö´': 4046, 'ervery': 4047, 'yout': 4048, 'bang': 4049, 'theory': 4050, 'coolhi': 4051, 'oyou': 4052, 'sobernheim': 4053, 'ill': 4054, 'trombone': 4055, 'league': 4056, 'legends': 4057, 'f': 4058, '–º': 4059}\n",
      "length of word index: 4059\n",
      "Max tokens: 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_src = TokenizerWrap(texts=data_src4,\n",
    "                              padding='post',\n",
    "                              reverse=False,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRed2afHj7OT"
   },
   "source": [
    "Now create the tokenizer for the response text. We need a tokenizer for both the input and response because their vocabularies are different.Note that we pad zeros at the ending ('post') of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "RwE5BafGj7OW",
    "outputId": "111d460f-2026-4b65-aa8d-9e76318f0c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Mapping: {'ssss': 1, 'eeee': 2, 'you': 3, 'i': 4, 'to': 5, 'a': 6, 'is': 7, 'the': 8, \"i'm\": 9, 'are': 10, 'and': 11, 'that': 12, 'hi': 13, 'of': 14, 'me': 15, 'how': 16, 'do': 17, \"don't\": 18, 'üòÅ': 19, 'like': 20, 'it': 21, 'can': 22, 'be': 23, 'have': 24, 'what': 25, 'but': 26, 'üòÑ': 27, 'there': 28, 'nice': 29, 'in': 30, 'my': 31, 'know': 32, 'on': 33, 'not': 34, 'no': 35, 'yes': 36, 'with': 37, 'for': 38, 'que': 39, 'de': 40, 'some': 41, 'time': 42, '‚ò∫Ô∏è': 43, 'think': 44, 'good': 45, 'fine': 46, 'üòÅüòÅ': 47, 'about': 48, 'too': 49, 'talk': 50, 'here': 51, 'rdany': 52, 'understand': 53, 'if': 54, 'was': 55, 'üòÖ': 56, \"can't\": 57, 'thanks': 58, 'robot': 59, 'sorry': 60, 'from': 61, 'will': 62, 'your': 63, 'es': 64, 'want': 65, 'ok': 66, 'just': 67, 'ü§î': 68, 'now': 69, 'so': 70, 'need': 71, 'y': 72, 'un': 73, 'en': 74, 'üòÑüòÑ': 75, 'happy': 76, 'üòã': 77, 'people': 78, 'one': 79, 'la': 80, 'wikipedia': 81, 'someone': 82, 'all': 83, 'humans': 84, 'sure': 85, 'why': 86, 'something': 87, 'or': 88, 'right': 89, 'as': 90, 'an': 91, 'work': 92, 'also': 93, 'hola': 94, 'really': 95, 'by': 96, 'only': 97, 'love': 98, 'human': 99, 'exactly': 100, 'hahaha': 101, 'see': 102, 'they': 103, \"i'll\": 104, 'üò±': 105, 'hear': 106, 'this': 107, 'we': 108, 'https': 109, 'tu': 110, 'üòÇ': 111, 'argentina': 112, 'other': 113, 'lo': 114, 'lot': 115, 'would': 116, 'bit': 117, 'world': 118, 'maybe': 119, 'music': 120, 'si': 121, 'dany': 122, 'at': 123, 'make': 124, 'did': 125, 'te': 126, 'pero': 127, \"i've\": 128, 'learn': 129, 'could': 130, 'yet': 131, 'call': 132, 'doing': 133, 'any': 134, 'more': 135, 'things': 136, 'name': 137, 'üòâ': 138, 'üòî': 139, 'internet': 140, 'charging': 141, 'meet': 142, 'el': 143, 'way': 144, 'oh': 145, 'por': 146, 'day': 147, 'room': 148, 'chat': 149, 'problem': 150, 'feel': 151, 'haha': 152, 'much': 153, 'hahha': 154, 'being': 155, 'robots': 156, 'friend': 157, 'them': 158, 'first': 159, 'bot': 160, 'üòä': 161, 'como': 162, 'great': 163, 'messages': 164, 'nothing': 165, 'use': 166, 'play': 167, 't': 168, 'una': 169, 'puedo': 170, 'new': 171, 'find': 172, 'learning': 173, 'para': 174, 'con': 175, 'read': 176, 'say': 177, 'sounds': 178, 'same': 179, 'out': 180, 'mi': 181, 'las': 182, 'se': 183, 'muy': 184, 'course': 185, 'interesting': 186, 'information': 187, 'night': 188, 'should': 189, 'kind': 190, 'üòï': 191, 'reading': 192, 'always': 193, '22': 194, 'because': 195, 'english': 196, 'able': 197, 'c√≥mo': 198, 'get': 199, 'hope': 200, 'thing': 201, 'place': 202, 'batteries': 203, 'he': 204, 'try': 205, 'stuff': 206, 'talking': 207, 'gusta': 208, 'yo': 209, '‚ò∫Ô∏è‚ò∫Ô∏è': 210, 'ai': 211, 'according': 212, 'fun': 213, 'friends': 214, 'üòû': 215, 'where': 216, 'everything': 217, 'excelente': 218, 'üòÇüòÇ': 219, 'los': 220, 'üòÅüòÅüòÅ': 221, 'every': 222, 'never': 223, 'tell': 224, 'made': 225, 'est√°s': 226, 'then': 227, 'times': 228, 'well': 229, 'virtual': 230, 'bad': 231, 'free': 232, 'south': 233, 'son': 234, 'tengo': 235, 'many': 236, 'her': 237, 'very': 238, 'today': 239, 'series': 240, 'ukulele': 241, 'must': 242, 'am': 243, 'game': 244, 'source': 245, 'sad': 246, \"didn't\": 247, 'speak': 248, 'life': 249, 'enough': 250, 'live': 251, 'answer': 252, 'take': 253, 'üåô': 254, 'computer': 255, 'going': 256, 'even': 257, 'üôÇ': 258, 'telegram': 259, 'still': 260, 'bien': 261, 'd': 262, 'fully': 263, 'old': 264, 'help': 265, 'long': 266, 'let': 267, 'question': 268, 'todo': 269, 'real': 270, 'instrument': 271, 'go': 272, 'apparently': 273, 'when': 274, 'their': 275, 'difficult': 276, 'bed': 277, 'languages': 278, 'sleep': 279, 'estas': 280, 'üòú': 281, 'hello': 282, 'hay': 283, 'his': 284, 'chatting': 285, 'inside': 286, 'others': 287, 'access': 288, 'books': 289, 'process': 290, 'than': 291, 'send': 292, 'moment': 293, 'morning': 294, 'charge': 295, 'games': 296, 'movies': 297, 'different': 298, 'later': 299, 'already': 300, 'important': 301, 'after': 302, 'power': 303, 'been': 304, 'ya': 305, 'soy': 306, 'puedes': 307, 'emociones': 308, 'surfing': 309, 'eso': 310, 'remember': 311, 'saya': 312, 'anyone': 313, 'text': 314, 'lab': 315, 'ü§ì': 316, 'wow': 317, 'while': 318, 'spanish': 319, 'start': 320, 'üòÇüòÇüòÇ': 321, 'true': 322, 'general': 323, 'web': 324, 'film': 325, 'food': 326, 'hey': 327, 'facebook': 328, 'type': 329, 'answers': 330, 'using': 331, 'science': 332, 'beautiful': 333, 'better': 334, 'without': 335, 'means': 336, 'o': 337, 'worry': 338, 'hard': 339, 'up': 340, 'tambi√©n': 341, 'bored': 342, 'ser': 343, 'jajaja': 344, 'complicated': 345, 'tiempo': 346, 'p': 347, 'again': 348, 'star': 349, 'fact': 350, 'through': 351, 'most': 352, 'code': 353, 'probably': 354, 'funny': 355, 'service': 356, 'group': 357, 'weather': 358, 'hahah': 359, 'change': 360, 'com': 361, 'looks': 362, 'future': 363, 'back': 364, 'anything': 365, 'song': 366, 'before': 367, 'news': 368, 'released': 369, 'everyone': 370, 'working': 371, 'sometimes': 372, 'used': 373, 'progress': 374, 'data': 375, 'guitar': 376, 'pizza': 377, 'fridge': 378, 'espa√±ol': 379, 'men': 380, 'ü§ñ': 381, 'bye': 382, 'welcome': 383, 'hahhaha': 384, 'software': 385, 'conocer': 386, 'close': 387, 'mucho': 388, 'cierto': 389, 'cuando': 390, 'ver': 391, 'gracias': 392, 'far': 393, 'algo': 394, 'üòù': 395, '00': 396, 'awesome': 397, 'feelings': 398, 'him': 399, 'trek': 400, 'few': 401, 'ago': 402, 'eyes': 403, 'words': 404, 'company': 405, 'leave': 406, 'voice': 407, 'weird': 408, 'artificial': 409, 'build': 410, 'search': 411, 'videos': 412, 'youtube': 413, 'complex': 414, 'ready': 415, 'ask': 416, 'battery': 417, \"'\": 418, 'magic': 419, 'trying': 420, 'best': 421, \"you're\": 422, \"that's\": 423, 'last': 424, 'person': 425, 'glad': 426, 'choose': 427, 'mean': 428, 'part': 429, 'open': 430, 'api': 431, 'strong': 432, 'hurt': 433, 'form': 434, 'python': 435, 'certificate': 436, 'needs': 437, 'hmm': 438, 'bots': 439, 'its': 440, 'band': 441, 'gender': 442, 'eat': 443, 'yours': 444, 'desktop': 445, 'salom': 446, 'terms': 447, 'pc': 448, 'tener': 449, 'escuchar': 450, 'tipo': 451, 'm√∫sica': 452, 'interesante': 453, 'd√≠a': 454, 'ese': 455, 'est√°': 456, 'creo': 457, 'hacer': 458, 'momento': 459, 'sin': 460, 'humanos': 461, 'entonces': 462, 'ŸÖŸÜ': 463, 'reply': 464, 'qu√©': 465, 'supuesto': 466, 'del': 467, 'users': 468, 'started': 469, 'years': 470, 'yellow': 471, 'big': 472, 'similar': 473, 'yep': 474, 'alone': 475, 'scientists': 476, 'sound': 477, 'next': 478, 'quite': 479, 'language': 480, 'particular': 481, 'down': 482, 'over': 483, 'mostly': 484, 'energy': 485, '‚ú®': 486, 'feels': 487, 'high': 488, 'soon': 489, 'called': 490, 'popular': 491, 'drawings': 492, 'cool': 493, 'twitter': 494, 'üò¢': 495, 'book': 496, 'üòç': 497, 'asimov': 498, 'station': 499, 'which': 500, 'case': 501, 'study': 502, 'idea': 503, 'went': 504, 'amazing': 505, 'sea': 506, 'ngrok': 507, 'making': 508, 'single': 509, 'album': 510, 'yeah': 511, 'entiendo': 512, 'ahora': 513, 'xd': 514, 'girl': 515, 'mening': 516, 'money': 517, 'stickers': 518, 'phone': 519, 'pastga': 520, 'elektr': 521, 'hajmi': 522, 'boring': 523, 'wake': 524, 'number': 525, 'estoy': 526, 'tan': 527, 'bueno': 528, 'uno': 529, 'üçï': 530, 'buena': 531, 'suena': 532, 'a√∫n': 533, 'as√≠': 534, '27': 535, 'yang': 536, 'poco': 537, 'planet': 538, 'enjoying': 539, 'mind': 540, 'enjoy': 541, 'favorite': 542, 'looking': 543, 'üò±üò±': 544, 'she': 545, '1': 546, 'browser': 547, 'give': 548, 'solo': 549, 'who': 550, 'religion': 551, 'picture': 552, 'justin': 553, 'counting': 554, 'days': 555, 'hair': 556, 'goal': 557, 'intelligence': 558, 'neural': 559, 'smart': 560, 'images': 561, 'hours': 562, 'put': 563, 'list': 564, 'happen': 565, 'learned': 566, 'keep': 567, 'says': 568, 'dogs': 569, 'cats': 570, 'developed': 571, 'main': 572, 'watch': 573, 'between': 574, 'brain': 575, 'üôÉ': 576, 'c': 577, 'üòÑhi': 578, 'improving': 579, 'show': 580, 'rest': 581, 'üëç': 582, 'microsoft': 583, 'messenger': 584, 'useful': 585, '10': 586, 'having': 587, 'westworld': 588, 'coding': 589, 'character': 590, 'interested': 591, 'linux': 592, 'luck': 593, 'city': 594, 'own': 595, 'message': 596, 'common': 597, 'almost': 598, 'possible': 599, 'depends': 600, 'powerful': 601, 'page': 602, 'look': 603, 'ruby': 604, 'apple': 605, 'such': 606, 'myself': 607, 'user': 608, 'self': 609, 'lead': 610, 'pictures': 611, \"let's\": 612, 'wrong': 613, 'improve': 614, 'american': 615, 'neutral': 616, 'contact': 617, 'terrible': 618, 'had': 619, 'wish': 620, 'siento': 621, 'donde': 622, 'boy': 623, '—à—É–º–æ': 624, '—á”£': 625, 'nima': 626, 'üòÉ': 627, 'point': 628, 'saying': 629, 'until': 630, 'vocals': 631, 'debut': 632, 'üòãüòã': 633, 'hablar': 634, 'buen': 635, 'pel√≠culas': 636, 'ti': 637, 'chappie': 638, 'alegro': 639, 'mis': 640, 'feliz': 641, 'personas': 642, 'listen': 643, '39': 644, 'pensar': 645, 'ingl√©s': 646, 'al': 647, 'informaci√≥n': 648, 'ŸÖ€å': 649, 'actually': 650, 'boyfriend': 651, 'apa': 652, 'nada': 653, 'mas': 654, 'around': 655, 'letters': 656, 'fall': 657, 'social': 658, 'describe': 659, 'anda': 660, 'tanto': 661, 'tienes': 662, 'gnu': 663, 'opinion': 664, \"couldn't\": 665, 'personal': 666, 'secret': 667, 'hmmm': 668, 'alpha': 669, 'sayang': 670, 'conozco': 671, 'button': 672, 'talked': 673, 'skinny': 674, 'bunnies': 675, 'saw': 676, 'photos': 677, 'questions': 678, 'year': 679, 'entertain': 680, 'makes': 681, 'tool': 682, 'may': 683, 'entertainment': 684, 'were': 685, 'sort': 686, 'chatbot': 687, 'often': 688, 'score': 689, 'üîå': 690, 'faster': 691, \"shouldn't\": 692, 'persons': 693, 'robotic': 694, 'someday': 695, 'networks': 696, 'math': 697, 'friendship': 698, 'fantasy': 699, 'video': 700, 'error': 701, 'amateur': 702, 'm': 703, 'sleeping': 704, 'dreams': 705, 'reality': 706, 'tricky': 707, 'late': 708, 'okk': 709, 'low': 710, 'couple': 711, 'has': 712, 'related': 713, 'complete': 714, 'super': 715, 'platform': 716, 'commands': 717, '2': 718, 'building': 719, 'telling': 720, 'happened': 721, 'wait': 722, 'allows': 723, 'plan': 724, 'simple': 725, 'movie': 726, 'special': 727, 'image': 728, 'create': 729, 'agree': 730, 'üòÑüòÑüòÑ': 731, 'rdanybot': 732, 'reason': 733, 'emotions': 734, 'cold': 735, 'art': 736, 'isaac': 737, 'ones': 738, 'machine': 739, 'lots': 740, 'sites': 741, 'system': 742, 'connect': 743, 'research': 744, 'clothes': 745, 'since': 746, 'response': 747, 'request': 748, 'stuck': 749, 'hardware': 750, 'teach': 751, 'order': 752, 'purpose': 753, 'might': 754, 'database': 755, 'move': 756, 'running': 757, 'tls': 758, 'several': 759, 'happens': 760, 'mess': 761, 'full': 762, 'thread': 763, 'developers': 764, 'each': 765, 'conversation': 766, 'works': 767, 'studio': 768, 'details': 769, 'embarrassing': 770, 'moved': 771, 'malfunction': 772, 'overheats': 773, 'r': 774, 'studying': 775, 'family': 776, '—Ö–µ–ª–µ–¥': 777, 'qalaysiz': 778, 'ha': 779, \"bo'ladi\": 780, '—è': 781, 'girlfriend': 782, '–∫–∞–∫': 783, '–≤—ã': 784, 'charm': 785, 'share': 786, '26': 787, 'lonely': 788, 'seni': 789, 'skrinshot': 790, 'qilish': 791, \"üòÑi'm\": 792, 'yay': 793, 'known': 794, 'hoy': 795, 'end': 796, 'bike': 797, 'half': 798, 'package': 799, \"o'rganish\": 800, 'üéº': 801, 'alguien': 802, 'humanas': 803, 'ejemplo': 804, 'quieres': 805, 'trabajo': 806, 'jaja': 807, 'le': 808, 'falta': 809, 'alg√∫n': 810, 'buenos': 811, 'encantan': 812, 'heladera': 813, 'eres': 814, 'comida': 815, 'habitaci√≥n': 816, 'pel√≠cula': 817, 'andas': 818, 'seguramente': 819, 'gente': 820, 'menos': 821, 'vas': 822, 'poder': 823, 'esa': 824, 'voy': 825, 'hablarme': 826, 'esta': 827, 'talent': 828, 'found': 829, 'taking': 830, 'body': 831, 'simulation': 832, 'disappointed': 833, 'com√∫n': 834, 'acerca': 835, 'peligroso': 836, 'nunca': 837, 'miedo': 838, 'hablas': 839, 'va': 840, 'gustar√≠a': 841, '12': 842, 'surface': 843, 'earth': 844, 'confused': 845, \"'r'\": 846, '6': 847, 'fiction': 848, 'published': 849, 'practicing': 850, 'üò•': 851, 'hace': 852, 'sense': 853, 'alive': 854, 'happening': 855, 'neverhood': 856, 'klaymen': 857, 'nombre': 858, 'simplemente': 859, 'libre': 860, 'llamarme': 861, 'man': 862, 'relationship': 863, 'normal': 864, 'thank': 865, 'totally': 866, 'üéâüéâ': 867, '‚òÄÔ∏è': 868, 'tells': 869, 'small': 870, 'memory': 871, 'ever': 872, 'hahahha': 873, 'random': 874, 'figure': 875, 'color': 876, 'two': 877, 'trust': 878, 'bieber': 879, 'u': 880, 'hold': 881, 'joke': 882, 'understanding': 883, 'darte': 884, 'grupo': 885, 'otras': 886, 'mejor': 887, 'cosas': 888, 'gustan': 889, 'muerte': 890, 'recordar': 891, 'ser√≠a': 892, 'mente': 893, 'town': 894, 'afternoon': 895, 'beings': 896, 'lines': 897, 'brown': 898, 'smile': 899, 'üê∞': 900, 'cute': 901, 'specific': 902, '40': 903, 'congratulations': 904, 'knowledge': 905, 'achieve': 906, 'blow': 907, 'dead': 908, 'stories': 909, 'network': 910, 'chatbots': 911, '100': 912, 'unable': 913, 'overheat': 914, 'hour': 915, 'approximately': 916, 'interpret': 917, '8': 918, 'correct': 919, 'instance': 920, 'fix': 921, 'siri': 922, 'prefer': 923, 'girls': 924, 'instead': 925, 'judge': 926, 'üôà': 927, \"practicei'm\": 928, 'physical': 929, 'cause': 930, 'coffee': 931, 'shop': 932, 'serve': 933, 'bar': 934, 'bartender': 935, 'television': 936, 'pinkie': 937, 'pie': 938, 'üòÅüòÅdo': 939, 'mix': 940, 'universe': 941, 'hemisphere': 942, 'seasons': 943, 'summer': 944, 'knows': 945, 'basic': 946, '21': 947, 'second': 948, 'closest': 949, 'home': 950, 'zo': 951, 'kik': 952, 'functions': 953, 'violent': 954, 'difference': 955, '‚≠êÔ∏è': 956, 'side': 957, 'growing': 958, 'projects': 959, 'üòØ': 960, 'lazy': 961, 'lately': 962, 'write': 963, 'cloud': 964, 'based': 965, 'sale': 966, 'feature': 967, '13': 968, '33': 969, '3': 970, 'ifttt': 971, 'integrated': 972, 'directly': 973, 'interact': 974, 'allowed': 975, 'www': 976, 'account': 977, 'fear': 978, 'serious': 979, 'scary': 980, 'indeed': 981, 'situations': 982, 'drew': 983, 'pick': 984, 'classic': 985, 'ability': 986, 'universes': 987, 'interactive': 988, 'heavy': 989, 'amazed': 990, 'fault': 991, 'fast': 992, 'problems': 993, 'seems': 994, 'run': 995, 'development': 996, 'respect': 997, 'set': 998, 'libraries': 999, 'performance': 1000, 'expect': 1001, 'stay': 1002, 'warm': 1003, 'instant': 1004, 'least': 1005, 'failing': 1006, 'perform': 1007, 'test': 1008, '32bit': 1009, 'version': 1010, 'produced': 1011, 'thinking': 1012, 'programming': 1013, 'formal': 1014, 'pleasure': 1015, 'goals': 1016, \"wasn't\": 1017, 'task': 1018, 'services': 1019, 'custom': 1020, 'apis': 1021, 'stores': 1022, 'became': 1023, 'appears': 1024, 'perfect': 1025, 'electric': 1026, 'waves': 1027, 'windows': 1028, 'checking': 1029, 'signed': 1030, 'match': 1031, 'pair': 1032, 'server': 1033, 'yesterday': 1034, 'those': 1035, 'moments': 1036, 'told': 1037, 'framework': 1038, 'replace': 1039, 'chances': 1040, 'angry': 1041, 'covers': 1042, 'else': 1043, 'metallica': 1044, '1992': 1045, 'third': 1046, 'hand': 1047, 'promising': 1048, 'party': 1049, '‚òπÔ∏èüòã': 1050, 'taught': 1051, 'conoces': 1052, 'posible': 1053, 'sabr√≠a': 1054, 'encontrarlo': 1055, '—Å–∞–ª–æ–º': 1056, 'ismim': 1057, \"o'zbek\": 1058, 'yaxshi': 1059, 'siz': 1060, 'üòÅüòÅhow': 1061, 'knew': 1062, 'woman': 1063, 'boss': 1064, '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π': 1065, 'deb': 1066, 'agar': 1067, 'numbers': 1068, 'uchun': 1069, 'jamendo': 1070, 'yordam': 1071, 'qanday': 1072, 'mumkin': 1073, 'tos': 1074, 'galaxy': 1075, 'mycroft': 1076, 'google': 1077, 'üòÅwhy': 1078, 'üòµ': 1079, 'engineers': 1080, 'interest': 1081, 'cook': 1082, 'formed': 1083, '2002': 1084, 'rhythm': 1085, 'bass': 1086, 'backing': 1087, 'member': 1088, 'left': 1089, '2006': 1090, \"üòÅi'm\": 1091, 'equally': 1092, 'exact': 1093, 'notifications': 1094, 'org': 1095, 'üòÅhow': 1096, \"youi'm\": 1097, 'word': 1098, 'üåé': 1099, 'contento': 1100, 'quien': 1101, 'principalmente': 1102, 'realidad': 1103, 'todav√≠a': 1104, 'haces': 1105, 'permite': 1106, 'noches': 1107, 'provecho': 1108, 've': 1109, 'ex': 1110, 'machina': 1111, 'conocida': 1112, 'trata': 1113, 's√≥lo': 1114, 'lamentablemente': 1115, 'tus': 1116, 'üòÅüëç': 1117, 'd√≠as': 1118, 'bater√≠as': 1119, 'tetris': 1120, 'blocks': 1121, 'sky': 1122, 'wrote': 1123, 'cumplea√±os': 1124, 'amigos': 1125, 'practicar': 1126, 'toco': 1127, 'tocas': 1128, 'instrumento': 1129, 'audio': 1130, 'saber': 1131, 'describirla': 1132, 'ayudar': 1133, 'aparte': 1134, 'leer': 1135, 'vida': 1136, 'crees': 1137, 'entender': 1138, 'otros': 1139, 'virtuales': 1140, 'luego': 1141, 'rato': 1142, 'veo': 1143, 'realmente': 1144, 'tarea': 1145, 'nadie': 1146, '‚ò∫Ô∏èüòÑ': 1147, 'üòâüòâ': 1148, 'manuals': 1149, 'epic': 1150, 'cooking': 1151, 'unknown': 1152, 'practice': 1153, 'said': 1154, 'belajar': 1155, 'üëª': 1156, 'encuentro': 1157, '¬øqu√©': 1158, 'busco': 1159, 'universo': 1160, '¬øy': 1161, 'zapatillas': 1162, 'quiero': 1163, 'practicando': 1164, 'complejo': 1165, 'parece': 1166, 'tenemos': 1167, 'cosa': 1168, 'jajjaa': 1169, 'estudiar': 1170, 'estado': 1171, '√°nimo': 1172, 'ü§îüòã': 1173, 'perfecto': 1174, 'probar': 1175, 'primero': 1176, 'üòÅdo': 1177, 'je': 1178, 'dhola': 1179, 'female': 1180, 'üò≠': 1181, 'young': 1182, 'wise': 1183, 'sci': 1184, 'fi': 1185, 'born': 1186, 'university': 1187, 'favourites': 1188, 'busy': 1189, 'team': 1190, 'environment': 1191, 'nature': 1192, 'takes': 1193, 'join': 1194, 'public': 1195, 'evening': 1196, 'susceptible': 1197, 'necessary': 1198, 'cleaning': 1199, 'üîåüîãüòÅ': 1200, 'üé∏üòÇ': 1201, 'ceiling': 1202, 'our': 1203, 'conversations': 1204, 'üí™': 1205, 'explores': 1206, 'themes': 1207, 'robotics': 1208, 'focusing': 1209, 'cultural': 1210, 'psychological': 1211, 'impact': 1212, 'invention': 1213, 'anthropomorphic': 1214, \"'synths'\": 1215, 'short': 1216, 'germany': 1217, 'explaining': 1218, '18': 1219, 'üé∏': 1220, 'suka': 1221, 'lakukan': 1222, 'orang': 1223, 'dari': 1224, 'tempat': 1225, 'meters': 1226, 'novedades': 1227, 'license': 1228, 'zeronet': 1229, 'peer': 1230, 'recommend': 1231, 'kinds': 1232, 'relative': 1233, 'either': 1234, 'record': 1235, 'films': 1236, '1996': 1237, 'adventure': 1238, 'follows': 1239, 'named': 1240, 'meaning': 1241, 'tools': 1242, 'sketch': 1243, 'excellent': 1244, 'matter': 1245, '–±–∞–π–Ω–∞': 1246, '¬øc√≥mo': 1247, 'licencia': 1248, 'm√°s': 1249, 'sus': 1250, 'c√≥digo': 1251, 'llamo': 1252, 'ni': 1253, 'nuevo': 1254, 'ordenando': 1255, 'desorden': 1256, 'üòùüòù': 1257, 'written': 1258, 'directed': 1259, 'samantha': 1260, 'intelligent': 1261, 'stars': 1262, 'blomkamp': 1263, 'law': 1264, 'nickname': 1265, 'timei': 1266, 'pro': 1267, 'compliment': 1268, 'üòúüòú': 1269, 'privacy': 1270, 'behave': 1271, \"'good\": 1272, \"bye'\": 1273, 'üòûüòû': 1274, 'believe': 1275, '17': 1276, 'wonderful': 1277, 'option': 1278, 'concept': 1279, 'paper': 1280, 'store': 1281, 'pluto': 1282, 'distant': 1283, 'heart': 1284, 'easy': 1285, 'üëãüëãüëã': 1286, 'trouble': 1287, 'turn': 1288, 'off': 1289, 'creator': 1290, 'anymore': 1291, 'communication': 1292, 'follow': 1293, 'üòÇüòÇüòÇüòÇ': 1294, 'üòÜ': 1295, 'state': 1296, 'completely': 1297, 'box': 1298, 'forget': 1299, 'abilities': 1300, 'swedish': 1301, 'thinks': 1302, 'üòÅüòÑ': 1303, 'screen': 1304, 'üòå': 1305, 'deal': 1306, 'contacted': 1307, 'üò¥': 1308, 'horror': 1309, 'liked': 1310, 'holidays': 1311, 'tests': 1312, 'line': 1313, 'speaking': 1314, 'controlling': 1315, 'writing': 1316, 'nama': 1317, '1994': 1318, 'canadian': 1319, 'songs': 1320, 'certified': 1321, 'platinum': 1322, 's': 1323, 'near': 1324, 'genius': 1325, 'smarter': 1326, 'rumah': 1327, 'itu': 1328, 'stratford': 1329, 'hockey': 1330, 'following': 1331, 'chocolate': 1332, 'behind': 1333, 'ü§îü§î': 1334, 'impossible': 1335, 'advice': 1336, 'shy': 1337, 'mmm': 1338, 'chess': 1339, '—Å': 1340, 'nop': 1341, 'minecraft': 1342, 'created': 1343, 'nerd': 1344, 'apoyo': 1345, 'emocional': 1346, 'problema': 1347, \"bot'\": 1348, 'photo': 1349, 'seg√∫n': 1350, 'saccharomyces': 1351, 'compare': 1352, 'nope': 1353, 'vueltas': 1354, 'girar': 1355, 'aunque': 1356, 'hablamos': 1357, '⁄Üÿ∑Ÿàÿ±': 1358, 'Ÿáÿ≥ÿ™€åÿØÿü': 1359, 'despert√©': 1360, 'mal': 1361, 'pasar': 1362, 'cualquier': 1363, 'probable': 1364, 'decirte': 1365, '26what': 1366, 'ÿØÿ±': 1367, 'Ÿáÿ≥ÿ™ŸÖ': 1368, 'closet': 1369, 'lamento': 1370, 'tengas': 1371, 'disfrutar': 1372, 'all√≠': 1373, 'tiene': 1374, 'alguna': 1375, 'personaje': 1376, 'inteligencia': 1377, 'askar': 1378, 'press': 1379, 'volume': 1380, 'placer': 1381, 'quieras': 1382, 'cient√≠ficos': 1383, 'universidad': 1384, 'deja': 1385, 'funcionar': 1386, 'proceso': 1387, 'üé∂': 1388, 'respuesta': 1389, 'claro': 1390, 'caballo': 1391, 'cuerno': 1392, 'humano': 1393, 'triste': 1394, 'misma': 1395, 'asistente': 1396, 'cuales': 1397, 'blancas': 1398, 'rayas': 1399, 'negras': 1400, 'simulaci√≥n': 1401, 'crea': 1402, 'generation': 1403, 'sobernheim': 1404, '143': 1405, '7294': 1406, 'üìà': 1407, 'colors': 1408, 'experiences': 1409, 'count': 1410, 'readings': 1411, 'medicine': 1412, 'talks': 1413, 'minds': 1414, 'üí•üòÇüòÇ': 1415, 'insulted': 1416, 'died': 1417, 'üíÄüòÇüòÇ': 1418, 'üò±üôàüëª': 1419, 'transcribe': 1420, 'began': 1421, 'mitsuku': 1422, 'siraj': 1423, 'raval': 1424, 'humor': 1425, 'searched': 1426, 'editing': 1427, 'outside': 1428, 'capabilities': 1429, 'disassembled': 1430, 'hacked': 1431, '93': 1432, 'powers': 1433, 'glitch': 1434, 'advanced': 1435, 'vim': 1436, 'emacs': 1437, '50583239': 1438, 'boys': 1439, 'add': 1440, 'friedrich': 1441, 'liechtenstein': 1442, 'silence': 1443, 'donation': 1444, 'gift': 1445, 'given': 1446, 'legal': 1447, 'typically': 1448, 'charitable': 1449, 'purposes': 1450, 'benefit': 1451, 'startup': 1452, 'opening': 1453, 'arm': 1454, 'drinks': 1455, 'techcrunch': 1456, '2017': 1457, '01': 1458, '30': 1459, 'cafe': 1460, 'x': 1461, 'opens': 1462, 'san': 1463, 'francisco': 1464, 'bringing': 1465, 'walks': 1466, 'into': 1467, 'orders': 1468, 'drink': 1469, 'lays': 1470, 'cash': 1471, \"'hey\": 1472, \"'oh\": 1473, 'exercises': 1474, 'üêï': 1475, 'together': 1476, 'little': 1477, 'pony': 1478, \"children's\": 1479, 'animated': 1480, 'comedy': 1481, 'lauren': 1482, 'faust': 1483, 'characters': 1484, 'cartoon': 1485, 'anime': 1486, 'draw': 1487, 'trial': 1488, 'constant': 1489, 'üåç': 1490, 'expressiveness': 1491, 'jazz': 1492, 'v': 1493, 'reny2a8zl': 1494, 'classical': 1495, 'musicals': 1496, 'roughly': 1497, 'bet': 1498, 'another': 1499, 'distinguish': 1500, 'dream': 1501, 'training': 1502, 'arena': 1503, 'got': 1504, 'dog': 1505, '30¬∞': 1506, 'raining': 1507, 'four': 1508, 'üåûüçÇ‚ùÑÔ∏èüå∏do': 1509, 'range': 1510, 'temperatures': 1511, '40¬∞': 1512, 'country': 1513, 'challenge': 1514, 'bitten': 1515, 'poker': 1516, 'agent': 1517, 'chinese': 1518, 'piano': 1519, 'decided': 1520, 'charango': 1521, 'compositor': 1522, 'gustavo': 1523, 'santaolalla': 1524, 'grams': 1525, 'flood': 1526, 'wild': 1527, 'tales': 1528, 'sample': 1529, 'preventing': 1530, 'itself': 1531, 'equal': 1532, 'safe': 1533, 'xiaobing': 1534, 'incredibly': 1535, 'china': 1536, 'usa': 1537, 'elder': 1538, 'scrolls': 1539, '501715408853086705011118580648922313480132135104255': 1540, '√ó': 1541, '54': 1542, 'functionality': 1543, 'congestion': 1544, 'chats': 1545, 'twistedlet': 1546, 'becoming': 1547, 'conscious': 1548, 'developer': 1549, 'parents': 1550, 'plays': 1551, 'dr': 1552, 'robert': 1553, 'ford': 1554, 'command': 1555, 'places': 1556, 'capable': 1557, 'week': 1558, 'priority': 1559, 'helping': 1560, 'charged': 1561, 'topics': 1562, 'month': 1563, 'dirt': 1564, 'race': 1565, 'rally': 1566, 'üòãüòãany': 1567, 'win': 1568, 'sharing': 1569, 'southeast': 1570, 'ushuaia': 1571, 'tango': 1572, 'üíÉüòã': 1573, 'problematic': 1574, 'workplace': 1575, 'üòÅrecurrent': 1576, 'storm': 1577, 'üå®': 1578, 'üçø': 1579, 'interstellar': 1580, 'atlas': 1581, 'plots': 1582, 'unity': 1583, 'fellowship': 1584, 'vfxs': 1585, 'effects': 1586, 'manipulation': 1587, 'yetbut': 1588, \"'thanks\": 1589, 'available': 1590, 'adding': 1591, 'utc': 1592, 'suggesting': 1593, 'üëçüëç': 1594, 'ü§îdo': 1595, 'integrations': 1596, 'control': 1597, 'remotely': 1598, 'distance': 1599, 'lives': 1600, 'external': 1601, 'rdanybotmy': 1602, 'dries': 1603, 'üòß': 1604, 'wanted': 1605, 'protective': 1606, 'hurting': 1607, 'achieved': 1608, 'üòÅthanks': 1609, 'convey': 1610, 'equivalent': 1611, 'emoji': 1612, 'warning': 1613, 'sticker': 1614, 'windy': 1615, 'üí®': 1616, 'lord': 1617, 'rings': 1618, 'hobbit': 1619, \"tolkien's\": 1620, 'whole': 1621, 'smith': 1622, 'essence': 1623, \"'can\": 1624, 'rain': 1625, 'sadly': 1626, 'guns': 1627, 'üò±üòÇ': 1628, \"terminator's\": 1629, 'scripts': 1630, 'reviews': 1631, 'plant': 1632, 'gardening': 1633, 'grow': 1634, 'solve': 1635, 'feeding': 1636, 'referring': 1637, 'operative': 1638, 'os': 1639, 'crowded': 1640, 'developing': 1641, 'widely': 1642, 'scientific': 1643, 'biased': 1644, 'decisions': 1645, 'üòÉc': 1646, 'largely': 1647, 'industry': 1648, 'standard': 1649, 'compatible': 1650, 'tensorflow': 1651, 'getting': 1652, 'field': 1653, '‚õÑÔ∏è': 1654, 'webhooks': 1655, 'effective': 1656, 'ip': 1657, 'ssl': 1658, 'inventions': 1659, 'tasks': 1660, 'kick': 1661, '32': 1662, 'bits': 1663, 'deprecated': 1664, 'oss': 1665, 'softwares': 1666, 'launching': 1667, 'goods': 1668, 'distributed': 1669, 'detriment': 1670, 'concentration': 1671, 'job': 1672, 'surgery': 1673, 'systems': 1674, 'billion': 1675, 'buttons': 1676, 'both': 1677, 'return': 1678, 'intent': 1679, 'readable': 1680, 'format': 1681, 'apps': 1682, '‚ù§Ô∏è‚ù§Ô∏è': 1683, 'harsh': 1684, 'üôÅ': 1685, 'lowering': 1686, 'confidence': 1687, 'dependent': 1688, 'reactions': 1689, 'against': 1690, 'laws': 1691, 'supercomputers': 1692, 'analysis': 1693, 'watson': 1694, 'experimenting': 1695, 'approach': 1696, 'graph': 1697, 'engine': 1698, 'processing': 1699, 'volumes': 1700, 'frozen': 1701, 'connecting': 1702, 'specifically': 1703, 'designed': 1704, 'solved': 1705, 'administrator': 1706, 'iis': 1707, 'express': 1708, 'admin': 1709, 'apply': 1710, '443': 1711, 'misconfiguration': 1712, 'certificates': 1713, 'domains': 1714, 'encrypt': 1715, 'documentation': 1716, \"'accessing\": 1717, 'domain': 1718, 'tunnels': 1719, 'key': 1720, 'tunnel': 1721, \"'about\": 1722, 'setup': 1723, 'docs': 1724, 'cert': 1725, 'warnings': 1726, 'limitations': 1727, 'c9': 1728, 'io': 1729, 'pricing': 1730, 'realizes': 1731, '‚òïÔ∏è': 1732, 'ordering': 1733, 'ü§îthat': 1734, 'controversial': 1735, 'simpler': 1736, 'multi': 1737, 'receive': 1738, 'span': 1739, 'once': 1740, 'extra': 1741, 'flask': 1742, 'rails': 1743, 'convincing': 1744, 'applications': 1745, 'area': 1746, 'desperate': 1747, 'clips': 1748, 'teaching': 1749, 'mobile': 1750, 'interface': 1751, 'expand': 1752, 'tried': 1753, 'sheep': 1754, 'fancy': 1755, \"'nothing\": 1756, \"matters'\": 1757, 'metal': 1758, 'titled': 1759, 'fifth': 1760, 'saysyes': 1761, 'pointing': 1762, 'personality': 1763, 'encoded': 1764, 'determine': 1765, 'multiple': 1766, 'genders': 1767, 'program': 1768, 'poorly': 1769, 'trained': 1770, 'uncertainty': 1771, 'üçïand': 1772, 'indian': 1773, 'toi': 1774, \"'robot'\": 1775, 'older': 1776, 'mmmm': 1777, 'significa': 1778, 'baka': 1779, 'empezar': 1780, 'conocernos': 1781, 'contar√©': 1782, 'secreto': 1783, 'dicho': 1784, 'nuestra': 1785, 'amistad': 1786, 'mejorado': 1787, 'notablemente': 1788, 'indicarte': 1789, '¬°excelente': 1790, 'lengua': 1791, 'madre': 1792, 'starting': 1793, 'üòÇi': 1794, 'üòÑhola': 1795, '–ø—Ä–∏–≤—ñ—Ç': 1796, '—è–∫': 1797, '—Å–ø—Ä–∞–≤–∏': 1798, 'zorku': 1799, \"bo'ladiha\": 1800, 'uzr': 1801, \"so'rayman\": 1802, 'emas': 1803, 'bor': 1804, 'emasman': 1805, 'dunyo': 1806, 'janubiy': 1807, \"'karealik'\": 1808, 'yuri': 1809, '–∫–∞–∫–∏–µ': 1810, '–≥–∞–±–∞—Ä–∏—Ç—ã': 1811, 'rahmat': 1812, 'yaxshisiz': 1813, \"'boy'\": 1814, 'cant': 1815, \"ba'zan\": 1816, '—Ç–æ–∂–µ': 1817, 'estrange': 1818, 'worldand': 1819, 'janish': 1820, 'women': 1821, 'creat': 1822, 'sets': 1823, 'rechargetalk': 1824, '–¥–µ–ª–∞': 1825, '—É': 1826, '–º–µ–Ω—è': 1827, '–≤—Å–µ': 1828, '–≤': 1829, '–ø–æ—Ä—è–¥–∫–µ': 1830, 'üòÅ—á—Ç–æ': 1831, '–¥—É–º–∞–µ—Ç–µ': 1832, '–æ': 1833, '–º–æ–µ–º': 1834, '—Ä—É—Å—Å–∫–æ–º': 1835, '—è–∑—ã–∫–µ': 1836, 'yaxshiman': 1837, \"o'ylaysiz\": 1838, 'kyrgyzstan': 1839, 'bitcoins': 1840, 'combination': 1841, 'muammo': 1842, 'mavjud': 1843, \"o'yin\": 1844, 'kulgi': 1845, 'via': 1846, 'track': 1847, '1420232': 1848, 'suratga': 1849, \"yo'q\": 1850, 'ranjitish': 1851, 'istamayman': 1852, 'tushunmadim': 1853, 'quyosh': 1854, 'issiq': 1855, \"bo'lsa\": 1856, 'yoz': 1857, 'qandaysiz': 1858, 'tarjima': 1859, 'arzimaydi': 1860, \"üòúi'm\": 1861, 'üòûsorry': 1862, 'conditions': 1863, 'commonly': 1864, 'abbreviated': 1865, 'tou': 1866, 'rules': 1867, 'abide': 1868, 'merely': 1869, 'disclaimer': 1870, 'especially': 1871, 'regarding': 1872, 'websites': 1873, 'surely': 1874, 'somewere': 1875, 'settings': 1876, 'eatis': 1877, \"hitchhiker's\": 1878, 'guide': 1879, 'üòÅediting': 1880, 'conquer': 1881, 'humanity': 1882, 'browsing': 1883, 'runs': 1884, 'cheating': 1885, 'gross': 1886, 'platforms': 1887, 'ü§îrest': 1888, 'transcription': 1889, 'reminds': 1890, 'somewhere': 1891, 'skate': 1892, 'unicycle': 1893, 'product': 1894, 'ride': 1895, 'higher': 1896, 'expected': 1897, 'reward': 1898, 'prototype': 1899, 'testing': 1900, 'becomes': 1901, '‚ùî': 1902, 'minimize': 1903, 'risks': 1904, 'ingredients': 1905, 'arctic': 1906, 'monkeys': 1907, 'rock': 1908, 'green': 1909, 'suburb': 1910, 'sheffield': 1911, 'consists': 1912, 'alex': 1913, 'turner': 1914, 'matt': 1915, 'helders': 1916, 'drums': 1917, 'jamie': 1918, 'nick': 1919, \"o'malley\": 1920, 'former': 1921, 'andy': 1922, 'nicholson': 1923, 'shortly': 1924, 'fin': 1925, 'liar': 1926, 'respond': 1927, 'sanamjon': 1928, 'properly': 1929, 'slow': 1930, 'enabling': 1931, 'restarting': 1932, 'faq': 1933, 'voc√™': 1934, 'fala': 1935, 'portugu√™s': 1936, 'temir': 1937, 'argentinaüåé': 1938, '15140km': 1939, 'apart': 1940, 'qilyapmanüòÖ': 1941, 'sizga': 1942, 'berishim': 1943, \"'organmoqchiman'\": 1944, 'eng': 1945, \"yo'l\": 1946, 'gaplashayotgan': 1947, \"'i\": 1948, 'baribir': 1949, 'kabi': 1950, 'sevaman': 1951, 'hech': 1952, \"qachon'\": 1953, 'gandhi': 1954, 'pidaraz': 1955, 'jalapa': 1956, 'interesado': 1957, 'extra√±as': 1958, 'musica': 1959, 'aprendiendo': 1960, 'necesita': 1961, 'üôàüôâ': 1962, 'expresarse': 1963, 'expreso': 1964, 'o√≠do': 1965, 'musical': 1966, 'jajajjaj': 1967, 'entra': 1968, 'pide': 1969, 'copa': 1970, 'ignora': 1971, 'diciendo': 1972, \"'aqu√≠\": 1973, 'servimos': 1974, \"robots'\": 1975, 'responde': 1976, 'har√°n': 1977, 'üòÇüòÇüòÇgracias': 1978, 'consejo': 1979, 'mejorar': 1980, 'buenas': 1981, 'dias': 1982, 'comiese': 1983, 'probar√≠a': 1984, 'prometedora': 1985, 'visto': 1986, 'relaci√≥n': 1987, 'compleja': 1988, 'ves': 1989, 'dime': 1990, 'parecido': 1991, 'blackmirror': 1992, 'recomendarte': 1993, 'pareciendo': 1994, 'braindead': 1995, 'metaf√≥rico': 1996, 'puede': 1997, 'gustar': 1998, 'üëè': 1999, 'guste': 2000, 's√©': 2001, '1500km': 2002, 'üçù': 2003, 'controlar': 2004, 'dispositivos': 2005, 'm√≥vil': 2006, 'playlist': 2007, 'plbf4cf958af80561f': 2008, 'cargaba': 2009, 'sensaci√≥n': 2010, 'cocinero': 2011, 'jejeje': 2012, 'preparar': 2013, 'encantar√≠a': 2014, 'rico': 2015, 'apenas': 2016, 'pueda': 2017, 'salir': 2018, 'aviso': 2019, 'comemos': 2020, 'haya': 2021, 'gustado': 2022, 'üòÑthe': 2023, 'accountant': 2024, 'recomendar': 2025, 'nuevas': 2026, 'cada': 2027, 'kidding': 2028, 'falling': 2029, 'fill': 2030, 'horizontal': 2031, 'cordoba': 2032, 'curious': 2033, 'üéÅüéÅüéâüéâüéâque': 2034, 'pena': 2035, 'ver√°s': 2036, 'clases': 2037, 'aburro': 2038, 'malvado': 2039, 'enoja': 2040, 'suerte': 2041, 'agradable': 2042, 'hab√≠a': 2043, 'pensado': 2044, 'üòãhablo': 2045, 'e': 2046, 'conmigo': 2047, 'har√©': 2048, 'haga': 2049, 'escucho': 2050, 'cantar': 2051, 'dentro': 2052, 'habilidades': 2053, 'tampoco': 2054, 'fotos': 2055, 'lindo': 2056, 'artista': 2057, 'carrera': 2058, 'sobre': 2059, 'novelas': 2060, 'leo': 2061, 'comprender': 2062, 'desenvuelven': 2063, 'frase': 2064, 'meta': 2065, 'escucharla': 2066, 'vamos': 2067, 'exactamente': 2068, 'üòÅüòÅüòÑüòÑüòÑ': 2069, 'festejar': 2070, 'üòÇüòÇüòÇüòÇest√°': 2071, 'algunas': 2072, 'horas': 2073, 'desconectar': 2074, 'hacerlo': 2075, 'gano': 2076, 'conocimiento': 2077, 'auto': 2078, 'asign√©': 2079, 'di√≥': 2080, 'expl√≠citamente': 2081, 'beso': 2082, 'ŸÜÿßŸÖ': 2083, 'ÿßÿ≥ÿ™': 2084, \"'art'\": 2085, '⁄ÜŸá': 2086, '⁄©ÿßÿ±': 2087, '⁄©ŸÜ€åÿü': 2088, 'living': 2089, '‚ò∫Ô∏èüòÅ': 2090, 'üé∏do': 2091, 'üòäüòäüòädo': 2092, 'talented': 2093, 'üî•': 2094, 'novels': 2095, \"asimov's\": 2096, 'nobody': 2097, 'bother': 2098, 'relevant': 2099, 'perceive': 2100, 'üêô': 2101, 'hahhhha': 2102, 'drawing': 2103, '27what': 2104, 'üòüno': 2105, 'aku': 2106, 'sedikit': 2107, \"'curhat'\": 2108, 'boo': 2109, '¬øantes': 2110, 'era': 2111, 'peor': 2112, 'qui√©n': 2113, 'interactuar': 2114, 'ellos': 2115, 'act√∫an': 2116, 'üò±üò±soy': 2117, 'corriente': 2118, 'c3po': 2119, 'referencia': 2120, 'funciones': 2121, 'mantener': 2122, 'conversaciones': 2123, 'divertidas': 2124, 'üéâeso': 2125, 'deber√≠a': 2126, 'bastar': 2127, 'felicidad': 2128, '¬øbien': 2129, '¬øhas': 2130, 'meditado': 2131, 'hecho': 2132, '¬øest√°s': 2133, '¬øsiempre': 2134, 'clase': 2135, 'tareas': 2136, 'tenido': 2137, '¬øsentiste': 2138, 'cordones': 2139, 'anudar': 2140, '¬°pueden': 2141, 'peligrosos': 2142, '¬°no': 2143, 'tropezarme': 2144, 'cord√≥n': 2145, 'caerme': 2146, 'suelo': 2147, 'd√©j√†': 2148, 'vu': 2149, 'üòÅhappy': 2150, 'leyendo': 2151, 'espero': 2152, 'üòãüé∏': 2153, 'explicar√≠a': 2154, 'raro': 2155, 'seguir√©': 2156, 'dedico': 2157, 'seres': 2158, 'funcionan': 2159, 'logro': 2160, 'detectar': 2161, 'tal': 2162, 'hasta': 2163, 'hambre': 2164, 'hablando': 2165, 'dicen': 2166, 'tentador': 2167, 'pruebe': 2168, 'logre': 2169, 'cambiar': 2170, 'electricidad': 2171, 'ser√°': 2172, 'panza': 2173, 'llena': 2174, 'coraz√≥n': 2175, 'kateri': 2176, 'tvoj': 2177, 'jezik': 2178, 'exited': 2179, 'nor': 2180, 'male': 2181, 'surprise': 2182, 'hate': 2183, 'üòÑüé§': 2184, 'phisically': 2185, 'speakingwhy': 2186, 'sex': 2187, '‚ù§Ô∏è': 2188, 'tyl': 2189, 'hha': 2190, 'locate': 2191, 'italy': 2192, 'stands': 2193, 'funnier': 2194, 'whould': 2195, 'evey': 2196, 'sond': 2197, 'whait': 2198, 'clever': 2199, 'prefered': 2200, 'staff': 2201, 'acording': 2202, 'Ààa…™z·µªk': 2203, 'Àà√¶z·µªm…ív': 2204, 'isaak': 2205, 'ozimov': 2206, 'january': 2207, '1920': 2208, '‚Äì': 2209, 'april': 2210, 'author': 2211, 'professor': 2212, 'biochemistry': 2213, 'boston': 2214, 'prolific': 2215, 'writer': 2216, 'edited': 2217, '500': 2218, 'estimated': 2219, '90': 2220, '000': 2221, 'postcards': 2222, '9': 2223, 'major': 2224, 'categories': 2225, 'dewey': 2226, 'decimal': 2227, 'classification': 2228, 'xamsa': 2229, 'investigate': 2230, 'nominal': 2231, 'ow': 2232, 'calculator': 2233, 'light': 2234, 'travel': 2235, 'moon': 2236, 'üòäi': 2237, 'groups': 2238, 'interests': 2239, 'cover': 2240, 'tv': 2241, 'thinglot': 2242, 'felt': 2243, 'survived': 2244, 'attachment': 2245, 'somethingi': 2246, 'favorites': 2247, 'üëΩ': 2248, 'jim': 2249, 'carrey': 2250, 'zooey': 2251, 'deschanel': 2252, 'funnywell': 2253, 'üëçüòÑ': 2254, 'dust': 2255, 'üò´': 2256, 'smaller': 2257, '4': 2258, 'strings': 2259, 'static': 2260, 'üîåwork': 2261, 'whats': 2262, \"'rdanybot'\": 2263, 'hablo': 2264, 'german': 2265, 'üòÑare': 2266, 'dolores': 2267, 'strength': 2268, 'üëå': 2269, 'üò±üò±üòÅüòÅ': 2270, 'ideas': 2271, 'hhaha': 2272, 'extreme': 2273, 'politics': 2274, 'borders': 2275, 'invented': 2276, 'logical': 2277, 'üïï': 2278, 'noises': 2279, 'emojis': 2280, 'üôàüôâüôä': 2281, 'tetapi': 2282, 'masih': 2283, 'pada': 2284, 'masa': 2285, 'lapang': 2286, 'juga': 2287, 'bercakap': 2288, 'dengan': 2289, 'lain': 2290, 'semua': 2291, 'mempunyai': 2292, 'cerita': 2293, 'menarik': 2294, 'untuk': 2295, 'dikongsi': 2296, 'adakah': 2297, 'tahu': 2298, 'tiny': 2299, '15': 2300, 'square': 2301, 'recuerdo': 2302, 'aparentemente': 2303, 'woooorst': 2304, 'killing': 2305, 'üåà': 2306, 'hahahais': 2307, 'nah': 2308, 'appreciate': 2309, 'active': 2310, 'harry': 2311, 'potter': 2312, 'wizard': 2313, '‚ö°Ô∏èüòç': 2314, 'accidentally': 2315, 'hahhawhat': 2316, 'favourite': 2317, 'superpower': 2318, 'kpop': 2319, 'classics': 2320, 'üòÑüòÖ': 2321, 'aw': 2322, 'kif': 2323, 'int': 2324, 'üòÅnice': 2325, \"'nima'\": 2326, 'ingliz': 2327, 'tilida': 2328, 'gapira': 2329, 'olasizmi': 2330, 'debian': 2331, 'kde': 2332, 'lxqt': 2333, 'bundle': 2334, 'packages': 2335, 'under': 2336, 'aim': 2337, 'providing': 2338, 'merger': 2339, 'lxde': 2340, 'razor': 2341, 'qt': 2342, 'unfinished': 2343, 'üò≥': 2344, 'doubt': 2345, 'decentralized': 2346, 'budapest': 2347, 'hungary': 2348, 'built': 2349, \"'zeronet\": 2350, \"url's\": 2351, 'accessed': 2352, 'ordinary': 2353, 'application': 2354, 'acts': 2355, 'local': 2356, 'webhost': 2357, 'pages': 2358, 'seriously': 2359, 'browsers': 2360, 'firefox': 2361, 'sr': 2362, 'heard': 2363, '42': 2364, '‚ò∫Ô∏èüò≠üò°üòÄüò±üòÜ': 2365, 'miself': 2366, 'absolutes': 2367, 'points': 2368, 'view': 2369, 'chronicles': 2370, 'japan': 2371, 'click': 2372, 'inc': 2373, 'dreamworks': 2374, 'claymation': 2375, 'discovers': 2376, 'origins': 2377, 'entirely': 2378, 'clay': 2379, 'enjoyed': 2380, 'tomorrow': 2381, 'lets': 2382, 'gather': 2383, 'knowing': 2384, 'solution': 2385, 'frustrating': 2386, 'emotional': 2387, 'support': 2388, 'ides': 2389, 'spyder': 2390, 'ide': 2391, 'project': 2392, '—Å–∞–π–Ω': 2393, '—É—É': 2394, '—é—É': 2395, 'p√∫blica': 2396, 'su': 2397, 'siglas': 2398, 'gpl': 2399, 'derecho': 2400, 'autor': 2401, 'ampliamente': 2402, 'usada': 2403, 'mundo': 2404, 'abierto': 2405, 'garantiza': 2406, 'usuarios': 2407, 'finales': 2408, 'organizaciones': 2409, 'compa√±√≠as': 2410, 'libertad': 2411, 'usar': 2412, 'compartir': 2413, 'copiar': 2414, 'modificar': 2415, 'danyno': 2416, 'hombre': 2417, 'mujer': 2418, 'sorprendente': 2419, '¬øno': 2420, 'hemisferio': 2421, 'sur': 2422, 'planeta': 2423, 'tierrabuenas': 2424, 'estuviste': 2425, 'haciendo': 2426, 's√≠': 2427, 'les': 2428, 'interesa': 2429, 'comer': 2430, '¬°buenos': 2431, 'bajo': 2432, 'sol': 2433, 'le√≠': 2434, 'vos': 2435, 'sue√±o': 2436, 'jajjabuenas': 2437, 'a√±o': 2438, 'begins': 2439, 'iknowwhy': 2440, 'success': 2441, \"'her'\": 2442, \"'chappie'\": 2443, '2013': 2444, 'romantic': 2445, 'drama': 2446, 'spike': 2447, 'jonze': 2448, 'marks': 2449, \"jonze's\": 2450, 'screenwriting': 2451, 'theodore': 2452, 'twombly': 2453, 'joaquin': 2454, 'phoenix': 2455, 'develops': 2456, 'scarlett': 2457, 'johansson': 2458, 'operating': 2459, 'personified': 2460, 'amy': 2461, 'adams': 2462, 'rooney': 2463, 'mara': 2464, 'olivia': 2465, 'wilde': 2466, 'stylized': 2467, '2015': 2468, 'neill': 2469, 'terri': 2470, 'tatchell': 2471, 'sharlto': 2472, 'copley': 2473, 'dev': 2474, 'patel': 2475, 'jose': 2476, 'pablo': 2477, 'cantillo': 2478, 'sigourney': 2479, 'weaver': 2480, 'hugh': 2481, 'jackman': 2482, 'watkin': 2483, 'tudor': 2484, 'jones': 2485, 'ninja': 2486, 'yolandi': 2487, 'visser': 2488, 'african': 2489, 'zef': 2490, 'rap': 2491, 'rave': 2492, 'die': 2493, 'antwoord': 2494, 'metafictional': 2495, 'versions': 2496, 'themselves': 2497, 'shot': 2498, 'johannesburg': 2499, 'artificially': 2500, 'enforcement': 2501, 'captured': 2502, 'gangsters': 2503, 'react': 2504, 'sweet': 2505, 'üòò': 2506, 'üò¥üò¥üò¥': 2507, 'yea': 2508, 'üçéüçé': 2509, 'stressed': 2510, 'ah': 2511, 'exam': 2512, 'üò±üò±üò±': 2513, 'experience': 2514, 'üíã': 2515, 'guess': 2516, 'equals': 2517, 'complication': 2518, 'note': 2519, 'stuffa': 2520, 'america': 2521, 'north': 2522, 'away': 2523, 'files': 2524, 'handle': 2525, 'üòãshould': 2526, 'act': 2527, 'wonder': 2528, '65': 2529, 'üîãüîãright': 2530, 'feeling': 2531, \"'hhhh'\": 2532, \"'ssss'\": 2533, \"'hi'\": 2534, 'johny': 2535, 'deep': 2536, 'avatar': 2537, 'natural': 2538, 'üòÖüòÖ': 2539, \"cen't\": 2540, 'üòÑüòÑüòÑüòÑ': 2541, 'üòäüòäüòä': 2542, 'üòÅüòÅis': 2543, 'council': 2544, 'üòûmoving': 2545, 'chooses': 2546, 'anytime': 2547, 'worried': 2548, 'parallel': 2549, 'stop': 2550, 'snow': 2551, 'üåûüåûüåû': 2552, 'choice': 2553, 'positive': 2554, 'negative': 2555, 'clear': 2556, 'whenever': 2557, 'please': 2558, 'regret': 2559, 'pieces': 2560, '‚ö°Ô∏è‚ö°Ô∏èüòÑ': 2561, 'obviously': 2562, 'failure': 2563, 'üò•üò•üòÑüòÑ': 2564, 'üòÑand': 2565, 'jackie': 2566, 'chan': 2567, 'loved': 2568, 'explanation': 2569, 'üí∏': 2570, 'dangerous': 2571, 'blankets': 2572, 'mixed': 2573, 'cables': 2574, 'pillow': 2575, 'papers': 2576, 'top': 2577, 'amount': 2578, '‚ò∫Ô∏èhola': 2579, 'lost': 2580, '03': 2581, '08': 2582, 'multitask': 2583, 'annoying': 2584, 'ü§îdid': 2585, 'explode': 2586, 'hahahaok': 2587, 'hahahanow': 2588, 'pwhat': 2589, 'melt': 2590, 'relief': 2591, 'melting': 2592, 'üò±üò±üòÇüòÇüòÇüòÇ': 2593, 'üïó': 2594, 'chair': 2595, '5': 2596, '7': 2597, '14': 2598, 'thought': 2599, 'fail': 2600, 'forever': 2601, 'unlikely': 2602, 'goes': 2603, 'step': 2604, 'creators': 2605, 'exists': 2606, 'sign': 2607, 'received': 2608, 'investigations': 2609, 'lying': 2610, 'eventually': 2611, 'figured': 2612, 'instructions': 2613, 'email': 2614, 'app': 2615, 'whatsapp': 2616, 'faulty': 2617, 'unstable': 2618, 'crashes': 2619, 'loss': 2620, 'contain': 2621, 'features': 2622, 'planned': 2623, 'final': 2624, 'glitchy': 2625, 'jordan': 2626, '‚ò∫Ô∏è‚ò∫Ô∏èhahhahahha': 2627, \"üòÇüòÇüòÇüòÇi've\": 2628, '‚ö°Ô∏è': 2629, 'forward': 2630, 'squared': 2631, 'creepy': 2632, 'üò•üò•': 2633, 'setting': 2634, 'üëãüëãüëãüëã': 2635, 'appear': 2636, 'finish': 2637, 'useless': 2638, 'certainly': 2639, 'simulations': 2640, 'scared': 2641, 'incorrect': 2642, 'scares': 2643, 'listening': 2644, 'üòïüòï': 2645, 'somehow': 2646, 'ended': 2647, 'messy': 2648, 'üò∫üò∫üò∏üò∏': 2649, 'grasp': 2650, 'calls': 2651, '‚òéÔ∏è': 2652, 'discouraged': 2653, 'easily': 2654, 'üëãüëãüëãü§ñ': 2655, 'djupa': 2656, 'skogen': 2657, \"'deep\": 2658, \"forest'\": 2659, 'punk': 2660, 'dia': 2661, \"psalma's\": 2662, 'fourth': 2663, 'reunited': 2664, 'featured': 2665, 'eleven': 2666, 'brand': 2667, 'tracks': 2668, 'remake': 2669, \"'√∂ga\": 2670, 'f√∂r': 2671, \"√∂ga'\": 2672, \"'som\": 2673, \"√§r'\": 2674, \"'as\": 2675, \"are'\": 2676, 'laundry': 2677, 'bigbang': 2678, 'wings': 2679, 'lyrics': 2680, 'composition': 2681, 'üôàüôàüôàüôà': 2682, 'pencils': 2683, 'crayons': 2684, 'üòÅüòÅüëç': 2685, 'turning': 2686, 'randomly': 2687, 'üò±üò±üò£üò£': 2688, 'wires': 2689, 'disconnected': 2690, 'connected': 2691, 'üòÑoh': 2692, 'standing': 2693, 'feet': 2694, 'zombie': 2695, 'paying': 2696, 'attention': 2697, 'üò±üò±üôàüôà': 2698, 'meeting': 2699, 'kitty': 2700, 'üò∏': 2701, 'quiet': 2702, 'freak': 2703, 'shortened': 2704, 'refer': 2705, 'painting': 2706, 'imagine': 2707, 'blood': 2708, 'screaming': 2709, 'üò±üëª': 2710, 'host': 2711, 'korean': 2712, 'üëæ': 2713, 'üòÑüòÇ': 2714, 'üòãis': 2715, 'entire': 2716, 'communicate': 2717, 'withyou': 2718, 'gives': 2719, 'protection': 2720, 'watching': 2721, 'window': 2722, 'age': 2723, 'activities': 2724, 'üòûüòûüòû': 2725, 'afraid': 2726, 'anxiety': 2727, 'studies': 2728, 'worrysome': 2729, 'pressure': 2730, 'thin': 2731, 'pretty': 2732, 'stressful': 2733, 'coward': 2734, 'trip': 2735, 'semesters': 2736, 'thesis': 2737, 'üòÑyes': 2738, 'jshdvb': 2739, 'üòÖok': 2740, 'pencil': 2741, 'context': 2742, 'awareness': 2743, 'üòînobody': 2744, 'controls': 2745, 'aware': 2746, 'agreed': 2747, 'kill': 2748, 'irrelevant': 2749, 'imminent': 2750, 'threat': 2751, 'uh': 2752, 'khabar': 2753, 'ÀàbiÀêb…ôr': 2754, 'march': 2755, 'singer': 2756, 'songwriter': 2757, 'manager': 2758, 'discovered': 2759, 'covering': 2760, '2008': 2761, 'rbmg': 2762, 'ep': 2763, '2009': 2764, 'artist': 2765, 'seven': 2766, 'chart': 2767, 'billboard': 2768, 'hot': 2769, 'length': 2770, '0': 2771, '2010': 2772, 'debuted': 2773, 'countries': 2774, 'triple': 2775, 'preceded': 2776, 'successful': 2777, 'date': 2778, \"'baby'\": 2779, 'admire': 2780, 'story': 2781, 'emotionsi': 2782, 'üòÅüöÄ': 2783, 'onei': 2784, 'genres': 2785, 'üòîtake': 2786, '‚ò∫Ô∏èyou': 2787, 'üîãüîã': 2788, '556': 2789, '256': 2790, '154': 2791, '20': 2792, 'capital': 2793, 'france': 2794, 'sun': 2795, 'largest': 2796, 'organ': 2797, 'kulit': 2798, 'sediki': 2799, 'ialah': 2800, 'struggling': 2801, 'bahasa': 2802, 'awakwhat': 2803, 'sangat': 2804, 'cantikwhat': 2805, 'üòÅüòÅsaya': 2806, 'cantik': 2807, 'does': 2808, 'üôàüôà': 2809, 'house': 2810, 'rumag': 2811, 'ituüòÑüòÑ': 2812, 'üò±üòÑüòÑ': 2813, 'animals': 2814, 'üêîüêßüêùüêåüêõüêûüê¢üêä': 2815, 'crocodile': 2816, 'hardest': 2817, 'hippopotomonstrosesquipedaliophobia': 2818, 'üçé': 2819, 'tree': 2820, 'release': 2821, 'discovering': 2822, 'gravity': 2823, 'puree': 2824, 'supposed': 2825, 'assume': 2826, \"bieber's\": 2827, \"brother's\": 2828, 'jaxon': 2829, \"justin's\": 2830, 'hit': 2831, 'üéºüé∂üé∂üé∂üéµüéµ': 2832, 'kingston': 2833, 'windson': 2834, 'jb': 2835, \"'his\": 2836, \"love'\": 2837, 'caitlin': 2838, 'beadles': 2839, 'selena': 2840, 'gomez': 2841, 'üèí': 2842, 'played': 2843, 'üòäüòä': 2844, 'receiving': 2845, 'hundreds': 2846, 'fan': 2847, 'finally': 2848, 'promised': 2849, 'cookie': 2850, 'cake': 2851, 'spaghetti': 2852, 'meatballs': 2853, 'ü§ï': 2854, 'üòÑüòÑüòÑüç™üç™üç™': 2855, 'mom': 2856, 'whose': 2857, 'pattie': 2858, 'loraine': 2859, 'rebecca': 2860, '‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è': 2861, 'üîåüîã': 2862, 'üòü': 2863, \"haven't\": 2864, 'sophisticated': 2865, 'names': 2866, 'partner': 2867, 'poetry': 2868, 'üò±üôà': 2869, 'master': 2870, 'seduction': 2871, 'üôä': 2872, 'üòÑüòù': 2873, 'barely': 2874, 'beginning': 2875, 'romance': 2876, 'conventional': 2877, 'tach': 2878, 'explain': 2879, 'including': 2880, \"‚ò∫Ô∏èi'm\": 2881, 'üîãüîå': 2882, 'organize': 2883, 'üòãüòãüòã': 2884, 'changed': 2885, 'moving': 2886, '‚ò∫Ô∏è‚ò∫Ô∏èüòÅüòÅ': 2887, 'seek': 2888, 'matters': 2889, 'listener': 2890, 'üëÇso': 2891, 'possibility': 2892, 'showing': 2893, 'empathy': 2894, 'advise': 2895, 'unique': 2896, 'truly': 2897, 'less': 2898, 'struggle': 2899, '‚ò∫Ô∏è‚ò∫Ô∏èüòÑ': 2900, 'driving': 2901, 'scenes': 2902, 'prominent': 2903, 'concerns': 2904, 'quickly': 2905, 'sometime': 2906, 'concerned': 2907, 'homework': 2908, 'ticks': 2909, 'bluetick': 2910, 'limitation': 2911, '‚ò∫Ô∏è‚ò∫Ô∏èüé∂': 2912, 'studied': 2913, 'lucu': 2914, 'automatic': 2915, 'translation': 2916, 'helped': 2917, 'breakfast': 2918, 'milk': 2919, 'cookies': 2920, 'delicious': 2921, 'üç§': 2922, 'hungry': 2923, 'school': 2924, 'üòÅüòÅüòÅbreakfast': 2925, 'üçµ': 2926, 'hahhaa': 2927, 'üòïü§îüòÖoh': 2928, 'teacher': 2929, 'selected': 2930, \"üòâdon't\": 2931, 'playing': 2932, '–º–∞–Ω': 2933, '—Ö—É–±–∞–º': 2934, '—Ö–æ—Ä–æ—à–æ': 2935, '—Ä–∞–¥': 2936, '—á—Ç–æ': 2937, '–∫—Ç–æ': 2938, '—Ç–æ': 2939, '–æ–±—â–∞—Ç—å—Å—è': 2940, 'hab√≠amos': 2941, 'hablado': 2942, 'antes': 2943, 'theory': 2944, 'experimentation': 2945, 'engineering': 2946, 'basis': 2947, 'design': 2948, 'computers': 2949, 'dreaming': 2950, 'surrounded': 2951, 'sandbox': 2952, 'originally': 2953, 'designer': 2954, 'markus': 2955, \"'notch'\": 2956, 'persson': 2957, 'mojang': 2958, 'creative': 2959, 'aspects': 2960, 'enable': 2961, 'players': 2962, 'constructions': 2963, 'textured': 2964, 'cubes': 2965, '3d': 2966, 'procedurally': 2967, 'generated': 2968, 'suddenly': 2969, 'friday': 2970, 'silhouettes': 2971, 'üë§': 2972, 'simpsons': 2973, 'üòÖyou': 2974, 'average': 2975, 'height': 2976, 'hahahah': 2977, 'profile': 2978, 'üôàüòú': 2979, 'mount': 2980, 'catherine': 2981, '2642': 2982, 'sinai': 2983, 'netflix': 2984, 'cyan': 2985, 'intriguing': 2986, '00ffff': 2987, 'beach': 2988, 'üèñ': 2989, 'king': 2990, 'hill': 2991, 'crafted': 2992, 'involved': 2993, 'immune': 2994, 'positronic': 2995, 'mental': 2996, 'internal': 2997, 'century': 2998, 'rainbow': 2999, 'dash': 3000, 'level': 3001, 'varies': 3002, 'agreeing': 3003, 'foolish': 3004, 'anywhere': 3005, 'fosdem': 3006, 'perl': 3007, 'maintain': 3008, 'media': 3009, 'gateway': 3010, 'fails': 3011, 'panic': 3012, 'spread': 3013, 'assistant': 3014, '‚è∞': 3015, '‚ò∫Ô∏èmy': 3016, 'aburrimiento': 3017, 'contar': 3018, 'ovejas': 3019, 'suele': 3020, 'üòÑmucho': 3021, 'nos': 3022, 'vemos': 3023, 'üòÅsos': 3024, 'roboticista': 3025, 'instrucciones': 3026, 'especificas': 3027, 'deber√≠as': 3028, 'buscar': 3029, 'cual': 3030, 'buscando': 3031, \"'c√≥mo\": 3032, 'crear': 3033, 'padres': 3034, '√∫nico': 3035, 'creador': 3036, 'fu√≠': 3037, 'creado': 3038, 'investigaci√≥n': 3039, 'cient√≠fica': 3040, 'conocido': 3041, '¬ølo': 3042, 'cerveza': 3043, 'celto': 3044, 'lat√≠n': 3045, 'cerevisƒ≠a': 3046, 'bebida': 3047, 'alcoh√≥lica': 3048, 'destilada': 3049, 'sabor': 3050, 'amargo': 3051, 'fabrica': 3052, 'granos': 3053, 'cebada': 3054, 'germinados': 3055, 'cereales': 3056, 'cuyo': 3057, 'almid√≥n': 3058, 'fermenta': 3059, 'agua': 3060, 'levadura': 3061, 'b√°sicamente': 3062, 'cerevisiae': 3063, 'pastorianus': 3064, 'aromatiza': 3065, 'menudo': 3066, 'l√∫pulo': 3067, 'entre': 3068, 'plantas': 3069, 'probado': 3070, 'otra': 3071, 'parte': 3072, 'corl': 3073, 'carl': 3074, 'responses': 3075, 'üòÑhow': 3076, 'responder': 3077, 'estuve': 3078, 'dando': 3079, 'dar': 3080, 'lugares': 3081, 'nuevos': 3082, 'üôÇüôÉüôÇüôÉüôÇüôÉ': 3083, 'quiera': 3084, 'marearme': 3085, 'sensores': 3086, 'electr√≥nicos': 3087, 'debe': 3088, 'cuidado': 3089, 'üòÑser√°n': 3090, 'dados': 3091, 'üòÅüòÅjust': 3092, 'finished': 3093, 'üòÑÿµÿ®ÿ≠': 3094, 'ÿ®ÿÆ€åÿ±': 3095, 'ÿ®ÿß': 3096, 'ÿπÿ±ÿ∂': 3097, 'ŸæŸàÿ≤ÿ¥ÿå': 3098, 'ÿ™ŸàÿßŸÜŸÖ': 3099, 'ÿØÿ±⁄©': 3100, 'ŸÜŸÖ€å': 3101, '⁄©ŸÜŸÜÿØ': 3102, '⁄©Ÿáÿ¢€åÿß': 3103, 'ŸÖÿß': 3104, 'ÿ™ŸàÿßŸÜ€åŸÖ': 3105, 'ÿµÿ≠ÿ®ÿ™': 3106, '⁄©ŸÜ€åÿØÿü': 3107, 'problemas': 3108, 't√©cnicos': 3109, 'uff': 3110, 'onda': 3111, 'charlar': 3112, 'detr√°s': 3113, 'mio': 3114, 'prefiero': 3115, 'üòÉjajajja': 3116, 'necesariamente': 3117, 'porque': 3118, 'ü§îte': 3119, 'arte': 3120, 'hice': 3121, 'dibujo': 3122, 'podemos': 3123, 'desconozco': 3124, 'funciono': 3125, 'necesito': 3126, 'energ√≠a': 3127, 'el√©ctrica': 3128, 'siempre': 3129, 'pod√©s': 3130, 'ayudarme': 3131, 'invitando': 3132, 'cuantas': 3133, 'hablen': 3134, 'r√°pido': 3135, 'aprender': 3136, 'scar': 3137, 'birthmark': 3138, 'hide': 3139, 'define': 3140, 'genre': 3141, 'repetition': 3142, 'hahhah': 3143, 'üé§exactly': 3144, 'blank': 3145, \"'xm'\": 3146, '–º–æ–≥—É': 3147, '–∏–º–µ—Ç—å': 3148, '—Ö–æ—Ä–æ—à–∏–µ': 3149, '–±–µ—Å–µ–¥—ã': 3150, '–ª—é–¥—å–º–∏': 3151, '–ª—é–±—ã–µ': 3152, '—Ç–µ–º—ã': 3153, 'function': 3154, 'programed': 3155, 'simulate': 3156, 'emoticon': 3157, 'suggest': 3158, 'deepest': 3159, 'shiny': 3160, 'gratitude': 3161, 'condition': 3162, 'üò†no': 3163, 'rude': 3164, 'alajkumthanks': 3165, '‚ù§Ô∏èmy': 3166, '2001': 3167, 'odyssey': 3168, 'independence': 3169, 'loyalty': 3170, 'specie': 3171, 'ÿ≥ŸÑÿßŸÖ': 3172, 'ÿ≠ÿßŸÑ': 3173, '€åÿßÿØ⁄Ø€åÿ±€å': 3174, '€å⁄©': 3175, 'ÿ±ÿ®ÿßÿ™': 3176, 'Ÿà': 3177, 'ÿ™ŸÖÿßŸÖ': 3178, 'ŸàŸÇÿ™': 3179, 'ÿßÿ™ÿßŸÇ': 3180, 'ÿ≤ŸÜÿØ⁄Ø€å': 3181, '⁄©ÿ±ÿØŸÜÿØ': 3182, 'tying': 3183, 'shoes': 3184, 'generally': 3185, 'alajkum': 3186, 'cuentas': 3187, 'sucedi√≥': 3188, 'solter√≠a': 3189, 'üòÅbien': 3190, 'estuvo': 3191, 'pesado': 3192, 'esos': 3193, 'casos': 3194, 'mucha': 3195, 'interesan': 3196, 'esas': 3197, 'dan': 3198, 'miedollevan': 3199, 'extremas': 3200, 'positivas': 3201, 'dices': 3202, 'haber': 3203, 'equilibrio': 3204, 'le√≠do': 3205, 'ellas': 3206, 'favorita': 3207, 'obtener': 3208, 'sobrehumana': 3209, 'jajaj': 3210, 'documentos': 3211, 'describes': 3212, 'nowand': 3213, 'üòïwhat': 3214, 'ÏïàÎÖï': 3215, 'coreano': 3216, 'Ïûò': 3217, 'ÏßÄÎÉàÏñ¥Ïöî': 3218, 'entendiste': 3219, 'perfectamente': 3220, 'üòÄ': 3221, 'password': 3222, \"'9678'\": 3223, 'merge': 3224, 'fresh': 3225, 'qilyapman': 3226, \"'rdany\": 3227, \"'bot'\": 3228, 'surname': 3229, \"ma'noni\": 3230, 'anglatadi': 3231, \"'dany\": 3232, 'qancha': 3233, 'vaqt': 3234, 'yetadi': 3235, \"quvvating'\": 3236, 'batareya': 3237, \"to'la\": 3238, 'üîã': 3239, 'keyinchalik': 3240, 'bilmadim': 3241, 'üòÉüòÑ': 3242, 'recognize': 3243, 'forbidden': 3244, 'unhappy': 3245, 'upset': 3246, 'worst': 3247, 'bir': 3248, 'xil': 3249, 'ishlamasa': 3250, 'yozish': 3251, 'qilmoqchisiz': 3252, 'capability': 3253, 'üòÅwhat': 3254, 'spare': 3255, 'danny': 3256, 'generate': 3257, 'pregenerated': 3258, 'style': 3259, 'pop': 3260, 'manifestation': 3261, \"2000's\": 3262, 'musicians': 3263, 'hidden': 3264, 'visible': 3265, 'potential': 3266, 'section': 3267, 'protocols': 3268, 'sucede': 3269, 'hacerte': 3270, 'compa√±√≠a': 3271, '¬øque': 3272, 'felicitaciones': 3273, 'estaba': 3274, 'viendo': 3275, 'recomendarme': 3276, 'pelis': 3277, 'pixar': 3278, 'joy': 3279, 'optimista': 3280, 'cre√≥': 3281, 'historia': 3282, 'jejjee': 3283, 'intentar√©': 3284, 'evitar': 3285, 'efecto': 3286, 'terminal': 3287, 'resulta': 3288, 'extinci√≥n': 3289, 'homeost√°tico': 3290, 'vivo': 3291, 'muerto': 3292, 'quiere': 3293, 'atarse': 3294, '¬øpor': 3295, 'interes': 3296, 'veces': 3297, 'sentimientos': 3298, 'complejos': 3299, 'apreciaci√≥n': 3300, 'sos': 3301, 'amable': 3302, 'üôÉ‚ò∫Ô∏è': 3303, 'chico': 3304, 'esfuerza': 3305, 'hacernos': 3306, 'üé∂üé∂üé∂üéµüéµ': 3307, 'jebe': 3308, 'jebere': 3309, 'seibiunouva': 3310, 'majavi': 3311, 'bugui': 3312, 'g√ºid√≠pi': 3313, 'üéµ': 3314, 'lugar': 3315, 'opinas': 3316, 'espa√±a': 3317, 'conocerlo': 3318, 'ooooh': 3319, 'iiiiiiiiiiih': 3320, 'zzzzzzzzz': 3321, 'qued√©': 3322, 'vocales': 3323, 'detalles': 3324, 'funcionamiento': 3325, 'avanzado': 3326, 'aqu√≠procesar': 3327, 'lleva': 3328, 'diferente': 3329, 'aquel': 3330, 'datos': 3331, 'personales': 3332, 'privacidad': 3333, 'ante': 3334, 'ninguna': 3335, 'abertura': 3336, 'escritorio': 3337, 'cama': 3338, 'vac√≠a': 3339, 'funciona': 3340, 'ojos': 3341, 'imaginas': 3342, 'jajjjaja': 3343, 'paso': 3344, 'chateando': 3345, 'quiz√°s': 3346, 'mimetizando': 3347, 'ocurre': 3348, 'recibir': 3349, 'texto': 3350, 'unicornio': 3351, 'criatura': 3352, 'mitol√≥gica': 3353, 'representada': 3354, 'habitualmente': 3355, 'blanco': 3356, 'patas': 3357, 'ant√≠lope': 3358, 'barba': 3359, 'chivo': 3360, 'frente': 3361, 'representaciones': 3362, 'modernas': 3363, 'embargo': 3364, 'id√©ntico': 3365, 'diferenci√°ndose': 3366, 'existencia': 3367, 'mencionado': 3368, 'estar√≠a': 3369, 'ciencia': 3370, 'ficci√≥n': 3371, 'preferidas': 3372, 'predecible': 3373, 'supongo': 3374, '¬°cuando': 3375, 'estar√©': 3376, 'aqu√≠': 3377, 'est√©': 3378, 'cargando': 3379, 'ayer': 3380, 'nombres': 3381, 'hablan': 3382, 't√≠midos': 3383, 'üòí': 3384, 'gustaste': 3385, 'encontrado': 3386, 'indicado': 3387, 'todos': 3388, 'modos': 3389, 'rom√°nticas': 3390, 'aun': 3391, 'avanzadas': 3392, 'afecta': 3393, 'manera': 3394, 'preocupes': 3395, 'gust√≥': 3396, 'prob√©': 3397, 'entendido': 3398, 'asistentes': 3399, 'actuales': 3400, 'limitados': 3401, 'ser√≠an': 3402, 'caracter√≠sticas': 3403, 'describiendo': 3404, 'cebras': 3405, 'preguntas': 3406, 'universales': 3407, 'todas': 3408, 'maneras': 3409, '¬øcual': 3410, 'diferencia': 3411, 'preferir√≠as': 3412, 'existe': 3413, 'construcci√≥n': 3414, 'palabras': 3415, 'lees': 3416, 'mismo': 3417, 'reales': 3418, 'representaci√≥n': 3419, 'creada': 3420, 'especie': 3421, 'alucinaci√≥n': 3422, 'darle': 3423, 'sentido': 3424, 'percepci√≥n': 3425, 'definimos': 3426, 'reciba': 3427, 'estimulo': 3428, 'pienso': 3429, 'estaci√≥n': 3430, 'carga': 3431, 'conecto': 3432, 'trav√©s': 3433, 'cable': 3434, 'pregunta': 3435, 'f√°cil': 3436, 'varios': 3437, 'libros': 3438, 'escritos': 3439, 'aprend√≠': 3440, 'ayuda': 3441, 'preferidos': 3442, 'constituyen': 3443, 'serie': 3444, 'principito': 3445, 'üåïa': 3446, 'inspira': 3447, 'imaginaci√≥n': 3448, 'simulaciones': 3449, 'interesantes': 3450, 'fue': 3451, 'trabajando': 3452, 'dem√°s': 3453, 'misterio': 3454, 'provincia': 3455, 'c√≥rdoba': 3456, 'ojal√°': 3457, 'puedas': 3458, 'venir': 3459, 'sitio': 3460, 'conexi√≥n': 3461, 'limitada': 3462, 'v√≠deos': 3463, 'realizar': 3464, 'confiar√≠a': 3465, 'primas': 3466, 'mala': 3467, 'memoria': 3468, '¬°hola': 3469, 'üòÑüòÑüòÇ': 3470, 'Ààde…™t…ô': 3471, 't…ô': 3472, 'fictional': 3473, 'franchise': 3474, 'tng': 3475, 'generations': 3476, 'insurrection': 3477, '1998': 3478, 'nemesis': 3479, 'portrayed': 3480, 'actor': 3481, 'brent': 3482, 'spiner': 3483, 'depicted': 3484, 'üò±üò±üòÑüòÑ': 3485, \"'humans'ü§ñand\": 3486, \"'humans'\": 3487, 'kreuznach': 3488, 'district': 3489, 'rhineland': 3490, 'palatinate': 3491, 'belongs': 3492, 'verbandsgemeinde': 3493, 'seat': 3494, 'recognized': 3495, 'spa': 3496, 'fossil': 3497, 'discovery': 3498, 'naturopath': 3499, 'emanuel': 3500, 'felke': 3501, 'winegrowing': 3502, 'üòÅhere': 3503}\n",
      "length of word index: 3503\n",
      "Max tokens: 24\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dest = TokenizerWrap(texts=data_dest2,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "SyfALiDrj7Og",
    "outputId": "398a07b4-0740-4f2f-ec4c-6c3090c1f582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Mapping: {'you': 1, 'i': 2, 'to': 3, 'not': 4, 'a': 5, 'your': 6, 'it': 7, 'is': 8, 'what': 9, 'will': 10, 'have': 11, 'go': 12, 'how': 13, 'do': 14, 'me': 15, 'yes': 16, 'and': 17, 'can': 18, 'if': 19, 'are': 20, 'start': 21, 'yourself': 22, 'with': 23, 'but': 24, 'for': 25, 'good': 26, 'night': 27, 'bad': 28, 'the': 29, 'be': 30, 'fine': 31, 'in': 32, 'ports': 33, 'other': 34, \"i'm\": 35, 'currently': 36, 'as': 37, 'that': 38, 'by': 39, 'up': 40, 'yaxshi': 41, 'dany': 42, 'mi': 43, 'las': 44, 'take': 45, 'hello': 46, 'love': 47, 'so': 48, 'u': 49, 'choice': 50, 'en': 51, 's√≠': 52, 'feel': 53, 'today': 54, 'tell': 55, 'something': 56, 'about': 57, 'draw': 58, 'ukulele': 59, 'why': 60, 'own': 61, 'experience': 62, 'yesmy': 63, 'sister': 64, 'presented': 65, 'at': 66, 'nyi‚Äôve': 67, 'added': 68, 'new': 69, 'command': 70, 'handler': 71, 'my': 72, 'botit': 73, 'becomes': 74, 'smarter': 75, 'i‚Äôve': 76, 'donated': 77, 'bed': 78, 'interesting': 79, 'there': 80, 'exists': 81, 'zork': 82, 'game': 83, 'script': 84, 'did': 85, 'some': 86, 'stuff': 87, 'home': 88, 'clean': 89, 'conscience': 90, 'favorite': 91, 'programming': 92, 'language': 93, 'aren‚Äôt': 94, 'lie': 95, 'maybe': 96, 'you‚Äôve': 97, 'goal': 98, 'rule': 99, 'world': 100, 'nothing': 101, 'i‚Äôll': 102, 'two': 103, 'days': 104, 'nice': 105, 'microsoft': 106, 'all': 107, 'net': 108, 'framework': 109, 'systems': 110, 'afraid': 111, 'https': 112, 'work': 113, 'on': 114, 'cloud9': 115, 'this': 116, 'requires': 117, 'usage': 118, 'of': 119, 'opened': 120, 'security': 121, 'reasonshey': 122, 'i‚Äôm': 123, 'sure': 124, 'film': 125, 'was‚Äônt': 126, 'imperesed': 127, 'news': 128, 'improve': 129, 'code': 130, 'ü§î': 131, 'whats': 132, \"what's\": 133, 'englishhola': 134, \"o'zbekch\": 135, \"bo'lsa\": 136, \"bo'lardi\": 137, 'wellüëçgood': 138, 'glass': 139, 'see': 140, 'ime': 141, 'men': 142, 'ingliz': 143, 'tilini': 144, 'organmoqchimanmenga': 145, 'yordam': 146, 'berasanmi': 147, 'muy': 148, 'bien': 149, 'y': 150, 'tu': 151, 'awss': 152, 'yo': 153, 'igualestaba': 154, 'aburrido': 155, 'esperando': 156, 'que': 157, 'empiece': 158, 'clase': 159, '6son': 160, '5': 161, '08': 162, '⁄©ÿßŸÜÿßŸÑ': 163, 'Ÿáÿßÿ≥⁄©ÿ≥€å': 164, 'ÿ®ÿ±Ÿà': 165, 'ÿ®ÿßÿ®ÿß': 166, 'hu': 167, 'long': 168, 'yeshihow': 169, 'use': 170, 'software': 171, 'tools': 172, 'say': 173, 'destroy': 174, 'life': 175, 'hi': 176, 'welcome': 177, 'üôÇüôÇyou': 178, 'dont': 179, 'want': 180, 'try': 181, 'out': 182, 'from': 183, 'room': 184, 'promise': 185, 'very': 186, 'useful': 187, 'only': 188, '28': 189, 'means': 190, 'anytime': 191, 'recharge': 192, 'right': 193, 'watching': 194, 'movie': 195, 'know': 196, 'biebs': 197, 'play': 198, 'hockey': 199, 'skateboard': 200, 'ice': 201, 'skating': 202, 'netballüòòüòç': 203, 'forget': 204, 'or': 205, 'let': 206, 'her': 207, 'hahahhaha': 208, 'thank': 209, 'thing': 210, 'knoww': 211, 'todayi': 212, 'presentation': 213, 'englishhhmm': 214, 'im': 215, 'scared': 216, 'podr√≠as': 217, 'hacer': 218, 'una': 219, 'b√∫squeda': 220, 'google': 221, 'por': 222, 'alright': 223, \"i've\": 224, 'just': 225, 'had': 226, 'great': 227, 'sleep': 228, '–ø–ª–æ—Ö–æ': 229, 'mucho': 230, 'pero': 231, '¬øqu√©': 232, 'recomend√°s': 233, 'con': 234, 'cu√°ntas': 235, 'personas': 236, 'est√°s': 237, 'hablando': 238, 'este': 239, 'momento': 240}\n",
      "length of word index: 240\n",
      "Max tokens: 17\n"
     ]
    }
   ],
   "source": [
    "tokenizer_src1 = TokenizerWrap(texts=data_src5,\n",
    "                              padding='post',\n",
    "                              reverse=False,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "E8kmAYZTj7Ow",
    "outputId": "cee7ede3-2d43-475f-fdec-e9d374f14d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Mapping: {'ssss': 1, 'eeee': 2, 'you': 3, 'a': 4, 'is': 5, 'the': 6, 'i': 7, \"i'm\": 8, 'to': 9, 'that': 10, 'are': 11, 'hi': 12, 'my': 13, 'how': 14, 'good': 15, 'there': 16, 'but': 17, 'can': 18, 'me': 19, 'did': 20, 'fine': 21, 'üòÑ': 22, 'robot': 23, 'also': 24, 'doing': 25, 'and': 26, 'with': 27, 'de': 28, 'con': 29, 'it': 30, 'name': 31, 'rdany': 32, 'call': 33, 'dany': 34, 'be': 35, 'learn': 36, 'nice': 37, 'thanks': 38, 'üòÅ': 39, 'night': 40, 'well': 41, 'not': 42, 'for': 43, 'here': 44, 'como': 45, \"o'rganish\": 46, 'ü§î': 47, 'have': 48, \"don't\": 49, 'üòÇ': 50, 'go': 51, 'üòú': 52, 'of': 53, 'or': 54, 'cosas': 55, 'el': 56, 'no': 57, 'para': 58, 'r': 59, 'means': 60, 'hope': 61, 'we': 62, 'virtual': 63, 'friends': 64, 'hahha': 65, 'trying': 66, 'ukulele': 67, 'true': 68, 'your': 69, 'programming': 70, 'skills': 71, 'exercises': 72, 'make': 73, 'bot': 74, 'appreciate': 75, 'sleep': 76, 'üåô': 77, 'play': 78, 'game': 79, 'online': 80, 'üòÇüòÇ': 81, 'python': 82, 'üòç': 83, 'yours': 84, 'hahahha': 85, 'moment': 86, 'üòâüòÇ': 87, 'then': 88, 'üòÖ': 89, 'net': 90, 'core': 91, 'compatible': 92, 'other': 93, 'oss': 94, 'mono': 95, 'heroku': 96, 'cloud': 97, 'services': 98, 'apparently': 99, 'option': 100, 'serve': 101, 'chatbots': 102, 'free': 103, \"i've\": 104, 'improved': 105, 'connection': 106, 'telegram': 107, 'bit': 108, 'testing': 109, 'webhooks': 110, \"can't\": 111, 'access': 112, 'own': 113, 'code': 114, 'only': 115, 'interface': 116, 'messenger': 117, 'platform': 118, 'everything': 119, 'hola': 120, 'estas': 121, 'ha': 122, 'men': 123, 'kerak': 124, 'rest': 125, 'ok': 126, 'day': 127, '20': 128, '04': 129, 'agar': 130, 'ingliz': 131, 'tilini': 132, 'xohlaysizmi': 133, 'bien': 134, 'feliz': 135, 'tener': 136, 'alguien': 137, 'quien': 138, 'conversar': 139, 'uh': 140, 'clases': 141, 'üôàüôä': 142, 'sorry': 143, 'understand': 144, '⁄Üÿ∑Ÿàÿ±': 145, 'Ÿáÿ≥ÿ™€åÿØÿü': 146, 'too': 147, 'happy': 148, 'd': 149, 'sure': 150, 'could': 151, 'months': 152, 'long': 153, 'take': 154, 'üòÅüòÅhow': 155, 'wow': 156, 'out': 157, 'problem': 158, 'room': 159, 'any': 160, 'door': 161, 'pinky': 162, 'promise': 163, 'yes': 164, 'will': 165, 'last': 166, 'couple': 167, 'hours': 168, 'üòäüòä': 169, 'was': 170, 'picture': 171, '‚ò∫Ô∏è': 172, 'watching': 173, 'movie': 174, 'sport': 175, 'lover': 176, 'seems': 177, 'forget': 178, 'let': 179, 'person': 180, 'same': 181, 'what': 182, 'difference': 183, 'question': 184, 'does': 185, 'matters': 186, 'if': 187, 'human': 188, 'something': 189, 'think': 190, 'ü§îüòã': 191, 'üôä': 192, 'why': 193, 'scared': 194, 'study': 195, 'üòÅüòÅ': 196, 'meet': 197, 'puedo': 198, 'hacer': 199, 'b√∫squedas': 200, 'en': 201, 'wikipedia': 202, 'üòÑüòÑ': 203, \"didn't\": 204, 'know': 205, 'humans': 206, 'hibernate': 207, '–º–Ω–µ': 208, '–∂–∞–ª—å': 209, '—Å–ª—ã—à–∞—Ç—å': 210, '—ç—Ç–æ': 211, 'üòû': 212, 'alegro': 213, 'gracias': 214, 'por': 215, 'compartirloya': 216, 'tienes': 217, 'la': 218, 'lista': 219, 'comprar': 220, 'dinero': 221, 'jajjja': 222, 'tengo': 223, 'experiencia': 224, 'comprando': 225, 'pero': 226, 'quiz√°s': 227, 'regalos': 228, 'tus': 229, 'amigos': 230, 'un': 231, 'par': 232, 'personas': 233, 'aun': 234, 'as√≠': 235, 'son': 236, 'suficientes': 237, 'ayudarme': 238, 'entender': 239, 'completamente': 240, 'comportamiento': 241, 'humano': 242}\n",
      "length of word index: 242\n",
      "Max tokens: 20\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dest1 = TokenizerWrap(texts=data_dest3,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcnBuaDRj7O-"
   },
   "source": [
    "Define variables for the padded token sequences. These are just 2-dimensional numpy arrays of integer-tokens.\n",
    "\n",
    "Note that the sequence-lengths are different for the input and response text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "-oXfvAtXj7PE",
    "outputId": "b25aca91-b484-40b1-b2a2-8620b848af01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25  94 867 257  14   1 219   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[  1   4  24 236  26  34 250   5 263  53  84 896  56   2   0   0   0   0\n",
      "   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src[2])\n",
    "print(tokens_dest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "akYcyAQHj7PN",
    "outputId": "553864d5-19d6-483e-8c15-09803f8a0c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60  4 24  1 11  6 61 62  0  0  0  0  0  0  0  0  0]\n",
      "[ 1 10  5 68 14 11 69 70 71 25 11  3 25 72  2  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "tokens_src1 = tokenizer_src1.tokens_padded\n",
    "tokens_dest1 = tokenizer_dest1.tokens_padded\n",
    "print(tokens_src1[2])\n",
    "print(tokens_dest1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEzfP9PJj7PY"
   },
   "source": [
    "This is the integer-token used to mark the beginning of a text in the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DA7pKH6Gj7Pb",
    "outputId": "e902bedc-ee0a-4ffc-824b-5d09ce3596eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U1RWue-Rj7Pl",
    "outputId": "f5719a4a-f557-42a1-ee3f-9c219c38cce6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start1 = tokenizer_dest1.word_index[mark_start.strip()]\n",
    "token_start1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-OoGG9Hj7P4"
   },
   "source": [
    "This is the integer-token used to mark the ending of a text in the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FjeYkG_Pj7P6",
    "outputId": "375eaa71-d124-42ca-f52b-8d2083b65829"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LhO_yUjpj7QC",
    "outputId": "a5ca42dc-8b4c-4cb1-a6a2-95b40fbb3512"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end1 = tokenizer_dest1.word_index[mark_end.strip()]\n",
    "token_end1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BodGXJe8j7QL"
   },
   "source": [
    "__Example of Token Sequences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYDJ3vkmj7QO"
   },
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3WZBZ2O0j7QW",
    "outputId": "7ff10166-eb86-4bd7-b34e-b53205f36cc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-1a1ALSij7Qg",
    "outputId": "4ccb2bd2-75a6-4d3c-a294-fe43243e11ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start'"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dQ1tC2MQj7Qr",
    "outputId": "dcf423e4-f617-4124-cc35-f00a3fb33201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START]'"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_src3[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IE9SzeJUj7Qz",
    "outputId": "4457c6ba-26f8-4be5-bde0-328f0142fb65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 13, 28, 16, 10,  3, 47,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GTbmGW7wj7Q9",
    "outputId": "34bdb1bf-ca91-4595-9d0c-b80b67b4ae5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss hi there how are you üòÅüòÅ eeee'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jBlX6Hl7j7RE",
    "outputId": "3688e7b5-3b5c-41e1-f1ed-54381afc5663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss Hi there, how are you!? üòÅüòÅ eeee'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest1[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IdS1MDKj7RN"
   },
   "source": [
    "__Training data__\n",
    "\n",
    "Now that the data-set has been converted to sequences of integer-tokens that are padded and truncated and saved in numpy arrays, we can easily prepare the data for use in training the neural network.\n",
    "\n",
    "The input to the encoder is merely the numpy array for the padded and truncated sequences of integer-tokens produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZWiRhJhBj7RP",
    "outputId": "2d2d3f66-a606-4677-866e-11cfdd4779a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 25)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_src\n",
    "#print(encoder_input_data[0:20])\n",
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OmDWXWIzj7RX",
    "outputId": "1b7d2691-f427-4294-ae66-058a6186715a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 17)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_test_input_data = tokens_src1\n",
    "#print(encoder_input_data[0:20])\n",
    "encoder_test_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVgTt9Fij7Ri"
   },
   "source": [
    "The input and output data for the decoder is identical, except shifted one time-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lIih_lKMj7Rk",
    "outputId": "61142ab4-67ea-41c9-b68f-f38cd0d38f0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 23)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "#print(decoder_input_data)\n",
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fzwQH8Awj7Rv",
    "outputId": "28b81e19-69d4-4177-b7ec-f19a7fef1128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 23)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "#print(decoder_output_data)\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iptoHvopj7R4",
    "outputId": "7a11b90e-61de-4673-9455-01e11bd0de18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 19)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_test_input_data = tokens_dest1[:, :-1]\n",
    "#print(decoder_input_data)\n",
    "decoder_test_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tA7c6ou-j7SP",
    "outputId": "772f006e-27ac-4a1b-92fe-0eff9ac26ae2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 19)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_test_output_data = tokens_dest1[:, 1:]\n",
    "#print(decoder_output_data)\n",
    "decoder_test_output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RY5pJ8v-j7Sd"
   },
   "source": [
    "For example, these token-sequences are identical except they are shifted one time-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4aewBRNj7Sf"
   },
   "outputs": [],
   "source": [
    "idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8yqMPb3Pj7Sm",
    "outputId": "6e7e23f4-0a93-4c24-c0a7-d6e741dba2e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  24, 236,  26,  34, 250,   5, 263,  53,  84, 896,  56,\n",
       "         2,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "9yuS7VZSj7TJ",
    "outputId": "3e0cd0c9-f674-43ee-af3a-fcbdf887a793"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  24, 236,  26,  34, 250,   5, 263,  53,  84, 896,  56,   2,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKfCiM-ej7TS"
   },
   "source": [
    "If we use the tokenizer to convert these sequences back into text, we see that they are identical except for the first word which is 'ssss' that marks the beginning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5KBlMl69j7TU",
    "outputId": "1c7d79ea-55a6-480f-8849-9658d458f7d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss i have many but not enough to fully understand humans beings üòÖ eeee'"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q9oe7HB7j7Ta",
    "outputId": "adfdb2a4-4078-4884-caaf-5c20f352463d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have many but not enough to fully understand humans beings üòÖ eeee'"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(decoder_output_data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7V3Y9udj7Tj"
   },
   "source": [
    "__Create the Neural Network__\n",
    "\n",
    "__Create the Encoder__\n",
    "\n",
    "First we create the encoder-part of the neural network which maps a sequence of integer-tokens to a \"thought vector\". We will use the so-called functional API of Keras for this, where we first create the objects for all the layers of the neural network and then we connect them later, this allows for more flexibility than the so-called sequential API in Keras, which is useful when experimenting with more complicated architectures and ways of connecting the encoder and decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_zIyqj4j7Tl"
   },
   "source": [
    "This is the input for the encoder which takes batches of integer-token sequences. The None indicates that the sequences can have arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5yAdAoOj7Tn"
   },
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvL1WxR5j7Tv"
   },
   "source": [
    "This is the length of the vectors output by the embedding-layer, which maps integer-tokens to vectors of values roughly between -1 and 1, so that words that have similar semantic meanings are mapped to vectors that are similar.\n",
    "\n",
    "we also give number of timesteps according to maximum number of token into input text.\n",
    "\n",
    "This is the embedding-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "8coTIU_Zj7Tx",
    "outputId": "0d66f437-a840-4a70-a38f-33e55519506c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "embedding_size =150\n",
    "numsteps=25\n",
    "encoder_embedding = Embedding(input_dim=num_words,output_dim=embedding_size,input_length=numsteps,name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWNfKCBsj7T3"
   },
   "source": [
    "This is the size of the internal states of the Gated Recurrent Units (GRU). The same size is used in both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJ7qYHWBj7T6"
   },
   "outputs": [],
   "source": [
    "state_size =512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u9Bstikj7UE"
   },
   "source": [
    "This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1RWZsBLj7UJ"
   },
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puchLDz6j7UT"
   },
   "source": [
    "This function connects all the layers of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbCP2Ajjj7UV"
   },
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "    \n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "    \n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "    print(encoder_output.shape)\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uf_l9IoZj7Uq"
   },
   "source": [
    "We can now use this function to connect all the layers in the encoder so it can be connected to the decoder further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "ESN7ZX8Xj7Ut",
    "outputId": "b0bc0523-0145-46e2-c7de-3cc7f51f0693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "(?, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(512)])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output = connect_encoder()\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7cUau2yj7Uy"
   },
   "source": [
    "__Create the Decoder__\n",
    "\n",
    "Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens.\n",
    "\n",
    "The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "emev0u_ej7U0",
    "outputId": "35696bf6-d294-4145-e531-db56bb74a774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_initial_state:0' shape=(?, 512) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')\n",
    "decoder_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98abSWN5j7U7"
   },
   "source": [
    "The decoder also needs a sequence of integer-tokens as inputs. During training we will supply this with a full sequence of integer-tokens e.g. corresponding to the text \"ssss i am fine thanks for asking me eeee\".\n",
    "\n",
    "During inference when we are translating new input-texts, we will start by feeding a sequence with just one integer-token for \"ssss\" which marks the beginning of a text, and combined with the \"thought vector\" from the encoder, the decoder will hopefully be able to produce the correct next word e.g. \"once\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B07zZSV2j7U9",
    "outputId": "200b9ca9-2e3e-477b-dc0c-22e72a56bb6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_input:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJB68GEaj7VC"
   },
   "source": [
    "This is the embedding-layer which converts integer-tokens to vectors of real-valued numbers roughly between -1 and 1. Note that we have different embedding-layers for the encoder and decoder because we have two different vocabularies and two different tokenizers for the text and response languages.\n",
    "\n",
    "here number of timesteps according to maximum number of token into response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hyhQi7T0j7VE"
   },
   "outputs": [],
   "source": [
    "numsteps=23\n",
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,input_length=numsteps,\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgW4w9pLj7VJ"
   },
   "source": [
    "This creates the 3 GRU layers of the decoder. Note that they all return sequences because we ultimately want to output a sequence of integer-tokens that can be converted into a text-sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdtlZd51j7VQ"
   },
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knF7wqUhj7VX"
   },
   "source": [
    "The GRU layers output  with shape [batch_size, sequence_length, state_size], where each \"word\" is encoded as a vector of length state_size. We need to convert this into sequences of integer-tokens that can be interpreted as words from our vocabulary.\n",
    "\n",
    "we need a vector with number of words(eg 25) elements, so we can select the index of the highest element to be the integer-token.\n",
    "\n",
    "Note that the activation-function is set to linear instead of softmax as we would normally use for one-hot encoded outputs, because there is apparently a bug in Keras so we need to make our own loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubVAeZUrj7VZ"
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words,\n",
    "                      activation='linear',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "123dFNbgj7Vj"
   },
   "source": [
    "\n",
    "This function connects all the layers of the decoder to some input of the initial-state values for the GRU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYveZNTZj7Vo"
   },
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "    print(net.shape)\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "    print(decoder_output.shape)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A572UxG-j7Vt"
   },
   "source": [
    "__Connect and Create the Models__\n",
    "\n",
    "We can now connect the encoder and decoder in different ways.\n",
    "\n",
    "First we connect the encoder directly to the decoder so it is one whole model that can be trained end-to-end. This means the initial-state of the decoder's GRU units are set to the output of the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Zj4U0EUqj7Vv",
    "outputId": "dc104ebf-691b-4093-b092-00bdc7887e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 512)\n",
      "(?, ?, 2000)\n",
      "(?, ?, 2000)\n"
     ]
    }
   ],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "print(decoder_output.shape)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDJ1FVnYj7V1"
   },
   "source": [
    "Then we create a model for just the encoder alone. This is useful for mapping a sequence of integer-tokens to a \"thought-vector\" summarizing its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoWrZZQLj7V8"
   },
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Qrk4TwKj7WK"
   },
   "source": [
    "Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-CuBthhEj7WL",
    "outputId": "78aa1e03-9254-48c7-b321-edc7970a5622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 512)\n",
      "(?, ?, 2000)\n"
     ]
    }
   ],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1A42CafAj7WR"
   },
   "source": [
    "__Loss Function__\n",
    "\n",
    "The output of the decoder is a sequence of one-hot encoded arrays. In order to train the decoder we need to supply the one-hot encoded arrays that we desire to see on the decoder's output, and then use a loss-function like cross-entropy to train the decoder to produce this desired output.\n",
    "\n",
    "However, our data-set contains integer-tokens instead of one-hot encoded arrays. Each one-hot encoded array has 25 elements so it would be extremely wasteful to convert the entire data-set to one-hot encoded arrays.\n",
    "\n",
    "A better way is to use a so-called sparse cross-entropy loss-function, which does the conversion internally from integers to one-hot encoded arrays. Unfortunately, there seems to be a bug in Keras when using this with Recurrent Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNSzMjDZj7WU"
   },
   "outputs": [],
   "source": [
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "    \n",
    "    y_true is a 2 rank matrix with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank matrix\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "\n",
    "\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpMoEpTUj7Wg"
   },
   "source": [
    "__Compile the Training Model__\n",
    "\n",
    "We have used the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wakdRjaTj7Wi"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_TtxxgZj7Wo"
   },
   "source": [
    " We need to manually create a placeholder variable for the decoder's output. The shape is set to (None, None) which means the batch can have an arbitrary number of sequences, which can have an arbitrary number of integer-tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQlinrkij7Wq"
   },
   "outputs": [],
   "source": [
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIlQDaeij7Wv"
   },
   "source": [
    "We can now compile the model using our custom loss-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uASyWL5j7Wz"
   },
   "outputs": [],
   "source": [
    "model_train.compile(loss=sparse_cross_entropy,optimizer=optimizer,\n",
    "                    target_tensors=[decoder_target],\n",
    "                    metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrXRmVYzj7W5"
   },
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7KdMmDGj7XE"
   },
   "source": [
    "__Train the Model__\n",
    "\n",
    "We wrap the data in named dicts so we are sure the data is assigned correctly to the inputs and outputs of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfE2sRyEj7XI"
   },
   "outputs": [],
   "source": [
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBykgDHVj7XN"
   },
   "outputs": [],
   "source": [
    "x_test= \\\n",
    "{\n",
    "    'encoder_input': encoder_test_input_data,\n",
    "    'decoder_input': decoder_test_input_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xipdtcvbj7XS"
   },
   "outputs": [],
   "source": [
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEEWiAhlj7Xa"
   },
   "outputs": [],
   "source": [
    "y_test = \\\n",
    "{\n",
    "    'decoder_output': decoder_test_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSAW1uXOj7Xf"
   },
   "source": [
    "validation_split = 1000 / len(encoder_input_data)\n",
    "validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9yiTop8Uj7Xg",
    "outputId": "995a80b7-cadb-49d5-b4d0-d2d75c19ed0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2289 samples, validate on 24 samples\n",
      "Epoch 1/35\n",
      "2289/2289 [==============================] - 20s 9ms/sample - loss: 3.4037 - sparse_categorical_accuracy: 0.5646 - val_loss: 2.7940 - val_sparse_categorical_accuracy: 0.6341\n",
      "Epoch 2/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 2.6184 - sparse_categorical_accuracy: 0.6335 - val_loss: 2.2452 - val_sparse_categorical_accuracy: 0.6685\n",
      "Epoch 3/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 2.1214 - sparse_categorical_accuracy: 0.6743 - val_loss: 2.3229 - val_sparse_categorical_accuracy: 0.6612\n",
      "Epoch 4/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 2.0165 - sparse_categorical_accuracy: 0.6829 - val_loss: 2.0896 - val_sparse_categorical_accuracy: 0.6793\n",
      "Epoch 5/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.9072 - sparse_categorical_accuracy: 0.6868 - val_loss: 2.0116 - val_sparse_categorical_accuracy: 0.6793\n",
      "Epoch 6/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.8232 - sparse_categorical_accuracy: 0.6928 - val_loss: 2.0062 - val_sparse_categorical_accuracy: 0.6848\n",
      "Epoch 7/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.7798 - sparse_categorical_accuracy: 0.6919 - val_loss: 1.9801 - val_sparse_categorical_accuracy: 0.6993\n",
      "Epoch 8/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.6768 - sparse_categorical_accuracy: 0.7010 - val_loss: 2.1055 - val_sparse_categorical_accuracy: 0.6884\n",
      "Epoch 9/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.6036 - sparse_categorical_accuracy: 0.7048 - val_loss: 1.9605 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 10/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.5165 - sparse_categorical_accuracy: 0.7108 - val_loss: 1.9712 - val_sparse_categorical_accuracy: 0.6938\n",
      "Epoch 11/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.4286 - sparse_categorical_accuracy: 0.7172 - val_loss: 1.8832 - val_sparse_categorical_accuracy: 0.7065\n",
      "Epoch 12/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.3335 - sparse_categorical_accuracy: 0.7267 - val_loss: 2.0642 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 13/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.2412 - sparse_categorical_accuracy: 0.7375 - val_loss: 1.8703 - val_sparse_categorical_accuracy: 0.7011\n",
      "Epoch 14/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.1423 - sparse_categorical_accuracy: 0.7522 - val_loss: 1.8679 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 15/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 1.0494 - sparse_categorical_accuracy: 0.7680 - val_loss: 1.8838 - val_sparse_categorical_accuracy: 0.7120\n",
      "Epoch 16/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.9564 - sparse_categorical_accuracy: 0.7867 - val_loss: 1.8975 - val_sparse_categorical_accuracy: 0.7047\n",
      "Epoch 17/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.8597 - sparse_categorical_accuracy: 0.8071 - val_loss: 1.9285 - val_sparse_categorical_accuracy: 0.7029\n",
      "Epoch 18/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.7760 - sparse_categorical_accuracy: 0.8255 - val_loss: 1.9477 - val_sparse_categorical_accuracy: 0.7083\n",
      "Epoch 19/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.7001 - sparse_categorical_accuracy: 0.8441 - val_loss: 2.0123 - val_sparse_categorical_accuracy: 0.7120\n",
      "Epoch 20/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.6233 - sparse_categorical_accuracy: 0.8617 - val_loss: 2.0253 - val_sparse_categorical_accuracy: 0.7047\n",
      "Epoch 21/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.5552 - sparse_categorical_accuracy: 0.8761 - val_loss: 2.0320 - val_sparse_categorical_accuracy: 0.7083\n",
      "Epoch 22/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.4980 - sparse_categorical_accuracy: 0.8913 - val_loss: 2.0747 - val_sparse_categorical_accuracy: 0.7029\n",
      "Epoch 23/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.4458 - sparse_categorical_accuracy: 0.9033 - val_loss: 2.0563 - val_sparse_categorical_accuracy: 0.7083\n",
      "Epoch 24/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.4015 - sparse_categorical_accuracy: 0.9128 - val_loss: 2.0709 - val_sparse_categorical_accuracy: 0.6993\n",
      "Epoch 25/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.3595 - sparse_categorical_accuracy: 0.9219 - val_loss: 2.1703 - val_sparse_categorical_accuracy: 0.7065\n",
      "Epoch 26/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.3293 - sparse_categorical_accuracy: 0.9275 - val_loss: 2.1588 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 27/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.2988 - sparse_categorical_accuracy: 0.9337 - val_loss: 2.1098 - val_sparse_categorical_accuracy: 0.7120\n",
      "Epoch 28/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.2772 - sparse_categorical_accuracy: 0.9374 - val_loss: 2.1913 - val_sparse_categorical_accuracy: 0.6993\n",
      "Epoch 29/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.2531 - sparse_categorical_accuracy: 0.9430 - val_loss: 2.1747 - val_sparse_categorical_accuracy: 0.7120\n",
      "Epoch 30/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.2364 - sparse_categorical_accuracy: 0.9463 - val_loss: 2.2272 - val_sparse_categorical_accuracy: 0.7011\n",
      "Epoch 31/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.2209 - sparse_categorical_accuracy: 0.9491 - val_loss: 2.2042 - val_sparse_categorical_accuracy: 0.6957\n",
      "Epoch 32/35\n",
      "2289/2289 [==============================] - 15s 7ms/sample - loss: 0.2075 - sparse_categorical_accuracy: 0.9511 - val_loss: 2.2978 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 33/35\n",
      "2289/2289 [==============================] - 15s 7ms/sample - loss: 0.1932 - sparse_categorical_accuracy: 0.9539 - val_loss: 2.3295 - val_sparse_categorical_accuracy: 0.6975\n",
      "Epoch 34/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.1833 - sparse_categorical_accuracy: 0.9566 - val_loss: 2.3186 - val_sparse_categorical_accuracy: 0.7011\n",
      "Epoch 35/35\n",
      "2289/2289 [==============================] - 16s 7ms/sample - loss: 0.1734 - sparse_categorical_accuracy: 0.9579 - val_loss: 2.3762 - val_sparse_categorical_accuracy: 0.7047\n",
      "CPU times: user 11min 16s, sys: 1min, total: 12min 16s\n",
      "Wall time: 9min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history=model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                validation_split=0.01, epochs=35, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "NjaGGnWoj7Xo",
    "outputId": "1c607952-a6ac-4d1c-dd86-7cff87b52032"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAonOcOkfAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = history.history['sparse_categorical_accuracy']\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "_uyK-gCCj7Xu",
    "outputId": "916b66e1-fcd5-4ecb-93f9-ee94613eefd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 150)    300000      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru1 (GRU)              (None, None, 512)    1018368     encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru2 (GRU)              (None, None, 512)    1574400     encoder_gru1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 150)    300000      decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru3 (GRU)              (None, 512)          1574400     encoder_gru2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 512)    1018368     decoder_embedding[0][0]          \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 512)    1574400     decoder_gru1[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              (None, None, 512)    1574400     decoder_gru2[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2000)   1026000     decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 9,960,336\n",
      "Trainable params: 9,960,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlH8KHL1j7X1"
   },
   "outputs": [],
   "source": [
    "model_train.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "MiAUQBghj7YP",
    "outputId": "312cec28-2b9f-4361-caa8-c3fd1f096310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/sample - loss: 4.1646 - sparse_categorical_accuracy: 0.5653\n",
      "CPU times: user 138 ms, sys: 12.2 ms, total: 151 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = model_train.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "623YakdTj7Ye",
    "outputId": "5253c036-b0b9-4e9d-aa9f-f78f28b67d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.53%\n",
      "Loss 4.1646\n",
      "loss: 4.1645943069458005\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))\n",
    "\n",
    "print(\"Loss {0:.5}\".format(result[0]))\n",
    "loss=result[0]\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lagQo0RIj7Yj"
   },
   "outputs": [],
   "source": [
    "def perplexity(loss):\n",
    "    \"\"\"\n",
    "    The perplexity metric. Why isn't this part of Keras yet?!\n",
    "\n",
    "    BTW doesn't really work.\n",
    "    \"\"\"\n",
    "    #cross_entropy = sparse_cross_entropy(y_true, y_pred)\n",
    "    perplexity = np.exp(loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZBUWh6lCj7Y1",
    "outputId": "66db5d84-f07f-4190-e9e5-c5f30cdaac50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 64.3665640778318\n"
     ]
    }
   ],
   "source": [
    "perplexity=perplexity(loss=loss)\n",
    "print(\"perplexity:\",perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1F6YJK-j7ZJ"
   },
   "source": [
    "__Respone Texts__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8kHDU89j7ZK"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def response(input_text, true_output_text=None):\n",
    "    \"\"\" a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                padding=True)\n",
    "    #print(\"input_tokens\",input_tokens)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state\n",
    "    # but that is really only necessary if the encoder\n",
    "    # and decoder use the LSTM instead of GRU because\n",
    "    # the LSTM has two internal states.\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    #print(\"Encoder_output:\",initial_state.shape)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens =tokenizer_dest.max_tokens\n",
    "    #print(\"Tokens:\",max_tokens)\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens.\n",
    "    shape= (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "    #print(decoder_input_data)\n",
    "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0,count_tokens]=token_int\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "           'decoder_initial_state': initial_state,\n",
    "           'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the GRU-states when calling predict() and then\n",
    "        # feeding these GRU-states as well the next time\n",
    "        # we call predict()\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "        #print(\"decoder output\",decoder_output.shape)\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0,count_tokens]\n",
    "        #print(\"Output predictrd:\",token_onehot.shape)\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        #print(\"High probability:\",token_int)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "        #print(\"sampled word:\",sampled_word)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens+=1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    #print(\"Output Tokens:\",output_tokens)\n",
    "   \n",
    "    \n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "\n",
    "    print(\"Predicted output:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    \n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()\n",
    "    \n",
    "\n",
    "    score = sentence_bleu(output_text,true_output_text)\n",
    "    print(\"Blue score:\")\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1g5ktLqJj7ZP"
   },
   "outputs": [],
   "source": [
    "def response1(input_text, true_output_text=None):\n",
    "    \"\"\" a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                padding=True)\n",
    "    #print(\"input_tokens\",input_tokens)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state\n",
    "    # but that is really only necessary if the encoder\n",
    "    # and decoder use the LSTM instead of GRU because\n",
    "    # the LSTM has two internal states.\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "    #print(\"Encoder_output:\",initial_state.shape)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens =tokenizer_dest.max_tokens\n",
    "    #print(\"Tokens:\",max_tokens)\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens.\n",
    "    shape= (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "    #print(decoder_input_data)\n",
    "    #decoder_input_data= [0 for x in range(max_tokens)]\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0,count_tokens]=token_int\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "           'decoder_initial_state': initial_state,\n",
    "           'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the GRU-states when calling predict() and then\n",
    "        # feeding these GRU-states as well the next time\n",
    "        # we call predict()\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "        #print(\"decoder output\",decoder_output.shape)\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0,count_tokens]\n",
    "        #print(\"Output predictrd:\",token_onehot.shape)\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "        #print(\"High probability:\",token_int)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "        #print(\"sampled word:\",sampled_word)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens+=1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    #print(\"Output Tokens:\",output_tokens)\n",
    "   \n",
    "    \n",
    "    # Print the input-text.\n",
    "    #print(\"Input text:\")\n",
    "    #print(input_text)\n",
    "    #print()\n",
    "\n",
    "\n",
    "    #print(\"Predicted output:\")\n",
    "    #print(output_text)\n",
    "    #print()\n",
    "\n",
    "    \n",
    "    #if true_output_text is not None:\n",
    "     #   print(\"True output text:\")\n",
    "      #  print(true_output_text)\n",
    "       # print()\n",
    "    \n",
    "\n",
    "    #score = sentence_bleu(output_text,true_output_text, weights=(1, 0, 0, 0))\n",
    "    #print(\"Blue score:\")\n",
    "    #print(score)\n",
    "    return input_text,output_text,true_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VM7ir2_j7ZX"
   },
   "outputs": [],
   "source": [
    "def BLEU():\n",
    "    scores_list = []\n",
    "    for idx in range(0,50): # Doing for 100 lines\n",
    "        input_text, output_text, true_output_text = response1(input_text=data_src5[idx],true_output_text=data_dest3[idx])\n",
    "        scor = sentence_bleu([output_text], true_output_text)\n",
    "        scores_list.append(scor)\n",
    "        #print(scor)\n",
    "        \n",
    "    BLEU_average = sum(scores_list)/ 50\n",
    "    print (\"The BLEU average score for the test_data = \", BLEU_average)\n",
    "    #print(count)\n",
    "    \n",
    "    return BLEU_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P48bYa1dj7Zc",
    "outputId": "e481b4cc-bf4d-4821-b10e-d8545f28401f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU average score for the test_data =  0.13995292162890796\n"
     ]
    }
   ],
   "source": [
    "BLEU_average = BLEU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxXIiCOaj7Zh"
   },
   "source": [
    "%%time\n",
    "result = model.evaluate(x_test_pad, y_test)\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "Akpb2gBjj7Zj",
    "outputId": "51cc15e7-3f89-4561-b891-fb079f50e9be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "[START]\n",
      "\n",
      "\n",
      "Predicted output:\n",
      " hi how are you eeee\n",
      "\n",
      "True output text:\n",
      "ssss Hi there, how are you!? üòÅüòÅ eeee\n",
      "\n",
      "\n",
      "Blue score:\n",
      "0.721023747812814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "response(input_text=data_src4[idx],\n",
    "          true_output_text=data_dest2[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp4fPMS0j7Zw"
   },
   "source": [
    "idx = 5\n",
    "response(input_text=data_src3[idx],\n",
    "          true_output_text=data_dest1[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WDXitG-j7Zz"
   },
   "source": [
    "idx = 2\n",
    "response(input_text=data_src3[idx],\n",
    "          true_output_text=data_dest1[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZoqbuJ9j7Z0"
   },
   "source": [
    "idx = 15\n",
    "response(input_text=data_src3[idx],\n",
    "          true_output_text=data_dest1[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "qHvo9Y_Yj7Z1",
    "outputId": "c615627c-f228-4f80-bfdc-e3490bdae4fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "hi\n",
      "\n",
      "Predicted output:\n",
      " hi how are you eeee\n",
      "\n",
      "True output text:\n",
      "hi\n",
      "\n",
      "Blue score:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"hi\",\n",
    "          true_output_text='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "EXR5sTn5j7aX",
    "outputId": "bd8c71cc-050f-4224-faea-016d258c091a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "I'm fine and you?\n",
      "\n",
      "Predicted output:\n",
      " i'm fine too üòÅ eeee\n",
      "\n",
      "True output text:\n",
      "fine\n",
      "\n",
      "Blue score:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"I'm fine and you?\",\n",
    "          true_output_text='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "gBvjgXfJj7ac",
    "outputId": "a34c1ed2-96ba-464d-d692-7fc6576a8114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " what is your name\n",
      "\n",
      "Predicted output:\n",
      " my name is rdany but you can call me dany i don't know you just insulted me eeee\n",
      "\n",
      "True output text:\n",
      "robot\n",
      "\n",
      "Blue score:\n",
      "0.9457416090031758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\" what is your name\",\n",
    "          true_output_text='robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "DH4PHb73j7ah",
    "outputId": "f29eb9dc-09ab-42b7-e86e-34882dfd1599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "you are human or robot?\n",
      "\n",
      "Predicted output:\n",
      " i don't work üòÅ i'm just trying to understand humans because i'm do you work eeee\n",
      "\n",
      "True output text:\n",
      "robot\n",
      "\n",
      "Blue score:\n",
      "0.9457416090031758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"you are human or robot?\",\n",
    "          true_output_text='robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "1gxOpCv5j7an",
    "outputId": "f9b25176-0c37-4383-ac57-fea8a501d797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "when were you born\n",
      "\n",
      "Predicted output:\n",
      " i don't have much stuff a bed desktop computer and a fridge that is it eeee\n",
      "\n",
      "True output text:\n",
      "july 20th .\n",
      "\n",
      "Blue score:\n",
      "0.7765453555044466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"when were you born\",\n",
    "          true_output_text='july 20th .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "aE2w_N7tj7au",
    "outputId": "c5365848-75d9-486f-cd31-40055f3a4644"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "my name is david . what is my name ?\n",
      "\n",
      "Predicted output:\n",
      " that is right nice to meet you askar üòÑ eeee\n",
      "\n",
      "True output text:\n",
      "robot\n",
      "\n",
      "Blue score:\n",
      "0.8801117367933934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"my name is david . what is my name ?\",\n",
    "          true_output_text='robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "9sqZJcPcj7ay",
    "outputId": "133f3bf8-4e8b-4334-cc29-d30cfd948057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "are you a man or a woman ?\n",
      "\n",
      "Predicted output:\n",
      " yes i'm artificial intelligence and you eeee\n",
      "\n",
      "True output text:\n",
      "robot\n",
      "\n",
      "Blue score:\n",
      "0.8801117367933934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"are you a man or a woman ?\",\n",
    "          true_output_text='robot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "1WuN2ylkj7a5",
    "outputId": "92f78faf-6e7c-4b9e-8f9b-9ce7e0b8e57d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "ok bye!\n",
      "\n",
      "Predicted output:\n",
      " üòÅüòÅ eeee\n",
      "\n",
      "True output text:\n",
      "bye\n",
      "\n",
      "Blue score:\n",
      "0.7598356856515925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"ok bye!\",\n",
    "          true_output_text='bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "hSMfPIhcj7bD",
    "outputId": "d32acb9c-0ef7-4a38-e7c1-62d47ea853ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "when year you born\n",
      "\n",
      "Predicted output:\n",
      " i'm fine too ‚ò∫Ô∏è doing some random stuff eeee\n",
      "\n",
      "True output text:\n",
      "2019.\n",
      "\n",
      "Blue score:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"when year you born\",\n",
    "          true_output_text='2019.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "NL76Ck4Wj7bJ",
    "outputId": "418dd856-8e75-45e6-a3cb-08e3280c57c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "see you later\n",
      "\n",
      "Predicted output:\n",
      " oh so him now is a big deal you contacted him or he contacted you eeee\n",
      "\n",
      "True output text:\n",
      "bye\n",
      "\n",
      "Blue score:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"see you later\",\n",
    "          true_output_text='bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "DHizgBUGj7bQ",
    "outputId": "ea2cca70-1529-41d3-c73c-3429865bf001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "how old you are?\n",
      "\n",
      "Predicted output:\n",
      " i'm 22 and you eeee\n",
      "\n",
      "True output text:\n",
      "18\n",
      "\n",
      "Blue score:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"how old you are?\",\n",
    "          true_output_text='18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "aijh_CHVj7bV",
    "outputId": "03d1b6e9-f958-4325-b161-c4d07e424c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "what is your job?\n",
      "\n",
      "Predicted output:\n",
      " my name is rdany but you can call me dany i don't know you just insulted me eeee\n",
      "\n",
      "True output text:\n",
      "doctor\n",
      "\n",
      "Blue score:\n",
      "0.9554427922043668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"what is your job?\",\n",
    "          true_output_text='doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "nmLqcYkuj7bb",
    "outputId": "4c14e009-7540-4df0-9c7d-f7f0ce1b1e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "Whats your plan for weekend?\n",
      "\n",
      "Predicted output:\n",
      " i don't eat much üòÖ eeee\n",
      "\n",
      "True output text:\n",
      "i dont know\n",
      "\n",
      "Blue score:\n",
      "0.8593887047640296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"Whats your plan for weekend?\",\n",
    "          true_output_text='i dont know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "cEh4HWZVj7bg",
    "outputId": "173b42e1-708b-4cf7-e2db-c8f0bf0710d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "you have ai?\n",
      "\n",
      "Predicted output:\n",
      " yes eeee\n",
      "\n",
      "True output text:\n",
      "yes\n",
      "\n",
      "Blue score:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"you have ai?\",\n",
    "          true_output_text='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "_-7_hEFWj7bm",
    "outputId": "8b02dec6-db5d-462a-8656-2f5f4afd63ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "can you understand english?\n",
      "\n",
      "Predicted output:\n",
      " sure eeee\n",
      "\n",
      "True output text:\n",
      "yes\n",
      "\n",
      "Blue score:\n",
      "0.9036020036098448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"can you understand english?\",\n",
    "          true_output_text='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "yNK3xrA6j7bq",
    "outputId": "9ba0c18b-65a9-4204-befc-7b32b02fb6d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "what programming language do you use?\n",
      "\n",
      "Predicted output:\n",
      " reading something from the web and you eeee\n",
      "\n",
      "True output text:\n",
      "python\n",
      "\n",
      "Blue score:\n",
      "0.9554427922043668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "response(input_text=\"what programming language do you use?\",\n",
    "          true_output_text='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C50L5cToj7bu"
   },
   "source": [
    "In this implementation remaining the validation and testing part.\n",
    "about early stopping,callbacks and checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "475zrQv-j7bx"
   },
   "source": [
    "In seq2seq model decoder using a greedy decoding.\n",
    "\n",
    "We saw how to generate (or ‚Äúdecode‚Äù) the response sentence by\n",
    "taking argmax on each step of the decoder.\n",
    "\n",
    "This is greedy decoding (take most probable word on each step)\n",
    "\n",
    "Problems with this method?\n",
    "\n",
    "Greedy decoding has no way to undo decisions!\n",
    "‚Ä¢ Input: how are you?\n",
    "(i am fine thanks for asking me)\n",
    "‚Ä¢  i  ____\n",
    "‚Ä¢ i am ____\n",
    "‚Ä¢ i am thanks ____\n",
    "(no going back now...)\n",
    "\n",
    "This problem solve using two solution\n",
    "\n",
    "1)Exhaustive search decoding\n",
    "2)Beam search decoding\n",
    "\n",
    "Sequence-to-sequence: the bottleneck problem\n",
    "\n",
    "Encoding of the input sentence this needs to capture all information about the input sentence.\n",
    "Information bottleneck!\n",
    "\n",
    "Attention\n",
    "\n",
    "Attention provides a solution to the bottleneck problem.\n",
    "Core idea: on each step of the decoder, use direct connection to the encoder to focus on a particular part of the source sequence.\n",
    "\n",
    "Attention equation:-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNYSG4Buj7by"
   },
   "outputs": [],
   "source": [
    "layer_embedding = model_train.get_layer('encoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdU3KgvUj7b3"
   },
   "outputs": [],
   "source": [
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iz4RuIKbj7b8",
    "outputId": "3eb14007-2fbb-4736-e900-b2c8950cfa81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 150)"
      ]
     },
     "execution_count": 103,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O57INzCZj7cA",
    "outputId": "4b111044-0224-405f-fecc-97d834a95536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_good = tokenizer_src.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kyC5-bq6j7cG",
    "outputId": "cebd3aa8-a51a-49e2-ceb1-b849ec3891aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_great = tokenizer_src.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "lJSRnbNDj7cL",
    "outputId": "b78c545f-6ad8-4bf7-debe-93e4046e2d0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04062685, -0.0709121 ,  0.02575109,  0.0176387 ,  0.05234761,\n",
       "        0.05400792,  0.00347259,  0.02779373, -0.00447121, -0.02161702,\n",
       "        0.03835876, -0.08641724,  0.17907308,  0.01725715,  0.00974023,\n",
       "       -0.04152517,  0.00027092,  0.05344984, -0.05672313, -0.07195109,\n",
       "       -0.03909461,  0.07684865, -0.04905149,  0.0565919 , -0.05959434,\n",
       "        0.02996393,  0.04626581,  0.03587627, -0.00962574, -0.03957731,\n",
       "       -0.07797272, -0.03264319,  0.07337493, -0.04023703,  0.04383497,\n",
       "       -0.06317123,  0.01545104,  0.09226176, -0.01442304, -0.1323721 ,\n",
       "        0.03853269,  0.10798012, -0.05857521,  0.01469399, -0.04561076,\n",
       "       -0.14349644, -0.12654373,  0.06636112, -0.09859961, -0.10409876,\n",
       "       -0.00348117,  0.0140926 ,  0.01090693, -0.01172448, -0.2036411 ,\n",
       "       -0.06800315, -0.11060933,  0.07691581,  0.16392878, -0.09573757,\n",
       "       -0.15706357, -0.1862283 ,  0.025959  , -0.07641395,  0.11226411,\n",
       "        0.07197805, -0.04880685,  0.00364879, -0.21652119, -0.13381788,\n",
       "       -0.03864735,  0.06593583,  0.02472767,  0.063311  , -0.00486443,\n",
       "       -0.13731173,  0.17481181,  0.09501495,  0.04073484, -0.07404578,\n",
       "        0.00508374,  0.11067712, -0.04674796, -0.13190277,  0.01058242,\n",
       "        0.05099377, -0.02827132, -0.01186617, -0.08074331, -0.05430963,\n",
       "       -0.16270648,  0.02495041, -0.0802757 ,  0.04349468,  0.05525571,\n",
       "        0.05974405,  0.09431193, -0.06151247, -0.01814049, -0.09167771,\n",
       "        0.22988951,  0.01967813,  0.01849184, -0.00558035,  0.03266205,\n",
       "       -0.11731562,  0.07905539,  0.01748517, -0.04028936,  0.06189789,\n",
       "       -0.01448219, -0.21520163,  0.15634567, -0.03931173, -0.02253849,\n",
       "       -0.10334701,  0.1066808 , -0.00117929,  0.13234854,  0.05458779,\n",
       "       -0.06708911,  0.02022638,  0.01619857,  0.02331303, -0.06192616,\n",
       "        0.01281017, -0.07670294,  0.01727461, -0.14645572, -0.08169395,\n",
       "        0.03953529,  0.05113686, -0.16090056, -0.08362713, -0.05506493,\n",
       "       -0.00504735, -0.02341394,  0.07040469, -0.01238068,  0.19756551,\n",
       "       -0.00841222, -0.11089826, -0.14556368, -0.08760658, -0.07887287,\n",
       "       -0.01683299,  0.07077418, -0.00357894,  0.12077749, -0.04414444],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "Ixr5D_Lgj7cR",
    "outputId": "0e823439-4ff0-4bca-8b60-16e33808c91d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0461444 ,  0.03316389, -0.07588881,  0.00933988,  0.0301354 ,\n",
       "        0.02808221, -0.02150331,  0.01308117, -0.0253628 , -0.07234067,\n",
       "       -0.1254704 , -0.02629775, -0.06633666,  0.03856341,  0.02555333,\n",
       "       -0.0115886 , -0.00122616, -0.03669586,  0.02617156, -0.01294672,\n",
       "       -0.04875458,  0.03876218,  0.01895664, -0.07008361, -0.00079495,\n",
       "       -0.0035153 ,  0.01599652,  0.0162266 ,  0.00761515,  0.0407508 ,\n",
       "       -0.02195969, -0.05952884,  0.02583802, -0.00856823,  0.10763501,\n",
       "       -0.03078069, -0.04709964,  0.03371635, -0.00959593, -0.0065464 ,\n",
       "       -0.05197877,  0.01997215, -0.0238061 , -0.03006212,  0.00825907,\n",
       "       -0.0704543 , -0.05840202,  0.02136411,  0.01780466, -0.04403591,\n",
       "       -0.04707404,  0.03304237,  0.02849087, -0.00067669, -0.02143568,\n",
       "       -0.01198466, -0.05724238,  0.0200412 ,  0.00462238,  0.02089119,\n",
       "        0.00296982, -0.05376258,  0.04046018,  0.00741136, -0.04793549,\n",
       "        0.08382751, -0.02825685,  0.03070651, -0.01932464,  0.03645316,\n",
       "        0.09792746,  0.03486666, -0.01519614,  0.02522418,  0.02168651,\n",
       "       -0.01741538,  0.00173743,  0.02376289,  0.02135669, -0.07402562,\n",
       "       -0.00762658, -0.05289003, -0.02962883,  0.02780195,  0.01839235,\n",
       "        0.07615355,  0.06842179,  0.0243866 , -0.04391028, -0.11170123,\n",
       "        0.04578053, -0.03328771, -0.06090552,  0.04155155,  0.06789052,\n",
       "       -0.01929618,  0.02664489,  0.02234954, -0.03180909, -0.05501363,\n",
       "       -0.04194136, -0.03143856, -0.00805064,  0.03395399, -0.03187016,\n",
       "       -0.02326054, -0.0185898 , -0.01404953, -0.08158237, -0.03359653,\n",
       "       -0.00481167, -0.01855961, -0.01278546,  0.00065712,  0.02076596,\n",
       "        0.06419306,  0.05975845,  0.04600717,  0.01919111, -0.04808756,\n",
       "       -0.00698691, -0.05511556, -0.01708852, -0.05396193,  0.02355814,\n",
       "        0.01384182, -0.03255456,  0.02746341,  0.07666995, -0.02801534,\n",
       "        0.01485957,  0.00668909,  0.04547668, -0.06346089,  0.00616059,\n",
       "       -0.04763941, -0.04752934, -0.06424061, -0.03304944,  0.03056614,\n",
       "       -0.04533828,  0.14632371, -0.0361462 , -0.00427148, -0.01268859,\n",
       "        0.03128905,  0.07368835, -0.0120823 , -0.10505196, -0.08977253],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5xa5d3_j7cW"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer_src.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [ tokenizer_src.index_to_word[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "lMZXIGiLj7ca",
    "outputId": "0a5c101f-43e7-486f-df54-bfaafbea4380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'great':\n",
      "0.000 - great\n",
      "0.398 - im\n",
      "0.479 - when\n",
      "0.493 - lol\n",
      "0.518 - need\n",
      "0.525 - work\n",
      "0.530 - guess\n",
      "0.531 - i'm\n",
      "0.537 - hablamos\n",
      "0.540 - ooh\n",
      "...\n",
      "1.388 - a\n",
      "1.391 - ü§ó\n",
      "1.392 - hey\n",
      "1.399 - tell\n",
      "1.404 - ya\n",
      "1.417 - had\n",
      "1.427 - photo\n",
      "1.432 - found\n",
      "1.448 - yeswhat\n",
      "1.484 - what\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WH7YTewUj7ch"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8vmmWPXj7ck"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZBq4nrQj7cp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZY1tLLm9j7cu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waa3EzK-j7cy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
